{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803cfcb5",
   "metadata": {},
   "source": [
    "# Taller sobre clustering aglomerativo y DBSCAN\n",
    "# Agente Come Frutas\n",
    "\n",
    "## Integrantes: Ayala Ivonne, Cumbal Mateo, Garcés Boris, Morales David, Pereira Alicia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512b684",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### 1. Introducción\n",
    "El presente informe detalla el proceso de diseño, implementación y experimentación para el desarrollo de un agente autónomo de Inteligencia Artificial en el marco del proyecto \"Agente Come-Frutas\". El desafío se centra en un problema clásico de toma de decisiones secuenciales en un entorno dinámico y con riesgos, sirviendo como un caso de estudio práctico para la aplicación de técnicas avanzadas de Machine Learning.\n",
    "\n",
    "El problema consiste en un entorno de rejilla de 5x5 en el que un agente debe aprender a navegar de manera eficiente. El objetivo principal es maximizar una puntuación recolectando \"frutas\" (que otorgan recompensas positivas) y evitando \"venenos\" (que imponen castigos negativos). La meta final es desarrollar una política de comportamiento óptima que permita al agente limpiar el tablero de todas las frutas, garantizando su supervivencia al esquivar todos los venenos presentes.\n",
    "\n",
    "Para alcanzar este objetivo, el proyecto transitó por un riguroso proceso de experimentación, explorando múltiples paradigmas de la Inteligencia Artificial. Se inició con un enfoque en el Aprendizaje por Refuerzo (Reinforcement Learning), implementando y depurando algoritmos de vanguardia como Deep Q-Networks (DQN) y su variante mejorada, Double DQN (DDQN).\n",
    "\n",
    "Frente a los desafíos clásicos de convergencia y estabilidad inherentes a RL, la investigación se expandió para incluir otras estrategias. Se exploraron los Algoritmos Genéticos, un enfoque basado en principios de evolución, y el Aprendizaje por Imitación, una potente técnica de aprendizaje supervisado que requirió el desarrollo de un \"oráculo\" experto basado en el algoritmo de búsqueda A*.\n",
    "\n",
    "A continuación, se presenta el código documentado de la implementación final, reflejando la culminación de este profundo y multifacético proceso de desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200de7cd",
   "metadata": {},
   "source": [
    "### 2. Desarrollo y Experimentación\n",
    "El proyecto se desarrolló de forma iterativa, comenzando con un algoritmo fundamental y avanzando hacia técnicas más complejas para superar los desafíos encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9755b",
   "metadata": {},
   "source": [
    "#### 2.1. Punto de Partida: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6cbc7",
   "metadata": {},
   "source": [
    "El primer enfoque implementado fue el Q-Learning tradicional. Este algoritmo fundamental de Aprendizaje por Refuerzo sirvió como base para entender la interacción entre el agente y el entorno. El \"cerebro\" del agente en este modelo es una Tabla Q, una simple matriz que almacena el valor esperado para cada acción en cada posible estado (casilla) del tablero. El agente aprende actualizando esta tabla después de cada movimiento mediante la ecuación de Bellman, y decide sus acciones balanceando la exploración (probar movimientos al azar) y la explotación (usar la mejor ruta conocida). Aunque efectivo para problemas simples, este enfoque demostró tener limitaciones en escenarios más complejos, lo que motivó la transición a redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARÁMETROS DEL APRENDIZAJE POR REFUERZO (Q-LEARNING) ---\n",
    "# Estos son los \"hiperparámetros\" que controlan cómo aprende el agente.\n",
    "RECOMPENSA_FRUTA = 100         # Puntuación alta por encontrar una fruta.\n",
    "CASTIGO_VENENO = -100           # Castigo fuerte por tocar un veneno.\n",
    "RECOMPENSA_MOVIMIENTO = -0.1    # Pequeño castigo por cada movimiento para incentivar la eficiencia.\n",
    "ALPHA = 0.1                     # Tasa de aprendizaje.\n",
    "GAMMA = 0.9                     # Factor de descuento.\n",
    "EPSILON = 1.0                   # Tasa de exploración inicial.\n",
    "EPSILON_DECAY = 0.9995          # Factor de decaimiento de epsilon.\n",
    "MIN_EPSILON = 0.01              # Mínima tasa de exploración.\n",
    "NUM_EPISODIOS_ENTRENAMIENTO = 20000 # Número de partidas que jugará el agente para aprender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cae4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLASE DEL AGENTE ---\n",
    "# Define el \"cerebro\" del agente. Contiene la Tabla Q y la lógica para aprender y decidir.\n",
    "class AgenteQLearning:\n",
    "    def __init__(self, num_estados, num_acciones):\n",
    "        self.num_acciones = num_acciones\n",
    "        # La Tabla Q es una matriz que almacena el \"valor\" de cada acción en cada estado posible.\n",
    "        # Aquí, el estado está definido por la posición (x, y) del agente en el tablero.\n",
    "        self.q_table = np.zeros((num_estados[0], num_estados[1], num_acciones))\n",
    "        self.epsilon = EPSILON  # Tasa de exploración (curiosidad).\n",
    "\n",
    "    def elegir_accion(self, estado):\n",
    "        \"\"\"Decide qué acción tomar usando la estrategia epsilon-greedy.\"\"\"\n",
    "        # Con probabilidad epsilon, toma una acción aleatoria (exploración).\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randint(0, self.num_acciones - 1)\n",
    "        # De lo contrario, elige la mejor acción conocida según la Tabla Q (explotación).\n",
    "        else:\n",
    "            return np.argmax(self.q_table[estado])\n",
    "\n",
    "    def actualizar_q_table(self, estado, accion, recompensa, nuevo_estado):\n",
    "        \"\"\"Actualiza el valor en la Tabla Q usando la fórmula de Bellman.\"\"\"\n",
    "        valor_antiguo = self.q_table[estado][accion]\n",
    "        # El valor futuro es el máximo valor Q que se puede obtener desde el nuevo estado.\n",
    "        valor_futuro_maximo = np.max(self.q_table[nuevo_estado])\n",
    "        \n",
    "        # Fórmula de Q-Learning: se actualiza el valor antiguo basado en la recompensa\n",
    "        # obtenida y el valor futuro esperado.\n",
    "        nuevo_q = valor_antiguo + ALPHA * (\n",
    "            recompensa + GAMMA * valor_futuro_maximo - valor_antiguo\n",
    "        )\n",
    "        self.q_table[estado][accion] = nuevo_q\n",
    "\n",
    "    def decaimiento_epsilon(self):\n",
    "        \"\"\"Reduce gradualmente el valor de epsilon para pasar de explorar a explotar.\"\"\"\n",
    "        if self.epsilon > MIN_EPSILON:\n",
    "            self.epsilon *= EPSILON_DECAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb0971",
   "metadata": {},
   "source": [
    "#### 2.2 DQN con CNN (Deep Q-Network con Red Convolucional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a19768",
   "metadata": {},
   "source": [
    "Para superar las limitaciones de la Tabla Q, el proyecto evolucionó hacia las Deep Q-Networks (DQN). El cerebro del agente fue reemplazado por una \n",
    "\n",
    "Red Neuronal Convolucional (CNN), lo que le permitió interpretar el tablero como una imagen y reconocer patrones espaciales. Esto es fundamental para que el agente entienda las relaciones entre su posición, las frutas y los venenos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura de la red neuronal que actúa como el \"cerebro\" visual del agente. [cite: 1260]\n",
    "class CNN_DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs): [cite: 1279]\n",
    "        super(CNN_DQN, self).__init__()\n",
    "        # Capas convolucionales para \"ver\" el tablero [cite: 1265]\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Cálculo del tamaño para la capa lineal [cite: 1292]\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        # Capas lineales para \"decidir\" el valor de cada acción [cite: 1269]\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe144e4d",
   "metadata": {},
   "source": [
    "#### 2.3 DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac89c59",
   "metadata": {},
   "source": [
    "Para mejorar la estabilidad, se implementó la variante Double DQN (DDQN). Este algoritmo reduce la tendencia del DQN a sobreestimar el valor de las acciones. Lo logra utilizando dos redes: la \n",
    "\n",
    "red principal (que se actualiza constantemente) para seleccionar la mejor acción futura, y la red objetivo (una copia más antigua y estable) para evaluar el valor de dicha acción. Esta separación hace el aprendizaje más robusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb594ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El método de aprendizaje del agente DDQN. [cite: 1446]\n",
    "def replay(self, batch_size):\n",
    "    if len(self.memory) < batch_size: [cite: 1469]\n",
    "        return\n",
    "\n",
    "    minibatch = random.sample(self.memory, batch_size) [cite: 1474]\n",
    "    \n",
    "    states = torch.FloatTensor(np.array([e[0] for e in minibatch]))\n",
    "    actions = torch.LongTensor([e[1] for e in minibatch]).unsqueeze(1)\n",
    "    rewards = torch.FloatTensor([e[2] for e in minibatch]).unsqueeze(1)\n",
    "    next_states = torch.FloatTensor(np.array([e[3] for e in minibatch]))\n",
    "    dones = torch.BoolTensor([e[4] for e in minibatch]).unsqueeze(1)\n",
    "\n",
    "    current_q_values = self.model(states).gather(1, actions)\n",
    "    \n",
    "    # --- LÓGICA CLAVE DE DOUBLE DQN --- [cite: 1485]\n",
    "    with torch.no_grad():\n",
    "        # 1. Red principal SELECCIONA la mejor acción para el siguiente estado. [cite: 1488]\n",
    "        best_next_actions = self.model(next_states).max(1)[1].unsqueeze(1)\n",
    "        \n",
    "        # 2. Red objetivo EVALÚA el valor de la acción seleccionada. [cite: 1489]\n",
    "        next_q_values_target = self.target_model(next_states).gather(1, best_next_actions)\n",
    "    \n",
    "    target_q_values = rewards + (self.gamma * next_q_values_target * (~dones))\n",
    "    \n",
    "    loss = self.criterion(current_q_values, target_q_values)\n",
    "    \n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_value_(self.model.parameters(), 1)\n",
    "    self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4021b",
   "metadata": {},
   "source": [
    "#### 2.4. Paradigma Alternativo: Algoritmos Genéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a90e4b",
   "metadata": {},
   "source": [
    "Para explorar una solución completamente diferente, se implementó un Algoritmo Genético. Este enfoque no se basa en la experiencia de un solo agente, sino en la evolución de una población completa a lo largo de generaciones. El \"ADN\" de cada agente está representado por los pesos de su red neuronal. El proceso simula la selección natural: los agentes con mejor rendimiento (\"fitness\") son seleccionados como \"padres\", sus genes se combinan y mutan para crear una nueva generación de \"hijos\", reemplazando a los menos aptos. Este ciclo de evaluación, selección, cruce y mutación permite que la población evolucione gradualmente hacia soluciones óptimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El ciclo de vida de una generación en el Algoritmo Genético.\n",
    "def run_generation(population, env):\n",
    "    # FASE 1: EVALUACIÓN DE FITNESS\n",
    "    evaluate_fitness(population, env)\n",
    "    \n",
    "    # FASE 2: SELECCIÓN DE PADRES\n",
    "    parents = selection(population)\n",
    "    \n",
    "    # ANÁLISIS Y GUARDADO DEL MEJOR AGENTE\n",
    "    best_agent_of_gen = parents[0]\n",
    "    torch.save(best_agent_of_gen.network.state_dict(), \"best_agent_genes.pth\")\n",
    "    \n",
    "    # FASE 3: CREACIÓN DE NUEVA GENERACIÓN\n",
    "    new_population = []\n",
    "    \n",
    "    # ELITISMO: Los mejores agentes pasan directamente\n",
    "    new_population.extend(parents[:ELITISM_COUNT])\n",
    "\n",
    "    # REPRODUCCIÓN: Llenar el resto con descendencia\n",
    "    while len(new_population) < POPULATION_SIZE:\n",
    "        parent1, parent2 = random.sample(parents, 2)\n",
    "        # Cruzamiento: combinar genes de padres\n",
    "        child = crossover(parent1, parent2)\n",
    "        # Mutación: introducir variación genética\n",
    "        child = mutate(child)\n",
    "        new_population.append(child)\n",
    "        \n",
    "    # Reemplazar población anterior\n",
    "    return new_population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3e3bc",
   "metadata": {},
   "source": [
    "#### 2.5 Solución Definitiva: Aprendizaje por Imitación y Currículo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596289e",
   "metadata": {},
   "source": [
    "Tras los desafíos encontrados, la estrategia final y más exitosa fue el Aprendizaje por Imitación. Este enfoque cambia el paradigma: en lugar de que el agente descubra una estrategia por sí mismo, se le enseña directamente imitando a un experto perfecto. Para ello, se implementó el algoritmo de búsqueda A* como un \"oráculo\" capaz de calcular siempre la ruta óptima en cualquier tablero.\n",
    "\n",
    "El primer paso fue generar un \"libro de texto\" para el agente, creando miles de ejemplos de jugadas perfectas. Se crearon datasets separados para escenarios de 1, 2, 3 y 4 frutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función clave para generar los datos de entrenamiento para el currículo.\n",
    "def generate_expert_data_for_n_fruits(num_fruits, num_samples, output_file):\n",
    "    \"\"\"Genera un dataset experto para un número específico de frutas.\"\"\"\n",
    "    env = GridEnvironment()\n",
    "    expert_data = []\n",
    "    print(f\"Generando {num_samples} muestras para {num_fruits} fruta(s)...\")\n",
    "    \n",
    "    while len(expert_data) < num_samples:\n",
    "        # Generar un escenario aleatorio con `num_fruits` frutas.\n",
    "        # ... (código de generación de escenario) ...\n",
    "        env.reset(agent_pos=agent_p, fruit_pos=fruit_p, poison_pos=poison_p)\n",
    "        \n",
    "        # Simular el episodio usando el experto A* paso a paso.\n",
    "        for _ in range(50):\n",
    "            if not env.fruit_pos: break\n",
    "            \n",
    "            # El experto A* recalcula la mejor ruta desde la posición actual.\n",
    "            path = a_star_search(GRID_SIZE, env.agent_pos, goal_fruit, env.poison_pos)\n",
    "\n",
    "            if path and len(path) > 0:\n",
    "                # Se toma solo el primer paso de la ruta óptima.\n",
    "                action = get_action(env.agent_pos, path[0])\n",
    "                state = env.get_state()\n",
    "                # Se guarda el par (estado_actual, accion_correcta).\n",
    "                expert_data.append((state, action))\n",
    "                env.step(action)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(expert_data, f)\n",
    "    print(f\"Dataset '{output_file}' creado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670e8e0",
   "metadata": {},
   "source": [
    "La innovación crucial fue entrenar al agente con un Aprendizaje por Currículo. En lugar de mostrarle todos los datos a la vez, el agente fue entrenado por etapas, comenzando con la tarea más fácil (1 fruta) y aumentando gradualmente la dificultad. Esto le permitió dominar primero los conceptos básicos de \"ganar\" antes de enfrentarse a escenarios más complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d28ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El script de entrenamiento que implementa la estrategia de Curriculum Learning.\n",
    "if __name__ == \"__main__\":\n",
    "    model = AgentNetwork()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # El Currículo: una lista de \"lecciones\" de dificultad creciente.\n",
    "    CURRICULUM = [\n",
    "        (\"expert_data_1_fruit.pkl\", 25),   # Lección 1: Nivel básico\n",
    "        (\"expert_data_2_fruits.pkl\", 30),  # Lección 2: Nivel intermedio\n",
    "        (\"expert_data_3_fruits.pkl\", 40),  # Lección 3: Nivel avanzado\n",
    "        (\"expert_data_4_fruits.pkl\", 50)   # Lección 4: Nivel experto\n",
    "    ]\n",
    "\n",
    "    # Bucle principal que itera a través de las lecciones del currículo.\n",
    "    for i, (dataset_file, num_epochs) in enumerate(CURRICULUM):\n",
    "        print(f\"\\n--- Iniciando Lección {i+1}/{len(CURRICULUM)}: {dataset_file} ---\")\n",
    "        \n",
    "        # Cargar el dataset de la lección actual.\n",
    "        with open(dataset_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Preparar los datos para el entrenamiento.\n",
    "        # ... (código de creación del DataLoader) ...\n",
    "\n",
    "        # Entrenar el modelo (sin reiniciarlo) por el número de épocas de la lección.\n",
    "        for epoch in range(num_epochs):\n",
    "            # ... (bucle de entrenamiento estándar de aprendizaje supervisado) ...\n",
    "            print(f\"  Época {epoch+1}/{num_epochs}, Pérdida: {avg_loss:.4f}\")\n",
    "\n",
    "    # Guardar el modelo final, que ha aprendido de todo el currículo.\n",
    "    torch.save(model.state_dict(), \"imitacion_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a9c28",
   "metadata": {},
   "source": [
    "### Jugador humano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee67918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modo de juego humano con controles aleatorios - Come Frutas.\n",
    "\n",
    "Este módulo implementa una versión jugable del entorno donde un humano puede\n",
    "controlar el agente directamente. La característica única es que los controles\n",
    "de movimiento se asignan aleatoriamente cada vez que se inicia una partida,\n",
    "añadiendo un elemento de desafío y adaptabilidad.\n",
    "\n",
    "Características principales:\n",
    "- Modo Setup: Configuración manual del escenario\n",
    "- Modo Humano: Control directo del agente por el jugador\n",
    "- Controles aleatorios: Mapeo aleatorio de teclas a movimientos\n",
    "- Interfaz intuitiva: Gráficos y feedback visual\n",
    "- Desafío adaptativo: Cada partida requiere aprender nuevos controles\n",
    "\n",
    "Propósito educativo:\n",
    "- Comparar rendimiento humano vs. IA\n",
    "- Experimentar la dificultad de adaptación a controles cambiantes\n",
    "- Entender la importancia de la consistencia en interfaces\n",
    "- Apreciar la flexibilidad del aprendizaje humano\n",
    "\n",
    "Autor: [Tu nombre]\n",
    "Fecha: Agosto 2025\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "# CONFIGURACIÓN DEL ENTORNO VISUAL\n",
    "\"\"\"\n",
    "Parámetros visuales y dimensiones de la interfaz de juego.\n",
    "Utiliza celdas más grandes (120px) para mejor visibilidad durante el juego manual.\n",
    "\"\"\"\n",
    "GRID_WIDTH = 5              # Ancho de la cuadrícula en celdas\n",
    "GRID_HEIGHT = 5             # Alto de la cuadrícula en celdas\n",
    "CELL_SIZE = 120             # Tamaño de cada celda en píxeles (mayor para juego manual)\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la ventana (600px)\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto del área de juego (600px)\n",
    "\n",
    "# PALETA DE COLORES CONSISTENTE\n",
    "\"\"\"\n",
    "Esquema de colores oscuro profesional, consistente con otros módulos del proyecto.\n",
    "\"\"\"\n",
    "COLOR_FONDO = (25, 25, 25)      # Gris muy oscuro para el fondo\n",
    "COLOR_LINEAS = (40, 40, 40)     # Gris oscuro para líneas de cuadrícula\n",
    "COLOR_CURSOR = (255, 255, 0)    # Amarillo brillante para cursor de selección\n",
    "COLOR_TEXTO = (230, 230, 230)   # Gris claro para texto legible\n",
    "\n",
    "# SISTEMA DE CONTROLES ALEATORIOS\n",
    "\"\"\"\n",
    "Genera un conjunto de teclas válidas para asignación aleatoria de controles.\n",
    "Se evitan teclas especiales para prevenir conflictos con funciones del sistema.\n",
    "\"\"\"\n",
    "TECLAS_VALIDAS = [getattr(pygame, f\"K_{c}\") for c in string.ascii_lowercase + string.digits]\n",
    "\n",
    "class EntornoHumano:\n",
    "    \"\"\"\n",
    "    Entorno de juego optimizado para control humano directo.\n",
    "    \n",
    "    Esta clase maneja la lógica del juego cuando un humano controla el agente,\n",
    "    incluyendo movimiento, colisiones, recolección de objetos y condiciones\n",
    "    de victoria/derrota. Se enfoca en proporcionar feedback inmediato y\n",
    "    una experiencia de juego fluida.\n",
    "    \n",
    "    Diferencias con entornos de IA:\n",
    "    - Feedback inmediato con mensajes en consola\n",
    "    - Lógica de juego simplificada (sin recompensas numéricas)\n",
    "    - Terminación inmediata en victoria/derrota\n",
    "    - Controles responsivos para jugabilidad humana\n",
    "    \n",
    "    Attributes:\n",
    "        agente_pos (tuple): Posición actual del agente (x, y)\n",
    "        frutas (set): Conjunto de posiciones con frutas\n",
    "        venenos (set): Conjunto de posiciones con venenos\n",
    "        paredes (set): Conjunto de posiciones con paredes/obstáculos\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuración vacía.\n",
    "        \n",
    "        El agente comienza en la esquina superior izquierda (0,0) y todos\n",
    "        los conjuntos de elementos están vacíos, permitiendo configuración manual.\n",
    "        \"\"\"\n",
    "        self.agente_pos = (0, 0)    # Posición inicial del agente\n",
    "        self.frutas = set()         # Conjunto de posiciones de frutas\n",
    "        self.venenos = set()        # Conjunto de posiciones de venenos\n",
    "        self.paredes = set()        # Conjunto de posiciones de paredes\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resetea la posición del agente al inicio del juego.\n",
    "        \n",
    "        Coloca al agente en la posición inicial (0,0) sin modificar\n",
    "        la configuración del escenario. Utilizado al comenzar una nueva partida.\n",
    "        \"\"\"\n",
    "        self.agente_pos = (0, 0)\n",
    "\n",
    "    def limpiar(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno.\n",
    "        \n",
    "        Limpia frutas, venenos y paredes del escenario, dejando una\n",
    "        cuadrícula vacía para configuración desde cero.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción del jugador humano en el entorno.\n",
    "        \n",
    "        Procesa el movimiento del agente, verifica colisiones y maneja\n",
    "        las interacciones con elementos del entorno. Proporciona feedback\n",
    "        inmediato al jugador mediante mensajes en consola.\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Dirección de movimiento:\n",
    "                         0 = Arriba (decrementar y)\n",
    "                         1 = Abajo (incrementar y)\n",
    "                         2 = Izquierda (decrementar x)\n",
    "                         3 = Derecha (incrementar x)\n",
    "        \n",
    "        Returns:\n",
    "            bool: True si el juego terminó (victoria o derrota), False si continúa\n",
    "        \"\"\"\n",
    "        # Calcular nueva posición basada en la acción\n",
    "        x, y = self.agente_pos\n",
    "        if accion == 0:     # Arriba\n",
    "            y -= 1\n",
    "        elif accion == 1:   # Abajo\n",
    "            y += 1\n",
    "        elif accion == 2:   # Izquierda\n",
    "            x -= 1\n",
    "        elif accion == 3:   # Derecha\n",
    "            x += 1\n",
    "\n",
    "        # Verificar colisiones: límites del tablero o paredes\n",
    "        if x < 0 or x >= GRID_WIDTH or y < 0 or y >= GRID_HEIGHT or (x, y) in self.paredes:\n",
    "            # Movimiento inválido: no actualizar posición\n",
    "            return False\n",
    "\n",
    "        # Movimiento válido: actualizar posición del agente\n",
    "        self.agente_pos = (x, y)\n",
    "        \n",
    "        # Verificar interacciones con elementos del entorno\n",
    "        if self.agente_pos in self.frutas:\n",
    "            # Fruta recogida: eliminar del conjunto\n",
    "            self.frutas.remove(self.agente_pos)\n",
    "            \n",
    "            # Verificar condición de victoria\n",
    "            if not self.frutas:\n",
    "                print(\"\\n✨ ¡Ganaste! Recolectaste todas las frutas.\\n\")\n",
    "                return True  # Juego terminado con éxito\n",
    "                \n",
    "        elif self.agente_pos in self.venenos:\n",
    "            # Veneno tocado: derrota inmediata\n",
    "            print(\"\\n☠️ ¡Oh no! Tocaste un veneno.\\n\")\n",
    "            return True  # Juego terminado con fallo\n",
    "            \n",
    "        # Continuar juego\n",
    "        return False\n",
    "\n",
    "    def dibujar(self, pantalla, modo, cursor_pos, img_fruta, img_veneno, img_pared, img_agente, _):\n",
    "        \"\"\"\n",
    "        Renderiza el estado completo del entorno con interfaz interactiva.\n",
    "        \n",
    "        Dibuja todos los elementos visuales del juego incluyendo grid, objetos\n",
    "        del entorno y cursor de selección. Proporciona feedback visual para\n",
    "        la interacción del jugador en diferentes modos (colocación/juego).\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde renderizar\n",
    "            modo (str): Modo actual de la interfaz ('frutas', 'venenos', 'paredes', 'jugar')\n",
    "            cursor_pos (tuple): Posición (x,y) del cursor en coordenadas de grid\n",
    "            img_fruta (pygame.Surface): Sprite de las frutas\n",
    "            img_veneno (pygame.Surface): Sprite de los venenos\n",
    "            img_pared (pygame.Surface): Sprite de las paredes\n",
    "            img_agente (pygame.Surface): Sprite del agente\n",
    "            _ : Parámetro no utilizado (compatibilidad de interfaz)\n",
    "        \n",
    "        Note:\n",
    "            Renderiza en orden específico: fondo, grid, objetos, agente, cursor.\n",
    "            El cursor cambia de color según el modo de colocación activo.\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "\n",
    "        # Dibujar líneas del grid para guía visual\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibujar elementos del entorno\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0]*CELL_SIZE, fruta[1]*CELL_SIZE))\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0]*CELL_SIZE, veneno[1]*CELL_SIZE))\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0]*CELL_SIZE, pared[1]*CELL_SIZE))\n",
    "\n",
    "        # Dibujar agente (jugador) - siempre visible en primer plano\n",
    "        pantalla.blit(img_agente, (self.agente_pos[0]*CELL_SIZE, self.agente_pos[1]*CELL_SIZE))\n",
    "\n",
    "        # Dibujar cursor de selección en modo configuración\n",
    "        if modo == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(cursor_pos[0]*CELL_SIZE, cursor_pos[1]*CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar información de interfaz\n",
    "        font = pygame.font.Font(None, 30)\n",
    "        pantalla.blit(font.render(f\"Modo: {modo}\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(font.render(\"F: Fruta, V: Veneno, W: Pared, C: Limpiar, H: Jugar\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(font.render(\"Descubre los controles ocultos usando letras/números\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "def cargar_imagen(nombre, fallback_color):\n",
    "    \"\"\"\n",
    "    Carga una imagen desde archivo con sistema de respaldo.\n",
    "    \n",
    "    Intenta cargar una imagen sprite desde el directorio actual.\n",
    "    Si la carga falla, crea una superficie de color sólido como respaldo.\n",
    "    Escala automáticamente al tamaño de celda definido.\n",
    "    \n",
    "    Args:\n",
    "        nombre (str): Nombre del archivo de imagen a cargar\n",
    "        fallback_color (tuple): Color RGB (r,g,b) para superficie de respaldo\n",
    "    \n",
    "    Returns:\n",
    "        pygame.Surface: Superficie cargada y escalada, o superficie de color\n",
    "                       si la carga falló\n",
    "    \n",
    "    Note:\n",
    "        Todas las imágenes se escalan a CELL_SIZE x CELL_SIZE píxeles.\n",
    "        Utiliza convert_alpha() para optimizar el renderizado con transparencia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir ruta completa al archivo de imagen\n",
    "        ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "        # Cargar imagen con soporte de transparencia\n",
    "        img = pygame.image.load(ruta).convert_alpha()\n",
    "        # Escalar a tamaño de celda estándar\n",
    "        return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "    except:\n",
    "        # Crear superficie de respaldo con color sólido si falla la carga\n",
    "        s = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "        s.fill(fallback_color)\n",
    "        return s\n",
    "\n",
    "def generar_controles_aleatorios():\n",
    "    \"\"\"\n",
    "    Genera un mapeo aleatorio de teclas para controles de movimiento.\n",
    "    \n",
    "    Crea una asignación aleatoria entre teclas del teclado y direcciones\n",
    "    de movimiento para añadir un elemento de desafío y descubrimiento\n",
    "    al juego. Los jugadores deben encontrar qué teclas controlan cada dirección.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapeo de códigos de tecla pygame a acciones de movimiento:\n",
    "              {tecla_pygame: accion_int}\n",
    "              donde accion_int es 0=Arriba, 1=Abajo, 2=Izquierda, 3=Derecha\n",
    "    \n",
    "    Note:\n",
    "        Utiliza teclas alfanuméricas (A-Z, 0-9) para máxima compatibilidad.\n",
    "        Garantiza que cada dirección tenga exactamente una tecla asignada.\n",
    "    \"\"\"\n",
    "    # Seleccionar 4 teclas aleatorias del conjunto disponible\n",
    "    teclas = random.sample(TECLAS_VALIDAS, 4)\n",
    "    # Crear lista de acciones de movimiento\n",
    "    acciones = [0, 1, 2, 3]  # Arriba, abajo, izquierda, derecha\n",
    "    # Mezclar aleatoriamente las acciones\n",
    "    random.shuffle(acciones)\n",
    "    # Crear diccionario de mapeo tecla->acción\n",
    "    return dict(zip(teclas, acciones))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal del juego en modo humano.\n",
    "    \n",
    "    Inicializa Pygame, configura la ventana de juego y ejecuta el bucle\n",
    "    principal que maneja dos modos: configuración del entorno y juego\n",
    "    con controles aleatorios. Proporciona una experiencia interactiva\n",
    "    donde el jugador puede diseñar niveles y luego jugarlos.\n",
    "    \n",
    "    Flujo del juego:\n",
    "        1. Modo SETUP: Colocar frutas, venenos y paredes con el mouse\n",
    "        2. Modo JUGAR: Controlar agente con teclas aleatorias descubiertas\n",
    "        3. Victoria: Recolectar todas las frutas\n",
    "        4. Derrota: Tocar veneno\n",
    "    \n",
    "    Controles SETUP:\n",
    "        - Mouse: Mover cursor\n",
    "        - F: Colocar fruta\n",
    "        - V: Colocar veneno  \n",
    "        - W: Colocar pared\n",
    "        - C: Limpiar todo\n",
    "        - H: Iniciar juego\n",
    "    \n",
    "    Controles JUGAR:\n",
    "        - Teclas aleatorias para movimiento (descubrir experimentando)\n",
    "        - ESC: Volver a configuración\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y configurar ventana\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 100))\n",
    "    pygame.display.set_caption(\"Modo Humano Aleatorio - Come Frutas\")\n",
    "\n",
    "    # Inicializar entorno y variables de estado\n",
    "    entorno = EntornoHumano()\n",
    "    cursor_pos = [0, 0]\n",
    "    modo = \"SETUP\"  # Modo inicial: configuración del entorno\n",
    "    mapeo_controles = {}  # Mapeo de teclas aleatorias (generado al jugar)\n",
    "\n",
    "    # Cargar sprites con colores de respaldo\n",
    "    img_fruta = cargar_imagen(\"fruta.png\", (40, 200, 40))\n",
    "    img_veneno = cargar_imagen(\"veneno.png\", (255, 50, 50))\n",
    "    img_pared = cargar_imagen(\"pared.jpg\", (80, 80, 80))\n",
    "    img_agente = cargar_imagen(\"agente.png\", (60, 100, 255))\n",
    "\n",
    "    # Variables de control del juego\n",
    "    reloj = pygame.time.Clock()\n",
    "    corriendo = True\n",
    "\n",
    "    # Bucle principal del juego\n",
    "    while corriendo:\n",
    "        # Procesar eventos de entrada\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "            elif evento.type == pygame.KEYDOWN:\n",
    "                if evento.key == pygame.K_s:\n",
    "                    modo = \"SETUP\"\n",
    "                elif evento.key == pygame.K_h:\n",
    "                    modo = \"HUMANO\"\n",
    "                    entorno.reset()\n",
    "                    mapeo_controles = generar_controles_aleatorios()\n",
    "\n",
    "                if modo == \"SETUP\":\n",
    "                    if evento.key == pygame.K_UP: cursor_pos[1] = max(0, cursor_pos[1]-1)\n",
    "                    elif evento.key == pygame.K_DOWN: cursor_pos[1] = min(GRID_HEIGHT-1, cursor_pos[1]+1)\n",
    "                    elif evento.key == pygame.K_LEFT: cursor_pos[0] = max(0, cursor_pos[0]-1)\n",
    "                    elif evento.key == pygame.K_RIGHT: cursor_pos[0] = min(GRID_WIDTH-1, cursor_pos[0]+1)\n",
    "                    # Colocación de elementos con teclas específicas\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    if evento.key == pygame.K_f: \n",
    "                        # F: Colocar/quitar fruta (toggle)\n",
    "                        entorno.frutas.symmetric_difference_update({pos})\n",
    "                        entorno.venenos.discard(pos)\n",
    "                        entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_v: \n",
    "                        # V: Colocar/quitar veneno (toggle)\n",
    "                        entorno.venenos.symmetric_difference_update({pos})\n",
    "                        entorno.frutas.discard(pos)\n",
    "                        entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_w: \n",
    "                        # W: Colocar/quitar pared (toggle)\n",
    "                        entorno.paredes.symmetric_difference_update({pos})\n",
    "                        entorno.frutas.discard(pos)\n",
    "                        entorno.venenos.discard(pos)\n",
    "                    elif evento.key == pygame.K_c: \n",
    "                        # C: Limpiar todo el entorno\n",
    "                        entorno.limpiar()\n",
    "\n",
    "                # Controles específicos del modo HUMANO\n",
    "                elif modo == \"HUMANO\":\n",
    "                    if evento.key in mapeo_controles:\n",
    "                        # Ejecutar acción de movimiento con tecla aleatoria\n",
    "                        accion = mapeo_controles[evento.key]\n",
    "                        terminado = entorno.step(accion)\n",
    "                        if terminado:\n",
    "                            # Volver a configuración al terminar el juego\n",
    "                            modo = \"SETUP\"\n",
    "\n",
    "        # Renderizar estado actual del juego\n",
    "        entorno.dibujar(pantalla, modo, cursor_pos, img_fruta, img_veneno, img_pared, img_agente, mapeo_controles)\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(30)\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a5bfd",
   "metadata": {},
   "source": [
    "### 3. Resultados y Conclusión\n",
    "El viaje a través de estos diferentes paradigmas de IA fue revelador. Mientras que los enfoques de Aprendizaje por Refuerzo y Algoritmos Genéticos mostraron potencial, también exhibieron dificultades para converger de manera consistente en un entorno tan \"frágil\", donde un solo error puede llevar al fracaso.\n",
    "\n",
    "La estrategia de Aprendizaje por Imitación combinada con el Aprendizaje por Currículo demostró ser la más efectiva y robusta. Al aprender de un experto perfecto y hacerlo de manera gradual, el agente final fue capaz de generalizar su conocimiento y resolver de manera confiable tanto escenarios simples como complejos. Para la demostración visual, se desarrolló una interfaz en Pygame que permite a los usuarios configurar sus propios niveles y observar al agente entrenado resolverlos en tiempo real.\n",
    "\n",
    "Este proyecto subraya una lección fundamental en el desarrollo de IA: a menudo, la estrategia de entrenamiento y la calidad de los datos son tan o más importantes que la elección del algoritmo en sí. El resultado es un agente competente y una profunda comprensión práctica de los desafíos y soluciones en el campo del Machine Learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
