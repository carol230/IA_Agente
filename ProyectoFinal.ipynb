{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803cfcb5",
   "metadata": {},
   "source": [
    "# Taller sobre clustering aglomerativo y DBSCAN\n",
    "# Agente Come Frutas\n",
    "\n",
    "## Integrantes: Ayala Ivonne, Cumbal Mateo, Garcés Boris, Morales David, Pereira Alicia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512b684",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### 1. Introducción\n",
    "El presente informe detalla el proceso de diseño, implementación y experimentación para el desarrollo de un agente autónomo de Inteligencia Artificial en el marco del proyecto \"Agente Come-Frutas\". El desafío se centra en un problema clásico de toma de decisiones secuenciales en un entorno dinámico y con riesgos, sirviendo como un caso de estudio práctico para la aplicación de técnicas avanzadas de Machine Learning.\n",
    "\n",
    "El problema consiste en un entorno de rejilla de 5x5 en el que un agente debe aprender a navegar de manera eficiente. El objetivo principal es maximizar una puntuación recolectando \"frutas\" (que otorgan recompensas positivas) y evitando \"venenos\" (que imponen castigos negativos). La meta final es desarrollar una política de comportamiento óptima que permita al agente limpiar el tablero de todas las frutas, garantizando su supervivencia al esquivar todos los venenos presentes.\n",
    "\n",
    "Para alcanzar este objetivo, el proyecto transitó por un riguroso proceso de experimentación, explorando múltiples paradigmas de la Inteligencia Artificial. Se inició con un enfoque en el Aprendizaje por Refuerzo (Reinforcement Learning), implementando y depurando algoritmos de vanguardia como Deep Q-Networks (DQN) y su variante mejorada, Double DQN (DDQN).\n",
    "\n",
    "Frente a los desafíos clásicos de convergencia y estabilidad inherentes a RL, la investigación se expandió para incluir otras estrategias. Se exploraron los Algoritmos Genéticos, un enfoque basado en principios de evolución, y el Aprendizaje por Imitación, una potente técnica de aprendizaje supervisado que requirió el desarrollo de un \"oráculo\" experto basado en el algoritmo de búsqueda A*.\n",
    "\n",
    "A continuación, se presenta el código documentado de la implementación final, reflejando la culminación de este profundo y multifacético proceso de desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200de7cd",
   "metadata": {},
   "source": [
    "### 2. Desarrollo y Experimentación\n",
    "El proyecto se desarrolló de forma iterativa, comenzando con un algoritmo fundamental y avanzando hacia técnicas más complejas para superar los desafíos encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9755b",
   "metadata": {},
   "source": [
    "#### 2.1. Punto de Partida: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6cbc7",
   "metadata": {},
   "source": [
    "El desarrollo del agente autónomo inició con la implementación del algoritmo clásico de Aprendizaje por Refuerzo: Q-Learning. Este enfoque permitió construir una base conceptual sólida para comprender cómo un agente puede aprender a actuar en un entorno dinámico sin conocimiento previo del mismo. A través de ensayo y error, el agente aprende una política óptima que maximiza su recompensa acumulada al recolectar frutas y evitar venenos.\n",
    "\n",
    "**Entorno y Representación de Estados** \n",
    "\n",
    "El entorno fue modelado como una cuadrícula de 5×5, donde cada celda puede contener una fruta (recompensa positiva), un veneno (castigo fuerte) o estar vacía. El estado del agente se representa únicamente por su posición (x,y) en la rejilla, lo que resulta en un espacio de estados de tamaño 25 (5 filas × 5 columnas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f858c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.q_table = np.zeros((num_estados[0], num_estados[1], num_acciones))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f1282",
   "metadata": {},
   "source": [
    "Cada entrada de la tabla Q[s][a] almacena el valor esperado de realizar la acción a en el estado s. El agente puede realizar cuatro acciones posibles: arriba, abajo, izquierda y derecha.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164bd6b",
   "metadata": {},
   "source": [
    "**Parámetros del Aprendizaje**\n",
    "\n",
    "Se definieron los siguientes hiperparámetros para controlar el proceso de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARÁMETROS DEL APRENDIZAJE POR REFUERZO (Q-LEARNING) ---\n",
    "# Estos son los \"hiperparámetros\" que controlan cómo aprende el agente.\n",
    "RECOMPENSA_FRUTA = 100         # Puntuación alta por encontrar una fruta.\n",
    "CASTIGO_VENENO = -100           # Castigo fuerte por tocar un veneno.\n",
    "RECOMPENSA_MOVIMIENTO = -0.1    # Castigo por movimiento para la eficiencia.\n",
    "ALPHA = 0.1                     # Tasa de aprendizaje.\n",
    "GAMMA = 0.9                     # Factor de descuento.\n",
    "EPSILON = 1.0                   # Tasa de exploración inicial.\n",
    "EPSILON_DECAY = 0.9995          # Factor de decaimiento de epsilon.\n",
    "MIN_EPSILON = 0.01              # Mínima tasa de exploración.\n",
    "NUM_EPISODIOS_ENTRENAMIENTO = 20000 #Número de partidas que jugará el agente para aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb7348",
   "metadata": {},
   "source": [
    "- ALPHA define cuánto se actualiza la tabla Q con la nueva información.\n",
    "\n",
    "- GAMMA determina la importancia del valor futuro frente a la recompensa inmediata.\n",
    "\n",
    "- EPSILON regula el equilibrio entre exploración y explotación, y su decaimiento permite al agente ser curioso al inicio y luego explotar lo aprendido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfa4ed",
   "metadata": {},
   "source": [
    "**Política de Acción: Epsilon-Greedy**\n",
    "\n",
    "El agente selecciona sus movimientos mediante una política epsilon-greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cae4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_accion(self, estado):\n",
    "    if random.uniform(0, 1) < self.epsilon:\n",
    "        return random.randint(0, self.num_acciones - 1)  # Exploración\n",
    "    else:\n",
    "        return np.argmax(self.q_table[estado])          # Explotación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db18b1c",
   "metadata": {},
   "source": [
    "Esta estrategia permite al agente explorar nuevas acciones al inicio del entrenamiento, pero gradualmente se convierte en un experto que toma decisiones óptimas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadef1ad",
   "metadata": {},
   "source": [
    "**Aprendizaje con la Ecuación de Bellman**\n",
    "\n",
    "La función de actualización aplica la ecuación de Bellman, actualizando el valor de la acción ejecutada según la recompensa recibida y el mejor valor futuro estimado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3da2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_q_table(self, estado, accion, recompensa, nuevo_estado):\n",
    "    valor_antiguo = self.q_table[estado][accion]\n",
    "    valor_futuro_maximo = np.max(self.q_table[nuevo_estado])\n",
    "    nuevo_q = valor_antiguo + ALPHA * (\n",
    "        recompensa + GAMMA * valor_futuro_maximo - valor_antiguo\n",
    "    )\n",
    "    self.q_table[estado][accion] = nuevo_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a32c3",
   "metadata": {},
   "source": [
    "Este mecanismo es el núcleo del aprendizaje en Q-Learning: el agente se adapta en función de la retroalimentación recibida del entorno.\n",
    "\n",
    "**Entrenamiento del Agente**\n",
    "\n",
    "El agente se entrena a lo largo de 20,000 episodios, en los que explora y aprende sobre el entorno. Cada episodio se reinicia desde la posición inicial y termina cuando el agente recolecta todas las frutas o toca un veneno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e945fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(NUM_EPISODIOS_ENTRENAMIENTO):\n",
    "    entorno.frutas = frutas_iniciales.copy()\n",
    "    entorno.venenos = venenos_iniciales.copy()\n",
    "    estado = entorno.reset_a_configuracion_inicial()\n",
    "    terminado = False\n",
    "    while not terminado:\n",
    "        accion = agente.elegir_accion(estado)\n",
    "        nuevo_estado, recompensa, terminado = entorno.step(accion, \"TRAINING\")\n",
    "        agente.actualizar_q_table(estado, accion, recompensa, nuevo_estado)\n",
    "        estado = nuevo_estado\n",
    "    agente.decaimiento_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c34be2",
   "metadata": {},
   "source": [
    "**Visualización y Modo Juego**\n",
    "\n",
    "Una vez entrenado, el agente puede ser probado en el modo PLAYING, donde actúa usando únicamente explotación (ϵ=0). El entorno fue diseñado con pygame, incluyendo sprites para representar frutas, venenos y paredes, lo cual facilita visualizar las decisiones del agente en tiempo real.\n",
    "\n",
    "**Limitaciones Observadas**\n",
    "\n",
    "Aunque este enfoque demostró ser funcional en escenarios simples, presentó problemas de convergencia y eficiencia cuando el número de frutas y venenos aumentaba. La tabla Q escala mal en entornos con más dimensiones, lo que motivó la transición hacia métodos más avanzados como Deep Q-Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb0971",
   "metadata": {},
   "source": [
    "#### 2.2 DQN con CNN (Deep Q-Network con Red Convolucional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a19768",
   "metadata": {},
   "source": [
    "**Motivación y Limitaciones del Q-Learning**\n",
    "\n",
    "Tras observar las limitaciones del Q-Learning clásico —en particular su incapacidad para escalar a entornos complejos o generalizar eficientemente— se adoptó el algoritmo Deep Q-Network (DQN) como siguiente paso evolutivo. Este enfoque reemplaza la tabla Q discreta por una red neuronal convolucional (CNN), capaz de aproximar los valores Q mediante aprendizaje profundo a partir de representaciones visuales del entorno.\n",
    "\n",
    "**Representación del Estado como Imagen Multicanal**\n",
    "\n",
    "Cada estado del entorno fue transformado en un tensor tridimensional de forma (3, 5, 5), donde:\n",
    "\n",
    "- Canal 0 indica la posición del agente.\n",
    "\n",
    "- Canal 1 marca las frutas.\n",
    "\n",
    "- Canal 2 representa los venenos.\n",
    "\n",
    "Esta representación permite que la CNN procese el entorno como una \"imagen\", reconociendo patrones espaciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment.py\n",
    "state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "state[0, agent_pos[0], agent_pos[1]] = 1.0  # Canal del agente\n",
    "state[1, fruta[0], fruta[1]] = 1.0         # Canal de frutas\n",
    "state[2, veneno[0], veneno[1]] = 1.0       # Canal de venenos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebeb96d",
   "metadata": {},
   "source": [
    "**Arquitectura de la Red Neuronal**\n",
    "\n",
    "La red CNN_DQN, definida en agent.py, sigue una arquitectura sencilla pero eficaz:\n",
    "\n",
    "- 2 capas convolucionales para extraer características espaciales.\n",
    "\n",
    "- 2 capas densas para calcular el valor Q de cada acción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura de la red neuronal que actúa como el \"cerebro\" visual del agente.\n",
    "class CNN_DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs): [cite: 1279]\n",
    "        super(CNN_DQN, self).__init__()\n",
    "        # Capas convolucionales para \"ver\" el tabler\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Cálculo del tamaño para la capa lineal\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        # Capas lineales para \"decidir\" el valor de cada acción\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff9073",
   "metadata": {},
   "source": [
    "Esta red recibe como entrada el estado en forma de tensor y devuelve un vector de valores Q (uno por acción)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7d6da",
   "metadata": {},
   "source": [
    "**Componentes del Agente DQN**\n",
    "\n",
    "El agente implementa técnicas avanzadas del algoritmo DQN según la arquitectura de Mnih et al. (2015):\n",
    "\n",
    "a) Red principal y red objetivo\n",
    "- La red principal es la que se entrena activamente.\n",
    "\n",
    "- La red objetivo se actualiza cada cierto número de pasos (update_target_every) para estabilizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273666d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595056bc",
   "metadata": {},
   "source": [
    "b) Memoria de Replay\n",
    "\n",
    "El agente almacena transiciones pasadas en una cola circular (deque), desde donde extrae mini-batches para entrenamiento aleatorio. Esto rompe la correlación temporal entre muestras y mejora la estabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.memory = deque(maxlen=20000)\n",
    "self.memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bedf73",
   "metadata": {},
   "source": [
    "c) Política Epsilon-Greedy\n",
    "\n",
    "Durante el entrenamiento, el agente decide sus acciones con una política que balancea exploración y explotación. Con probabilidad ϵ, elige una acción aleatoria; de lo contrario, toma la acción con mayor valor Q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ccf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore and np.random.rand() <= self.epsilon:\n",
    "    return random.randrange(self.action_size)\n",
    "...\n",
    "action_values = self.model(state_tensor)\n",
    "return np.argmax(action_values.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4613b0",
   "metadata": {},
   "source": [
    "El valor de ϵ decae progresivamente para favorecer la explotación del conocimiento adquirido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a768f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.epsilon > self.epsilon_min:\n",
    "    self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d172a9c",
   "metadata": {},
   "source": [
    "**Proceso de Entrenamiento**\n",
    "\n",
    "El agente entrena su red principal mediante retropropagación en lotes aleatorios (batch_size) tomados de la memoria de replay. Cada lote actualiza la red usando la pérdida entre los valores actuales y los objetivos (targets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo del valor objetivo (target Q)\n",
    "target_q_values = rewards + (gamma * next_q_values * (~dones))\n",
    "loss = criterion(current_q_values, target_q_values)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c60742",
   "metadata": {},
   "source": [
    "El uso de torch.no_grad() para calcular los targets evita que se acumulen gradientes en la red objetivo.\n",
    "\n",
    "**Interfaz Visual e Interacción**\n",
    "\n",
    "El entorno visual fue implementado en main.py usando Pygame. Incluye dos modos:\n",
    "\n",
    "- SETUP: permite al usuario colocar frutas y venenos manualmente en el tablero.\n",
    "\n",
    "- RUN: el agente entrenado toma el control y ejecuta su política aprendida, visualizando su comportamiento en tiempo real.\n",
    "\n",
    "Esto permitió una validación cualitativa del rendimiento del agente en diferentes configuraciones.\n",
    "\n",
    "**Resultados y Observaciones**\n",
    "\n",
    "- El uso de CNN permitió al agente generalizar mejor en diferentes escenarios.\n",
    "\n",
    "- El sistema de reward shaping (recompensas por acercarse a frutas) mejoró la eficiencia del entrenamiento.\n",
    "\n",
    "- La representación como imagen multicanal ayudó a la red a reconocer patrones espaciales útiles para la navegación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe144e4d",
   "metadata": {},
   "source": [
    "#### 2.3 DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac89c59",
   "metadata": {},
   "source": [
    "**Problemas con DQN clásico**\n",
    "\n",
    "Aunque Deep Q-Networks (DQN) mejoró la generalización del agente frente al Q-Learning tradicional, aún sufría de un problema conocido como overestimation bias: la tendencia a sobrevalorar las recompensas futuras. Esto ocurre porque DQN utiliza la misma red para seleccionar y evaluar las acciones futuras, lo que puede introducir errores acumulativos.\n",
    "\n",
    "Para mitigar este problema, se implementó el algoritmo Double DQN (DDQN), una mejora propuesta por Van Hasselt et al. (2016), que desacopla el proceso de selección de acción del de evaluación de valor. Esto reduce el sesgo y mejora la estabilidad del entrenamiento.\n",
    "\n",
    "**Diferencia Clave: Selección vs Evaluación**\n",
    "\n",
    "En DQN clásico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = reward + gamma * max(Q_target(next_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e858a8",
   "metadata": {},
   "source": [
    "En Double DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4587da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección con red principal\n",
    "next_actions = model(next_state).argmax(dim=1)\n",
    "\n",
    "# Evaluación con red objetivo\n",
    "next_q = target_model(next_state).gather(1, next_actions.unsqueeze(1))\n",
    "target = reward + gamma * next_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343fa13",
   "metadata": {},
   "source": [
    "Este cambio sutil mejora notablemente la precisión de la estimación de los valores Q futuros.\n",
    "\n",
    "**Implementación en el Proyecto**\n",
    "\n",
    "La clase Agent en agent.py fue modificada para aplicar el criterio Double DQN en el método replay(). La arquitectura de la red se mantuvo similar al agente DQN, pero se ajustó la forma en que se calcula el valor objetivo durante el entrenamiento.\n",
    "\n",
    "Fragmento clave del método replay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb594ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Selección con la red principal\n",
    "next_actions = self.model(next_states).detach().argmax(dim=1).unsqueeze(1)\n",
    "\n",
    "# 2. Evaluación con la red objetivo\n",
    "next_q_values = self.target_model(next_states).gather(1, next_actions)\n",
    "\n",
    "# 3. Cálculo de target\n",
    "targets = rewards + gamma * next_q_values * (1 - dones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9413b2",
   "metadata": {},
   "source": [
    "**Otras Mejoras y Técnicas Usadas**\n",
    "\n",
    "Además del cambio en el cálculo de los valores objetivos, el agente DDQN mantiene:\n",
    "\n",
    "- Una política epsilon-greedy con decaimiento gradual.\n",
    "\n",
    "- Replay Buffer circular para almacenar transiciones de entrenamiento.\n",
    "\n",
    "- Actualización periódica de la red objetivo para estabilizar el aprendizaje.\n",
    "\n",
    "- Entrenamiento mini-batch con retropropagación en train.py.\n",
    "\n",
    "**Interfaz Interactiva**\n",
    "\n",
    "La demostración del agente DDQN se integró a través de main.py, mientras que el entrenamiento puede ser gestionado desde interfaztrain.py. La representación visual de frutas, venenos y el agente permite observar cómo la política aprendida se adapta al entorno.\n",
    "\n",
    "**Resultados Obtenidos**\n",
    "\n",
    "Durante el entrenamiento de DDQN, se observó una mejora significativa en:\n",
    "\n",
    "- Estabilidad del aprendizaje: menos oscilaciones durante el entrenamiento.\n",
    "\n",
    "- Precisión en la toma de decisiones: el agente evitó más venenos sin necesidad de aumentar la exploración.\n",
    "\n",
    "- Generalización: el agente entrenado pudo actuar correctamente en escenarios no vistos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4021b",
   "metadata": {},
   "source": [
    "#### 2.4. Paradigma Alternativo: Algoritmos Genéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a90e4b",
   "metadata": {},
   "source": [
    "**Motivación**\n",
    "\n",
    "A diferencia de los algoritmos anteriores que dependen de gradientes y retropropagación (como Q-Learning o DQN), el enfoque evolutivo busca optimizar directamente las políticas de los agentes mediante la simulación de un proceso de selección natural. Esta técnica resulta especialmente útil cuando se trabaja con entornos ruidosos o no diferenciables, donde las funciones de pérdida tradicionales pueden fallar o llevar a soluciones subóptimas.\n",
    "\n",
    "**Fundamentos del Algoritmo Genético**\n",
    "\n",
    "El algoritmo genético implementado se basa en los principios fundamentales de la evolución biológica:\n",
    "\n",
    "1. Inicialización: Se genera una población aleatoria de agentes, cada uno con una red neuronal que actúa como su \"ADN\".\n",
    "\n",
    "2. Evaluación (Fitness): Cada agente se enfrenta a un entorno dinámico y su desempeño se mide como la suma de recompensas obtenidas.\n",
    "\n",
    "3. Selección: Se eligen los agentes con mayor fitness para reproducirse.\n",
    "\n",
    "4. Cruzamiento (Crossover): Los genes de dos padres se combinan para formar un nuevo agente hijo.\n",
    "\n",
    "5. Mutación: Se introduce variación aleatoria en los genes del hijo.\n",
    "\n",
    "6. Elitismo y Reemplazo: Se conservan los mejores agentes y se forma una nueva generación con los hijos generados.\n",
    "\n",
    "Este ciclo se repite durante múltiples generaciones hasta que el agente evoluciona una política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70729105",
   "metadata": {},
   "source": [
    "**Implementación del Agente Genético**\n",
    "\n",
    "La arquitectura general del entrenamiento se encuentra en train_ga.py, y el comportamiento del agente está definido en agent_ga.py.\n",
    "\n",
    "**a) Estructura de la Población**\n",
    "\n",
    "Cada agente está representado por una red neuronal densa que recibe el estado del entorno y genera una acción discreta. Los pesos de esta red constituyen su genotipo. Inicialmente, los pesos son asignados aleatoriamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6158e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_population():\n",
    "    return [Agent() for _ in range(POPULATION_SIZE)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc1513",
   "metadata": {},
   "source": [
    "**b) Evaluación de Fitness**\n",
    "\n",
    "Cada agente es evaluado en un escenario aleatorio donde debe recolectar frutas y evitar venenos. Su desempeño se mide por la suma total de recompensas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    action = agent.choose_action(state)\n",
    "    state, reward, done = env.step(action)\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276b19d",
   "metadata": {},
   "source": [
    "Este total es almacenado como su fitness, y determina su posibilidad de ser seleccionado como padre.\n",
    "\n",
    "**c) Selección de Padres**\n",
    "\n",
    "Se selecciona el 20% de los mejores agentes de la población como candidatos para reproducción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446552b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "parents = population[:int(POPULATION_SIZE * 0.2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ca8dc",
   "metadata": {},
   "source": [
    "**d) Cruzamiento Uniforme**\n",
    "\n",
    "La recombinación de genes se realiza de manera uniforme: cada peso tiene una probabilidad del 50% de heredarse de cada padre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in child_genes.keys():\n",
    "    if random.random() < 0.5:\n",
    "        child_genes[key] = p1_genes[key].clone()\n",
    "    else:\n",
    "        child_genes[key] = p2_genes[key].clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf78bf2",
   "metadata": {},
   "source": [
    "**e) Mutación Gaussiana**\n",
    "\n",
    "Cada parámetro tiene una probabilidad de ser alterado añadiendo ruido gaussiano con desviación estándar 0.1, lo que permite explorar nuevas soluciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn_like(child_genes[key]) * 0.1\n",
    "child_genes[key] += noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3e3bc",
   "metadata": {},
   "source": [
    "#### 2.5 Solución Definitiva: Aprendizaje por Imitación y Currículo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596289e",
   "metadata": {},
   "source": [
    "El aprendizaje por imitación (Imitation Learning) es una técnica del aprendizaje automático en la cual un agente aprende a comportarse observando las decisiones de un experto. En lugar de aprender únicamente a través de la exploración y la retroalimentación de recompensas como en Q-learning o DQN, el agente intenta replicar el comportamiento observado, generalizando a partir de ejemplos ya resueltos.\n",
    "\n",
    "Este enfoque es útil en tareas donde una política experta está disponible o es más eficiente que explorar de forma aleatoria. El agente no necesita experimentar con acciones riesgosas o costosas, sino que puede aprender directamente de ejemplos correctos.\n",
    "\n",
    "**Arquitectura General**\n",
    "\n",
    "En este proyecto, el agente utiliza una red neuronal convolucional para mapear directamente una representación visual del entorno (estado) hacia una acción (arriba, abajo, izquierda o derecha). El entorno es una cuadrícula de 5x5 donde el agente debe aprender a alcanzar frutas y evitar venenos.\n",
    "\n",
    "La red se entrena como un clasificador supervisado, usando los datos generados por un experto planificador (algoritmo A*) que provee las acciones correctas para diferentes configuraciones del entorno.\n",
    "\n",
    "**Generación de Datos Expertos**\n",
    "\n",
    "La clase AStarSolver ubicada en a_star_solver.py es responsable de encontrar la ruta óptima entre el agente y las frutas. El archivo generate_data.py genera un conjunto de ejemplos estado-acción (dataset) mediante simulaciones automáticas con A*.\n",
    "\n",
    "Cada entrada del dataset contiene:\n",
    "\n",
    "- Un estado del entorno como tensor 3x5x5 (canales para frutas, venenos y posición del agente).\n",
    "\n",
    "- La acción óptima sugerida por A* en ese escenario.\n",
    "\n",
    "**Entrenamiento con Curriculum Learning**\n",
    "\n",
    "El entrenamiento se realiza mediante el archivo train_curriculum.py, que implementa un enfoque Curriculum Learning. Este método consiste en enseñar al agente desde situaciones simples hasta escenarios más complejos, de forma progresiva, para mejorar la estabilidad y la capacidad de generalización.\n",
    "\n",
    "El currículo consta de 4 niveles:\n",
    "| Lección | Nº Frutas | Dataset                   | Nº Épocas |\n",
    "|---------|-----------|---------------------------|-----------|\n",
    "| 1       | 1         | expert_data_1_fruit.pkl   | 25        |\n",
    "| 2       | 2         | expert_data_2_fruits.pkl  | 30        |\n",
    "| 3       | 3         | expert_data_3_fruits.pkl  | 40        |\n",
    "| 4       | 4         | expert_data_4_fruits.pkl  | 50        |\n",
    "\n",
    "Durante el entrenamiento, el modelo aprende a clasificar correctamente la mejor acción en cada estado. Se usa la función CrossEntropyLoss y el optimizador Adam.\n",
    "\n",
    "Fragmento clave del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.FloatTensor(np.array([item[0] for item in data]))   # Entradas\n",
    "actions = torch.LongTensor(np.array([item[1] for item in data]))   # Etiquetas\n",
    "\n",
    "outputs = model(states)                  # Inferencia\n",
    "loss = criterion(outputs, actions)       # Cálculo de pérdida\n",
    "loss.backward()                          # Retropropagación\n",
    "optimizer.step()                         # Actualización de pesos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670e8e0",
   "metadata": {},
   "source": [
    "**Demostración del Agente**\n",
    "\n",
    "Una vez entrenado, el modelo se guarda como imitacion_model.pth y se utiliza en el archivo main.py para la simulación interactiva. Esta interfaz permite al usuario:\n",
    "\n",
    "- Configurar frutas y venenos con el mouse.\n",
    "\n",
    "- Presionar la barra espaciadora para que el agente actúe automáticamente.\n",
    "\n",
    "- Visualizar el movimiento paso a paso del agente basado en inferencia con la red entrenada.\n",
    "\n",
    "Fragmento del ciclo principal en main.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d28ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.get_state()\n",
    "action = agent.choose_action(state)\n",
    "_, _, done = env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c3c4a",
   "metadata": {},
   "source": [
    "**Resultados Observados**\n",
    "\n",
    "- Rápida convergencia: El agente aprende en pocos minutos gracias a la guía del experto.\n",
    "\n",
    "- Comportamiento estable: Sigue rutas seguras hacia las frutas.\n",
    "\n",
    "- Alta precisión en escenarios nuevos, especialmente con 1 o 2 frutas.\n",
    "\n",
    "Sin embargo, en escenarios con múltiples frutas o venenos muy cercanos, puede cometer errores, lo cual indica una limitación del entrenamiento supervisado frente a la necesidad de adaptación en entornos dinámicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a9c28",
   "metadata": {},
   "source": [
    "#### 2.6 Jugador humano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaac33f",
   "metadata": {},
   "source": [
    "Como parte del proceso de evaluación comparativa, se implementó una versión del entorno que permite la interacción directa de un ser humano. Este enfoque no se basa en técnicas de aprendizaje automático, sino en la observación y toma de decisiones por parte de un usuario humano. Esta modalidad tiene dos propósitos principales:\n",
    "\n",
    "1. Establecer una línea base humana para comparar el rendimiento de los agentes entrenados.\n",
    "\n",
    "2. Permitir una evaluación cualitativa de la dificultad del entorno desde la perspectiva de un jugador humano.\n",
    "\n",
    "**Implementación**\n",
    "\n",
    "La implementación está contenida en el archivo Humano.py, que define una interfaz visual e interactiva con el entorno. Se utiliza la biblioteca pygame para permitir el control del agente mediante el teclado. El usuario puede mover al agente en tiempo real con las teclas de dirección y observar el entorno completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee67918",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in pygame.event.get():\n",
    "    if event.type == pygame.KEYDOWN:\n",
    "        if event.key == pygame.K_UP:\n",
    "            env.move_agent(0, -1)\n",
    "        elif event.key == pygame.K_DOWN:\n",
    "            env.move_agent(0, 1)\n",
    "        elif event.key == pygame.K_LEFT:\n",
    "            env.move_agent(-1, 0)\n",
    "        elif event.key == pygame.K_RIGHT:\n",
    "            env.move_agent(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f9676",
   "metadata": {},
   "source": [
    "Cada vez que el jugador presiona una tecla:\n",
    "\n",
    "- El agente se mueve en la dirección correspondiente.\n",
    "\n",
    "- El entorno se actualiza gráficamente.\n",
    "\n",
    "- Se calcula si ha recolectado una fruta o caído en veneno.\n",
    "\n",
    "También se muestra en pantalla el puntaje actual, el número de frutas recolectadas y si el juego ha terminado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f66c8",
   "metadata": {},
   "source": [
    "**Experiencia de Usuario**\n",
    "\n",
    "Durante la ejecución, se observó que el control humano resulta efectivo en escenarios simples, pero tiende a:\n",
    "\n",
    "- Perder eficiencia en situaciones con múltiples venenos.\n",
    "\n",
    "- Reaccionar más lentamente que los agentes automatizados entrenados.\n",
    "\n",
    "- Cometer errores de juicio, especialmente cuando la ubicación de los objetos requiere planificación anticipada.\n",
    "\n",
    "Estas observaciones refuerzan la importancia de los algoritmos de planificación y aprendizaje, ya que el ser humano suele actuar de forma reactiva más que estratégica.\n",
    "\n",
    "**Comparación y Valor**\n",
    "\n",
    "Aunque el agente humano no constituye una estrategia de inteligencia artificial, su desempeño sirve como punto de referencia:\n",
    "\n",
    "- Permite validar si el entorno es razonablemente jugable.\n",
    "\n",
    "- Ayuda a calibrar la dificultad de los escenarios utilizados en el entrenamiento de los agentes.\n",
    "\n",
    "- Ofrece datos reales para diseñar modelos de imitación, en caso de querer capturar acciones humanas como dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a5bfd",
   "metadata": {},
   "source": [
    "### 3. Resultados y Conclusión\n",
    "El viaje a través de estos diferentes paradigmas de IA fue revelador. Mientras que los enfoques de Aprendizaje por Refuerzo y Algoritmos Genéticos mostraron potencial, también exhibieron dificultades para converger de manera consistente en un entorno tan \"frágil\", donde un solo error puede llevar al fracaso.\n",
    "\n",
    "La estrategia de Aprendizaje por Imitación combinada con el Aprendizaje por Currículo demostró ser la más efectiva y robusta. Al aprender de un experto perfecto y hacerlo de manera gradual, el agente final fue capaz de generalizar su conocimiento y resolver de manera confiable tanto escenarios simples como complejos. Para la demostración visual, se desarrolló una interfaz en Pygame que permite a los usuarios configurar sus propios niveles y observar al agente entrenado resolverlos en tiempo real.\n",
    "\n",
    "Este proyecto subraya una lección fundamental en el desarrollo de IA: a menudo, la estrategia de entrenamiento y la calidad de los datos son tan o más importantes que la elección del algoritmo en sí. El resultado es un agente competente y una profunda comprensión práctica de los desafíos y soluciones en el campo del Machine Learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
