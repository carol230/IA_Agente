{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803cfcb5",
   "metadata": {},
   "source": [
    "# Taller sobre clustering aglomerativo y DBSCAN\n",
    "# Agente Come Frutas\n",
    "\n",
    "## Integrantes: Ayala Ivonne, Cumbal Mateo, Garcés Boris, Morales David, Pereira Alicia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512b684",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### Introducción\n",
    "El presente informe detalla el proceso de diseño, implementación y experimentación para el desarrollo de un agente autónomo de Inteligencia Artificial en el marco del proyecto \"Agente Come-Frutas\". El desafío se centra en un problema clásico de toma de decisiones secuenciales en un entorno dinámico y con riesgos, sirviendo como un caso de estudio práctico para la aplicación de técnicas avanzadas de Machine Learning.\n",
    "\n",
    "El problema consiste en un entorno de rejilla de 5x5 en el que un agente debe aprender a navegar de manera eficiente. El objetivo principal es maximizar una puntuación recolectando \"frutas\" (que otorgan recompensas positivas) y evitando \"venenos\" (que imponen castigos negativos). La meta final es desarrollar una política de comportamiento óptima que permita al agente limpiar el tablero de todas las frutas, garantizando su supervivencia al esquivar todos los venenos presentes.\n",
    "\n",
    "Para alcanzar este objetivo, el proyecto transitó por un riguroso proceso de experimentación, explorando múltiples paradigmas de la Inteligencia Artificial. Se inició con un enfoque en el Aprendizaje por Refuerzo (Reinforcement Learning), implementando y depurando algoritmos de vanguardia como Deep Q-Networks (DQN) y su variante mejorada, Double DQN (DDQN).\n",
    "\n",
    "Frente a los desafíos clásicos de convergencia y estabilidad inherentes a RL, la investigación se expandió para incluir otras estrategias. Se exploraron los Algoritmos Genéticos, un enfoque basado en principios de evolución, y el Aprendizaje por Imitación, una potente técnica de aprendizaje supervisado que requirió el desarrollo de un \"oráculo\" experto basado en el algoritmo de búsqueda A*.\n",
    "\n",
    "A continuación, se presenta el código documentado de la implementación final, reflejando la culminación de este profundo y multifacético proceso de desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9755b",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90092b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONSTANTES DE CONFIGURACIÓN DEL JUEGO Y LA PANTALLA ---\n",
    "# Estas constantes definen el tamaño del tablero, de cada celda y de la ventana del juego.\n",
    "GRID_WIDTH = 5\n",
    "GRID_HEIGHT = 5\n",
    "CELL_SIZE = 120\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE\n",
    "\n",
    "# Colores (formato RGB) para los elementos de la interfaz.\n",
    "COLOR_FONDO = (25, 25, 25)\n",
    "COLOR_LINEAS = (40, 40, 40)\n",
    "COLOR_AGENTE = (60, 100, 255)\n",
    "COLOR_PARED = (80, 80, 80)\n",
    "COLOR_TEXTO = (230, 230, 230)\n",
    "COLOR_CURSOR = (255, 255, 0)\n",
    "\n",
    "# --- PARÁMETROS DEL APRENDIZAJE POR REFUERZO (Q-LEARNING) ---\n",
    "# Estos son los \"hiperparámetros\" que controlan cómo aprende el agente.\n",
    "RECOMPENSA_FRUTA = 100         # Puntuación alta por encontrar una fruta.\n",
    "CASTIGO_VENENO = -100           # Castigo fuerte por tocar un veneno.\n",
    "RECOMPENSA_MOVIMIENTO = -0.1    # Pequeño castigo por cada movimiento para incentivar la eficiencia.\n",
    "ALPHA = 0.1                     # Tasa de aprendizaje.\n",
    "GAMMA = 0.9                     # Factor de descuento.\n",
    "EPSILON = 1.0                   # Tasa de exploración inicial.\n",
    "EPSILON_DECAY = 0.9995          # Factor de decaimiento de epsilon.\n",
    "MIN_EPSILON = 0.01              # Mínima tasa de exploración.\n",
    "NUM_EPISODIOS_ENTRENAMIENTO = 20000 # Número de partidas que jugará el agente para aprender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cae4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLASE DEL AGENTE ---\n",
    "# Define el \"cerebro\" del agente. Contiene la Tabla Q y la lógica para aprender y decidir.\n",
    "class AgenteQLearning:\n",
    "    def __init__(self, num_estados, num_acciones):\n",
    "        self.num_acciones = num_acciones\n",
    "        # La Tabla Q es una matriz que almacena el \"valor\" de cada acción en cada estado posible.\n",
    "        # Aquí, el estado está definido por la posición (x, y) del agente en el tablero.\n",
    "        self.q_table = np.zeros((num_estados[0], num_estados[1], num_acciones))\n",
    "        self.epsilon = EPSILON  # Tasa de exploración (curiosidad).\n",
    "\n",
    "    def elegir_accion(self, estado):\n",
    "        \"\"\"Decide qué acción tomar usando la estrategia epsilon-greedy.\"\"\"\n",
    "        # Con probabilidad epsilon, toma una acción aleatoria (exploración).\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randint(0, self.num_acciones - 1)\n",
    "        # De lo contrario, elige la mejor acción conocida según la Tabla Q (explotación).\n",
    "        else:\n",
    "            return np.argmax(self.q_table[estado])\n",
    "\n",
    "    def actualizar_q_table(self, estado, accion, recompensa, nuevo_estado):\n",
    "        \"\"\"Actualiza el valor en la Tabla Q usando la fórmula de Bellman.\"\"\"\n",
    "        valor_antiguo = self.q_table[estado][accion]\n",
    "        # El valor futuro es el máximo valor Q que se puede obtener desde el nuevo estado.\n",
    "        valor_futuro_maximo = np.max(self.q_table[nuevo_estado])\n",
    "        \n",
    "        # Fórmula de Q-Learning: se actualiza el valor antiguo basado en la recompensa\n",
    "        # obtenida y el valor futuro esperado.\n",
    "        nuevo_q = valor_antiguo + ALPHA * (\n",
    "            recompensa + GAMMA * valor_futuro_maximo - valor_antiguo\n",
    "        )\n",
    "        self.q_table[estado][accion] = nuevo_q\n",
    "\n",
    "    def decaimiento_epsilon(self):\n",
    "        \"\"\"Reduce gradualmente el valor de epsilon para pasar de explorar a explotar.\"\"\"\n",
    "        if self.epsilon > MIN_EPSILON:\n",
    "            self.epsilon *= EPSILON_DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLASE DEL ENTORNO ---\n",
    "# Define las reglas del juego, el tablero y cómo interactúa el agente con él.\n",
    "class EntornoGrid:\n",
    "    def __init__(self):\n",
    "        self.agente_pos = (0, 0)\n",
    "        # Usamos 'sets' para un manejo eficiente de las posiciones de los objetos.\n",
    "        self.frutas = set()\n",
    "        self.venenos = set()\n",
    "        self.paredes = set()\n",
    "        self.reset_a_configuracion_inicial()\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"Resetea la posición del agente al inicio (esquina superior izquierda).\"\"\"\n",
    "        self.agente_pos = (0, 0)\n",
    "        return self.agente_pos\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"Elimina todos los objetos del tablero.\"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion, modo_juego):\n",
    "        \"\"\"\n",
    "        Ejecuta un paso en el juego.\n",
    "        El agente toma una 'accion' y el entorno devuelve el 'nuevo_estado',\n",
    "        la 'recompensa' y si el juego ha 'terminado'.\n",
    "        \"\"\"\n",
    "        x, y = self.agente_pos\n",
    "        # Acciones: 0=arriba, 1=abajo, 2=izquierda, 3=derecha\n",
    "        if accion == 0: y -= 1\n",
    "        elif accion == 1: y += 1\n",
    "        elif accion == 2: x -= 1\n",
    "        elif accion == 3: x += 1\n",
    "\n",
    "        # Comprueba si el movimiento es válido (dentro de los límites y no choca con una pared).\n",
    "        if (x < 0 or x >= GRID_WIDTH or y < 0 or y >= GRID_HEIGHT or (x, y) in self.paredes):\n",
    "            # Si el movimiento es inválido, el agente no se mueve y recibe una pequeña penalización.\n",
    "            return self.agente_pos, RECOMPENSA_MOVIMIENTO, False\n",
    "\n",
    "        # Actualiza la posición del agente si el movimiento es válido.\n",
    "        self.agente_pos = (x, y)\n",
    "        nuevo_estado = self.agente_pos\n",
    "        terminado = False\n",
    "\n",
    "        if nuevo_estado in self.frutas:\n",
    "            recompensa = RECOMPENSA_FRUTA\n",
    "            self.frutas.remove(nuevo_estado)\n",
    "            # El episodio solo termina con éxito si ya no quedan más frutas.\n",
    "            if not self.frutas:\n",
    "                terminado = True\n",
    "        elif nuevo_estado in self.venenos:\n",
    "            recompensa = CASTIGO_VENENO\n",
    "            terminado = True  # Tocar un veneno siempre termina el juego.\n",
    "        else:\n",
    "            recompensa = RECOMPENSA_MOVIMIENTO\n",
    "\n",
    "        return nuevo_estado, recompensa, terminado\n",
    "\n",
    "    def dibujar(self, pantalla, modo_juego, cursor_pos, img_fruta, img_veneno, img_pared, img_agente):\n",
    "        \"\"\"Dibuja todos los elementos del juego en la pantalla.\"\"\"\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "\n",
    "        # Dibuja la cuadrícula.\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibuja las paredes, frutas, venenos y el agente usando sus imágenes.\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "        pantalla.blit(img_agente, (self.agente_pos[0] * CELL_SIZE, self.agente_pos[1] * CELL_SIZE))\n",
    "\n",
    "        # En modo SETUP, dibuja un cursor para indicar dónde se colocarán los objetos.\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE, cursor_pos[1] * CELL_SIZE, CELL_SIZE, CELL_SIZE\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Dibuja la información de ayuda y el modo de juego actual en la parte inferior.\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        controles_setup = font.render(\n",
    "            \"SETUP: Mover con flechas. F=Fruta, V=Veneno, W=Pared. 'C' para limpiar.\", True, COLOR_TEXTO\n",
    "        )\n",
    "        controles_run = font.render(\n",
    "            \"'T' para Entrenar, 'P' para Jugar, 'S' para Setup.\", True, COLOR_TEXTO\n",
    "        )\n",
    "\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles_setup, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles_run, (10, SCREEN_HEIGHT + 55))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIÓN PRINCIPAL DEL JUEGO ---\n",
    "# Orquesta todo el juego: inicialización, bucle principal, manejo de eventos y modos.\n",
    "def main():\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente Come-Frutas 🍓 vs ☠️ (Q-Learning)\")\n",
    "\n",
    "    # --- Carga de imágenes ---\n",
    "    # Intenta cargar los archivos de imagen. Si no los encuentra, usa cuadrados de colores como respaldo.\n",
    "    try:\n",
    "        ruta_fruta = os.path.join(os.path.dirname(__file__), \"fruta.png\")\n",
    "        img_fruta_original = pygame.image.load(ruta_fruta).convert_alpha()\n",
    "        img_fruta = pygame.transform.scale(img_fruta_original, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        print(\"Advertencia: No se encontró 'fruta.png'. Se usará un cuadrado verde.\")\n",
    "        img_fruta = pygame.Surface((CELL_SIZE, CELL_SIZE)); img_fruta.fill((40, 200, 40))\n",
    "\n",
    "    try:\n",
    "        ruta_veneno = os.path.join(os.path.dirname(__file__), \"veneno.png\")\n",
    "        img_veneno_original = pygame.image.load(ruta_veneno).convert_alpha()\n",
    "        img_veneno = pygame.transform.scale(img_veneno_original, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        print(\"Advertencia: No se encontró 'veneno.png'. Se usará un cuadrado rojo.\")\n",
    "        img_veneno = pygame.Surface((CELL_SIZE, CELL_SIZE)); img_veneno.fill((255, 50, 50))\n",
    "\n",
    "    try:\n",
    "        ruta_pared = os.path.join(os.path.dirname(__file__), \"pared.png\")\n",
    "        img_pared_original = pygame.image.load(ruta_pared).convert_alpha()\n",
    "        img_pared = pygame.transform.scale(img_pared_original, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        print(\"Advertencia: No se encontró 'pared.png'. Se usará un cuadrado gris.\")\n",
    "        img_pared = pygame.Surface((CELL_SIZE, CELL_SIZE)); img_pared.fill(COLOR_PARED)\n",
    "\n",
    "    try:\n",
    "        ruta_agente = os.path.join(os.path.dirname(__file__), \"agente.png\")\n",
    "        img_agente_original = pygame.image.load(ruta_agente).convert_alpha()\n",
    "        img_agente = pygame.transform.scale(img_agente_original, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        print(\"Advertencia: No se encontró 'agente.png'. Se usará un cuadrado azul.\")\n",
    "        img_agente = pygame.Surface((CELL_SIZE, CELL_SIZE)); img_agente.fill(COLOR_AGENTE)\n",
    "\n",
    "    # Inicialización del entorno y el agente.\n",
    "    entorno = EntornoGrid()\n",
    "    agente = AgenteQLearning(num_estados=(GRID_HEIGHT, GRID_WIDTH), num_acciones=4)\n",
    "\n",
    "    reloj = pygame.time.Clock()\n",
    "    corriendo = True\n",
    "    modo_juego = \"SETUP\"  # El juego comienza en modo de configuración.\n",
    "    cursor_pos = [0, 0]\n",
    "\n",
    "    # Guarda la configuración inicial del tablero para poder resetearlo.\n",
    "    frutas_iniciales = entorno.frutas.copy()\n",
    "    venenos_iniciales = entorno.venenos.copy()\n",
    "\n",
    "    # --- BUCLE PRINCIPAL DEL JUEGO ---\n",
    "    while corriendo:\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            # Manejo de eventos de teclado para cambiar de modo y configurar el tablero.\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # --- MODO ENTRENAMIENTO (T) ---\n",
    "                if evento.key == pygame.K_t:\n",
    "                    if modo_juego != \"TRAINING\":\n",
    "                        print(\"--- INICIANDO ENTRENAMIENTO ---\")\n",
    "                        modo_juego = \"TRAINING\"\n",
    "                        # Crea un nuevo agente con una Tabla Q vacía.\n",
    "                        agente = AgenteQLearning(num_estados=(GRID_HEIGHT, GRID_WIDTH), num_acciones=4)\n",
    "                        # Muestra un mensaje de \"Entrenando...\" en pantalla.\n",
    "                        pantalla.fill(COLOR_FONDO)\n",
    "                        font = pygame.font.Font(None, 50)\n",
    "                        texto_entrenando = font.render(\"Entrenando...\", True, COLOR_TEXTO)\n",
    "                        rect = texto_entrenando.get_rect(center=(SCREEN_WIDTH / 2, SCREEN_HEIGHT / 2))\n",
    "                        pantalla.blit(texto_entrenando, rect)\n",
    "                        pygame.display.flip()\n",
    "\n",
    "                        # Bucle de entrenamiento principal.\n",
    "                        for episodio in range(NUM_EPISODIOS_ENTRENAMIENTO):\n",
    "                            # Resetea el entorno a la configuración definida en modo SETUP.\n",
    "                            entorno.frutas = frutas_iniciales.copy()\n",
    "                            entorno.venenos = venenos_iniciales.copy()\n",
    "                            estado = entorno.reset_a_configuracion_inicial()\n",
    "                            terminado = False\n",
    "                            \n",
    "                            # Bucle de una partida (episodio).\n",
    "                            while not terminado:\n",
    "                                accion = agente.elegir_accion(estado)\n",
    "                                nuevo_estado, recompensa, terminado = entorno.step(accion, \"TRAINING\")\n",
    "                                agente.actualizar_q_table(estado, accion, recompensa, nuevo_estado)\n",
    "                                estado = nuevo_estado\n",
    "                            \n",
    "                            # Reduce epsilon al final de cada episodio.\n",
    "                            agente.decaimiento_epsilon()\n",
    "                            \n",
    "                            # Imprime el progreso cada 1000 episodios.\n",
    "                            if (episodio + 1) % 1000 == 0:\n",
    "                                print(f\"Episodio: {episodio + 1}/{NUM_EPISODIOS_ENTRENAMIENTO}, Epsilon: {agente.epsilon:.4f}\")\n",
    "\n",
    "                        print(\"--- ENTRENAMIENTO COMPLETADO ---\")\n",
    "                        # Prepara el tablero para la demostración del agente ya entrenado.\n",
    "                        entorno.frutas = frutas_iniciales.copy()\n",
    "                        entorno.venenos = venenos_iniciales.copy()\n",
    "                        entorno.reset_a_configuracion_inicial()\n",
    "                        agente.epsilon = 0  # Modo experto: solo explotación, sin acciones aleatorias.\n",
    "                        modo_juego = \"PLAYING\"\n",
    "\n",
    "                # --- MODO JUEGO (P) ---\n",
    "                elif evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO JUEGO (AGENTE ENTRENADO) ---\")\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    # Resetea el tablero a la configuración inicial.\n",
    "                    entorno.frutas = frutas_iniciales.copy()\n",
    "                    entorno.venenos = venenos_iniciales.copy()\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    agente.epsilon = 0 # El agente usará su conocimiento sin explorar.\n",
    "\n",
    "                # --- MODO SETUP (S) ---\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # Lógica para configurar el tablero en modo SETUP.\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    if evento.key == pygame.K_UP: cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN: cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT: cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT: cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "                    \n",
    "                    pos_celda = tuple(cursor_pos)\n",
    "                    # Tecla F: Añade o quita una fruta.\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        if pos_celda in entorno.frutas: entorno.frutas.remove(pos_celda)\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos_celda)\n",
    "                            entorno.venenos.discard(pos_celda); entorno.paredes.discard(pos_celda)\n",
    "                    # Tecla V: Añade o quita un veneno.\n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        if pos_celda in entorno.venenos: entorno.venenos.remove(pos_celda)\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos_celda)\n",
    "                            entorno.frutas.discard(pos_celda); entorno.paredes.discard(pos_celda)\n",
    "                    # Tecla W: Añade o quita una pared.\n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        if pos_celda in entorno.paredes: entorno.paredes.remove(pos_celda)\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos_celda)\n",
    "                            entorno.frutas.discard(pos_celda); entorno.venenos.discard(pos_celda)\n",
    "                    # Tecla C: Limpia el tablero.\n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        print(\"--- TABLERO LIMPIO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "                    \n",
    "                    # Actualiza la configuración inicial guardada cada vez que se hace un cambio.\n",
    "                    frutas_iniciales = entorno.frutas.copy()\n",
    "                    venenos_iniciales = entorno.venenos.copy()\n",
    "\n",
    "        # --- Lógica de juego que se ejecuta en cada frame ---\n",
    "        # Lógica del juego en modo PLAYING: el agente toma decisiones.\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            if entorno.frutas:\n",
    "                estado = entorno.agente_pos\n",
    "                accion = agente.elegir_accion(estado)\n",
    "                _, _, terminado = entorno.step(accion, \"PLAYING\")\n",
    "                if terminado:\n",
    "                    if not entorno.frutas:\n",
    "                        print(\"¡Todas las frutas recolectadas! Volviendo a modo SETUP.\")\n",
    "                    else:\n",
    "                        print(\"Juego terminado (veneno). Volviendo a modo SETUP.\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "                time.sleep(0.1) # Pequeña pausa para ver el movimiento del agente.\n",
    "            else:\n",
    "                modo_juego = \"SETUP\"\n",
    "                print(\"No hay frutas en el tablero. Volviendo a modo SETUP.\")\n",
    "        \n",
    "        # Dibuja la pantalla en cada frame, excepto durante el entrenamiento.\n",
    "        if modo_juego != \"TRAINING\":\n",
    "            entorno.dibujar(pantalla, modo_juego, tuple(cursor_pos), img_fruta, img_veneno, img_pared, img_agente)\n",
    "            pygame.display.flip()\n",
    "\n",
    "        reloj.tick(60) # Limita el juego a 60 frames por segundo.\n",
    "\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto de entrada del programa.\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb0971",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff248b",
   "metadata": {},
   "source": [
    "#### agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c03c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\"\"\"\n",
    "Implementación del agente DQN (Deep Q-Network) con arquitectura CNN.\n",
    "\n",
    "Este módulo contiene la implementación completa del algoritmo DQN, incluyendo:\n",
    "- Red neuronal convolucional para procesamiento de estados espaciales.\n",
    "- Sistema de memoria de replay para entrenamiento estable.\n",
    "- Estrategia epsilon-greedy para balancear exploración/explotación.\n",
    "- Red objetivo para estabilizar el cálculo de los valores Q.\n",
    "- Optimización con el optimizador Adam.\n",
    "\n",
    "El agente está diseñado específicamente para problemas de navegación en grillas\n",
    "donde el estado se representa como imágenes multi-canal, aprovechando las\n",
    "capacidades de las CNNs para reconocer patrones espaciales.\n",
    "\n",
    "Características principales:\n",
    "- Arquitectura CNN optimizada para grillas pequeñas.\n",
    "- Memoria de replay para descorrelacionar experiencias.\n",
    "- Actualización periódica de la red objetivo.\n",
    "- Técnicas de estabilización (gradient clipping, target network).\n",
    "- Sistema de guardado/carga de modelos entrenados.\n",
    "\n",
    "Referencias:\n",
    "- DQN: Mnih et al. (2015) \"Human-level control through deep reinforcement learning\"\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. RED NEURONAL CONVOLUCIONAL PARA DQN ---\n",
    "class CNN_DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal convolucional optimizada para Q-learning en entornos de grilla.\n",
    "    \n",
    "    Esta arquitectura está diseñada para procesar estados representados como\n",
    "    tensores 3D (canales x altura x anchura).\n",
    "    \n",
    "    Arquitectura:\n",
    "    1. **Capas Convolucionales**: Para extraer características espaciales.\n",
    "       - Conv1: 3->16 canales.\n",
    "       - Conv2: 16->32 canales.\n",
    "    \n",
    "    2. **Capas Completamente Conectadas**: Para tomar decisiones basadas en las características.\n",
    "       - FC1: 256 neuronas.\n",
    "       - FC2: Salida de valores Q para cada acción.\n",
    "    \n",
    "    Args:\n",
    "        h (int): Altura de la grilla de entrada.\n",
    "        w (int): Anchura de la grilla de entrada.\n",
    "        outputs (int): Número de acciones posibles.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(CNN_DQN, self).__init__()\n",
    "        \n",
    "        # --- CAPAS CONVOLUCIONALES ---\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # --- CÁLCULO DINÁMICO DEL TAMAÑO DE CARACTERÍSTICAS ---\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        # --- CAPAS COMPLETAMENTE CONECTADAS ---\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante de la red.\n",
    "        \n",
    "        Procesa el estado de entrada a través de las capas para generar\n",
    "        valores Q para cada acción posible.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Estado de entrada con forma (batch, 3, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Valores Q para cada acción con forma (batch, num_actions).\n",
    "        \"\"\"\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    \n",
    "# --- 2. AGENTE DQN CON MEMORIA DE REPLAY Y RED OBJETIVO ---\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agente de aprendizaje por refuerzo que implementa el algoritmo DQN.\n",
    "    \n",
    "    Este agente combina varias técnicas clave de deep reinforcement learning:\n",
    "    \n",
    "    **Componentes principales:**\n",
    "    1. **Red Principal**: Se entrena activamente y decide las acciones.\n",
    "    2. **Red Objetivo**: Una copia de la red principal que se actualiza lentamente,\n",
    "       proporcionando targets estables para el entrenamiento y reduciendo oscilaciones.\n",
    "    3. **Memoria de Replay**: Almacena experiencias para un aprendizaje más estable.\n",
    "    4. **Estrategia Epsilon-Greedy**: Balancea entre explorar el entorno y explotar el conocimiento.\n",
    "    \n",
    "    Args:\n",
    "        state_shape (tuple): Forma del estado (canales, altura, anchura).\n",
    "        action_size (int): Número de acciones posibles en el entorno.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_shape, action_size):\n",
    "        # --- CONFIGURACIÓN BÁSICA ---\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # --- MEMORIA DE REPLAY ---\n",
    "        # Almacena tuplas de (estado, acción, recompensa, siguiente_estado, terminado).\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        \n",
    "        # --- HIPERPARÁMETROS DE APRENDIZAJE ---\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.learning_rate = 0.0001\n",
    "        self.update_target_every = 5\n",
    "        \n",
    "        # --- INICIALIZACIÓN DE REDES NEURONALES ---\n",
    "        h, w = state_shape[1], state_shape[2]\n",
    "        self.model = CNN_DQN(h, w, action_size)\n",
    "        self.target_model = CNN_DQN(h, w, action_size)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # --- CONFIGURACIÓN DE OPTIMIZACIÓN ---\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Actualiza la red objetivo copiando los pesos de la red principal.\n",
    "        \n",
    "        Esta operación es fundamental en DQN para mantener los targets estables\n",
    "        durante el entrenamiento, evitando que el objetivo cambie en cada paso.\n",
    "        \"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Almacena una experiencia en la memoria de replay.\n",
    "        \n",
    "        Esto permite al agente aprender de un conjunto de experiencias pasadas y\n",
    "        no correlacionadas, lo que estabiliza el entrenamiento.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado actual.\n",
    "            action (int): Acción tomada.\n",
    "            reward (float): Recompensa recibida.\n",
    "            next_state (np.array): Estado resultante.\n",
    "            done (bool): True si el episodio terminó.\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, explore=True):\n",
    "        \"\"\"\n",
    "        Selecciona una acción usando la estrategia epsilon-greedy.\n",
    "        \n",
    "        - **Exploración**: Con probabilidad epsilon, elige una acción al azar.\n",
    "        - **Explotación**: Con probabilidad 1-epsilon, elige la mejor acción según la red.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado actual del entorno.\n",
    "            explore (bool): Permite la exploración. Poner en False para la demostración.\n",
    "        \n",
    "        Returns:\n",
    "            int: La acción seleccionada.\n",
    "        \"\"\"\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state_tensor)\n",
    "        \n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Entrena la red neuronal usando un lote de experiencias de la memoria.\n",
    "        \n",
    "        Este es el núcleo del algoritmo de aprendizaje DQN.\n",
    "        \n",
    "        Proceso:\n",
    "        1. Muestrear un lote (batch) aleatorio de experiencias.\n",
    "        2. Calcular los valores Q actuales (predicciones) con la red principal.\n",
    "        3. Calcular los valores Q objetivo (targets) usando la red objetivo.\n",
    "        4. Optimizar la red principal para minimizar la diferencia entre predicciones y targets.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Número de experiencias a usar para el entrenamiento.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array([e[0] for e in minibatch]))\n",
    "        actions = torch.LongTensor([e[1] for e in minibatch]).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor([e[2] for e in minibatch]).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(np.array([e[3] for e in minibatch]))\n",
    "        dones = torch.BoolTensor([e[4] for e in minibatch]).unsqueeze(1)\n",
    "\n",
    "        current_q_values = self.model(states).gather(1, actions)\n",
    "        \n",
    "        # --- CÁLCULO DEL TARGET SEGÚN DQN ---\n",
    "        # La red objetivo calcula el valor máximo del siguiente estado.\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # Ecuación de Bellman para el target: R + gamma * max_Q(s', a')\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "        \n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping para prevenir gradientes explosivos y estabilizar.\n",
    "        torch.nn.utils.clip_grad_value_(self.model.parameters(), 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decaimiento de epsilon para reducir la exploración.\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def load(self, name):\n",
    "        \"\"\"\n",
    "        Carga los pesos de un modelo entrenado desde un archivo.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Ruta al archivo de pesos del modelo (.pth).\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "        self.update_target_network()\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Guarda los pesos del modelo actual en un archivo.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Ruta donde se guardará el archivo de pesos (.pth).\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c56236",
   "metadata": {},
   "source": [
    "#### dqn_agente_comefrutas.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn_agente_comefrutas.py\n",
    "\"\"\"\n",
    "Interfaz gráfica de demostración para agente DQN (Deep Q-Network).\n",
    "\n",
    "Este módulo proporciona una interfaz visual interactiva para demostrar el\n",
    "comportamiento de un agente DQN entrenado en el problema de recolección de frutas.\n",
    "A diferencia del DDQN, esta implementación utiliza DQN clásico con una sola red.\n",
    "\n",
    "Características principales:\n",
    "- Interfaz de configuración interactiva para crear escenarios personalizados\n",
    "- Visualización en tiempo real del comportamiento del agente entrenado\n",
    "- Sistema de dos modos: configuración (SETUP) y ejecución (PLAYING)\n",
    "- Compatibilidad con modelos DQN preentrenados\n",
    "- Interfaz de usuario intuitiva con controles de teclado y mouse\n",
    "\n",
    "El sistema está diseñado para:\n",
    "- Demostraciones educativas del comportamiento de IA\n",
    "- Validación visual del rendimiento del agente\n",
    "- Experimentación rápida con diferentes configuraciones de entorno\n",
    "- Evaluación cualitativa de estrategias aprendidas\n",
    "\n",
    "Diferencias con DDQN:\n",
    "- Utiliza una sola red neuronal (no red objetivo separada)\n",
    "- Implementación más simple del algoritmo Q-learning\n",
    "- Compatible con modelos entrenados usando DQN clásico\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from agent import Agent\n",
    "\n",
    "# --- CONFIGURACIÓN DE LA INTERFAZ VISUAL ---\n",
    "\"\"\"Parámetros de configuración para la ventana y visualización.\"\"\"\n",
    "GRID_WIDTH = 5          # Ancho de la grilla en número de celdas\n",
    "GRID_HEIGHT = 5         # Alto de la grilla en número de celdas\n",
    "CELL_SIZE = 120         # Tamaño de cada celda en píxeles (120x120)\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la ventana (600px)\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto total de la ventana (600px)\n",
    "\n",
    "# --- ESQUEMA DE COLORES ---\n",
    "\"\"\"Paleta de colores para una interfaz moderna y legible.\"\"\"\n",
    "COLOR_FONDO = (25, 25, 25)        # Fondo oscuro para reducir fatiga visual\n",
    "COLOR_LINEAS = (40, 40, 40)       # Líneas de grilla sutiles\n",
    "COLOR_CURSOR = (255, 255, 0)      # Cursor amarillo brillante para visibilidad\n",
    "COLOR_TEXTO = (230, 230, 230)     # Texto claro sobre fondo oscuro\n",
    "\n",
    "\n",
    "class EntornoGrid:\n",
    "    \"\"\"\n",
    "    Entorno de grilla especializado para demostración de agentes DQN.\n",
    "    \n",
    "    Esta clase maneja tanto la lógica del entorno como su representación visual,\n",
    "    proporcionando una plataforma completa para demostrar el comportamiento de\n",
    "    agentes DQN entrenados en problemas de navegación y recolección.\n",
    "    \n",
    "    Características del entorno:\n",
    "    - Grilla bidimensional con elementos configurables\n",
    "    - Gestión de colisiones y límites\n",
    "    - Sistema de recompensas integrado\n",
    "    - Representación visual con Pygame\n",
    "    - Compatibilidad con formato de estado DQN (tensor 3D)\n",
    "    \n",
    "    Elementos del entorno:\n",
    "    - Agente: Entidad controlada por IA que debe recolectar frutas\n",
    "    - Frutas: Objetivos que otorgan recompensas positivas\n",
    "    - Venenos: Obstáculos que causan penalizaciones y reseteo\n",
    "    - Paredes: Barreras físicas que bloquean el movimiento\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tamaño de la grilla (siempre cuadrada)\n",
    "        agent_pos (tuple): Posición actual del agente (fila, columna)\n",
    "        frutas (set): Conjunto de posiciones que contienen frutas\n",
    "        venenos (set): Conjunto de posiciones que contienen venenos\n",
    "        paredes (set): Conjunto de posiciones que contienen paredes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuración por defecto.\n",
    "        \n",
    "        El entorno comienza con:\n",
    "        - Agente en posición (0,0) - esquina superior izquierda\n",
    "        - Todos los conjuntos de elementos vacíos\n",
    "        - Tamaño de grilla determinado por GRID_WIDTH\n",
    "        \"\"\"\n",
    "        self.size = GRID_WIDTH\n",
    "        self.agent_pos = (0, 0)  # Posición inicial estándar\n",
    "        self.frutas = set()      # Conjunto vacío inicialmente\n",
    "        self.venenos = set()     # Conjunto vacío inicialmente\n",
    "        self.paredes = set()     # Conjunto vacío inicialmente\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"\n",
    "        Reinicia solo la posición del agente sin modificar el entorno.\n",
    "        \n",
    "        Esta función es útil para comenzar nuevos episodios manteniendo\n",
    "        la misma configuración de elementos (frutas, venenos, paredes)\n",
    "        establecida durante el modo setup.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno después del reset\n",
    "        \"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno excepto el agente.\n",
    "        \n",
    "        Función de utilidad para limpiar completamente el entorno\n",
    "        y comenzar una nueva configuración desde cero. Útil en\n",
    "        modo setup para crear nuevos escenarios rápidamente.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción del agente y actualiza el estado del entorno.\n",
    "        \n",
    "        Este método implementa la lógica principal del entorno, incluyendo:\n",
    "        - Procesamiento de movimientos del agente\n",
    "        - Detección de colisiones con paredes y límites\n",
    "        - Cálculo de recompensas según las interacciones\n",
    "        - Gestión de condiciones de terminación\n",
    "        - Manejo especial de venenos (penalización + reset)\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Acción a ejecutar por el agente\n",
    "                0: Mover hacia arriba (fila-1)\n",
    "                1: Mover hacia abajo (fila+1)\n",
    "                2: Mover hacia la izquierda (columna-1)\n",
    "                3: Mover hacia la derecha (columna+1)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, episodio_terminado)\n",
    "                - nuevo_estado (np.array): Estado resultante\n",
    "                - recompensa (float): Recompensa obtenida\n",
    "                - episodio_terminado (bool): True si completó o falló\n",
    "        \n",
    "        Sistema de recompensas:\n",
    "            - Colisión con pared/límite: -0.1 (movimiento inválido)\n",
    "            - Movimiento válido: -0.05 (costo de vida)\n",
    "            - Tocar veneno: -10.0 (penalización severa + reset a origen)\n",
    "            - Recolectar fruta: +1.0 (recompensa por objetivo)\n",
    "            - Completar nivel: +10.0 adicional (todas las frutas recolectadas)\n",
    "        \"\"\"\n",
    "        # Obtener posición actual del agente\n",
    "        fila, col = self.agent_pos\n",
    "        \n",
    "        # Calcular nueva posición según la acción\n",
    "        if accion == 0:      # Arriba\n",
    "            fila -= 1\n",
    "        elif accion == 1:    # Abajo\n",
    "            fila += 1\n",
    "        elif accion == 2:    # Izquierda\n",
    "            col -= 1\n",
    "        elif accion == 3:    # Derecha\n",
    "            col += 1\n",
    "\n",
    "        # Verificar colisiones con límites de grilla o paredes\n",
    "        if (\n",
    "            fila < 0                        # Límite superior\n",
    "            or fila >= GRID_HEIGHT          # Límite inferior\n",
    "            or col < 0                      # Límite izquierdo\n",
    "            or col >= GRID_WIDTH            # Límite derecho\n",
    "            or (fila, col) in self.paredes  # Colisión con pared\n",
    "        ):\n",
    "            # Movimiento inválido: penalización menor, mantener posición\n",
    "            return self.get_state(), -0.1, False\n",
    "        \n",
    "        # Movimiento válido: actualizar posición\n",
    "        x, y = fila, col\n",
    "        self.agent_pos = (x, y)\n",
    "        recompensa = -0.05  # Costo base por movimiento (fomenta eficiencia)\n",
    "        terminado = False\n",
    "\n",
    "        # Procesar interacciones con elementos del entorno\n",
    "        if self.agent_pos in self.venenos:\n",
    "            # Penalización por tocar veneno y reset a posición inicial\n",
    "            recompensa = -10.0\n",
    "            self.agent_pos = (0, 0)  # Reset automático a origen\n",
    "            \n",
    "        elif self.agent_pos in self.frutas:\n",
    "            # Recompensa por recolectar fruta\n",
    "            recompensa = 1.0\n",
    "            self.frutas.remove(self.agent_pos)  # Eliminar fruta recolectada\n",
    "            \n",
    "            # Verificar si se completó el nivel\n",
    "            if not self.frutas:  # No quedan frutas\n",
    "                recompensa += 10.0   # Bonus por completar\n",
    "                terminado = True     # Episodio exitoso\n",
    "                self.agent_pos = (0, 0)  # Reset para próximo episodio\n",
    "\n",
    "        return self.get_state(), recompensa, terminado\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Convierte el estado actual del entorno a formato tensor para DQN.\n",
    "        \n",
    "        Esta función es crucial para la compatibilidad con redes neuronales\n",
    "        convolucionales, transformando la representación discreta del entorno\n",
    "        en un tensor 3D que puede ser procesado eficientemente por la CNN.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Tensor 3D con forma (3, size, size) donde:\n",
    "                - Canal 0: Posición del agente (1.0 donde está, 0.0 resto)\n",
    "                - Canal 1: Posiciones de frutas (1.0 donde hay frutas)\n",
    "                - Canal 2: Posiciones de venenos (1.0 donde hay venenos)\n",
    "        \n",
    "        Características del formato:\n",
    "        - Tipo float32 para compatibilidad con PyTorch\n",
    "        - Representación binaria (0.0 o 1.0) para claridad\n",
    "        - Canales separados permiten que la CNN detecte patrones específicos\n",
    "        - Dimensiones compatibles con arquitectura Conv2D\n",
    "        \n",
    "        Nota: Las paredes no se incluyen en el estado ya que son estáticas\n",
    "              y el agente las aprende a través de las restricciones de movimiento.\n",
    "        \"\"\"\n",
    "        # Inicializar tensor de estado con ceros\n",
    "        estado = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posición del agente\n",
    "        estado[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruta in self.frutas:\n",
    "            estado[1, fruta[0], fruta[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de venenos\n",
    "        for veneno in self.venenos:\n",
    "            estado[2, veneno[0], veneno[1]] = 1.0\n",
    "            \n",
    "        return estado\n",
    "\n",
    "    def dibujar(\n",
    "        self,\n",
    "        pantalla,\n",
    "        modo_juego,\n",
    "        cursor_pos,\n",
    "        img_fruta,\n",
    "        img_veneno,\n",
    "        img_pared,\n",
    "        img_agente,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Renderiza el entorno completo en la pantalla usando Pygame.\n",
    "        \n",
    "        Esta función maneja toda la visualización del entorno, incluyendo\n",
    "        elementos del juego, interfaz de usuario y información contextual.\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde renderizar\n",
    "            modo_juego (str): Modo actual (\"SETUP\" o \"PLAYING\")\n",
    "            cursor_pos (tuple): Posición del cursor en modo setup\n",
    "            img_fruta (pygame.Surface): Imagen para representar frutas\n",
    "            img_veneno (pygame.Surface): Imagen para representar venenos\n",
    "            img_pared (pygame.Surface): Imagen para representar paredes\n",
    "            img_agente (pygame.Surface): Imagen para representar al agente\n",
    "        \n",
    "        Proceso de renderizado:\n",
    "        1. Limpiar pantalla con color de fondo\n",
    "        2. Dibujar grilla de referencia\n",
    "        3. Renderizar elementos por capas (paredes → frutas → venenos → agente)\n",
    "        4. Mostrar cursor en modo setup\n",
    "        5. Renderizar información de controles y estado\n",
    "        \n",
    "        El orden de renderizado es importante para la superposición correcta\n",
    "        de elementos visuales y la legibilidad de la interfaz.\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar grilla de referencia\n",
    "        # Líneas verticales\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        # Líneas horizontales\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Renderizar elementos del entorno (orden de capas importante)\n",
    "        # 1. Paredes (fondo) - obstáculos estáticos\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "            \n",
    "        # 2. Frutas (objetivos) - elementos a recolectar\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "            \n",
    "        # 3. Venenos (peligros) - elementos a evitar\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "\n",
    "        # 4. Agente (primer plano) - jugador controlado por IA\n",
    "        pantalla.blit(\n",
    "            img_agente, (self.agent_pos[0] * CELL_SIZE, self.agent_pos[1] * CELL_SIZE)\n",
    "        )\n",
    "\n",
    "        # 5. Cursor de selección (solo en modo setup)\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar información textual de la interfaz\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        \n",
    "        # Mostrar modo actual\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Instrucciones para modo setup\n",
    "        controles1 = font.render(\n",
    "            \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\", True, COLOR_TEXTO\n",
    "        )\n",
    "        \n",
    "        # Controles generales\n",
    "        controles2 = font.render(\"P=Jugar, S=Setup\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Posicionar textos en la parte inferior de la pantalla\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles1, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles2, (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta la interfaz de demostración DQN.\n",
    "    \n",
    "    Esta función implementa un sistema completo de demostración interactiva\n",
    "    que permite a los usuarios configurar entornos personalizados y observar\n",
    "    el comportamiento de un agente DQN entrenado.\n",
    "    \n",
    "    Flujo de la aplicación:\n",
    "    1. Inicialización de Pygame y carga de recursos visuales\n",
    "    2. Carga del agente DQN preentrenado\n",
    "    3. Bucle principal con dos modos de operación:\n",
    "       - SETUP: Configuración interactiva del entorno\n",
    "       - PLAYING: Demostración del agente entrenado\n",
    "    4. Renderizado continuo y gestión de eventos\n",
    "    \n",
    "    Modos de operación:\n",
    "    \n",
    "    **MODO SETUP (Configuración):**\n",
    "    - Navegación con flechas del teclado\n",
    "    - F: Añadir/quitar frutas en posición del cursor\n",
    "    - V: Añadir/quitar venenos en posición del cursor\n",
    "    - W: Añadir/quitar paredes en posición del cursor\n",
    "    - C: Limpiar completamente el entorno\n",
    "    \n",
    "    **MODO PLAYING (Demostración):**\n",
    "    - El agente DQN toma control automático\n",
    "    - Visualización en tiempo real de decisiones\n",
    "    - Finalización automática y retorno a setup\n",
    "    \n",
    "    **Controles Globales:**\n",
    "    - P: Cambiar a modo PLAYING\n",
    "    - S: Cambiar a modo SETUP\n",
    "    - ESC/X: Salir de la aplicación\n",
    "    \"\"\"\n",
    "    # Inicializar sistema gráfico Pygame\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente DQN - Come Frutas 🍓\")\n",
    "\n",
    "    def cargar_img(nombre, color_fallback):\n",
    "        \"\"\"\n",
    "        Función auxiliar para cargar imágenes con respaldo de color.\n",
    "        \n",
    "        Intenta cargar una imagen desde archivo y, si falla, crea una\n",
    "        superficie de color sólido como alternativa. Esto garantiza que\n",
    "        la aplicación funcione incluso sin los archivos de imagen.\n",
    "        \n",
    "        Args:\n",
    "            nombre (str): Nombre/ruta del archivo de imagen\n",
    "            color_fallback (tuple): Color RGB de respaldo (r, g, b)\n",
    "            \n",
    "        Returns:\n",
    "            pygame.Surface: Superficie escalada al tamaño de celda\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "            img = pygame.image.load(ruta).convert_alpha()\n",
    "            return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "        except:\n",
    "            # Crear superficie de color sólido como respaldo\n",
    "            surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "            surf.fill(color_fallback)\n",
    "            return surf\n",
    "\n",
    "    # Cargar recursos visuales con colores de respaldo\n",
    "    img_fruta = cargar_img(\"../fruta.png\", (0, 255, 0))      # Verde si falla\n",
    "    img_veneno = cargar_img(\"../veneno.png\", (255, 0, 0))     # Rojo si falla\n",
    "    img_pared = cargar_img(\"../pared.png\", (100, 100, 100))   # Gris si falla\n",
    "    img_agente = cargar_img(\"../agente.png\", (0, 0, 255))     # Azul si falla\n",
    "\n",
    "    # Inicializar componentes principales\n",
    "    entorno = EntornoGrid()                                    # Entorno de simulación\n",
    "    agente = Agent(state_shape=(3, GRID_HEIGHT, GRID_WIDTH), action_size=4)  # Agente DQN\n",
    "    agente.load(\"DQN/dqn_model.pth\")                          # Cargar modelo preentrenado\n",
    "\n",
    "    # Variables de control de la interfaz\n",
    "    cursor_pos = [0, 0]        # Posición del cursor en modo setup\n",
    "    modo_juego = \"SETUP\"       # Modo inicial\n",
    "    reloj = pygame.time.Clock()  # Control de framerate\n",
    "    corriendo = True           # Flag principal del bucle\n",
    "\n",
    "    # Bucle principal de la aplicación\n",
    "    while corriendo:\n",
    "        # Procesar eventos del usuario\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # --- CONTROLES GLOBALES ---\n",
    "                if evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO JUEGO ---\")\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    time.sleep(0.5)  # Pausa para evitar acciones inmediatas\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # --- CONTROLES ESPECÍFICOS DEL MODO SETUP ---\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Navegación del cursor con flechas del teclado\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Obtener posición actual del cursor\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    \n",
    "                    # Gestión de elementos en la posición del cursor\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        # Alternar fruta en posición actual\n",
    "                        if pos in entorno.frutas:\n",
    "                            entorno.frutas.remove(pos)\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posición\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        # Alternar veneno en posición actual\n",
    "                        if pos in entorno.venenos:\n",
    "                            entorno.venenos.remove(pos)\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posición\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        # Alternar pared en posición actual\n",
    "                        if pos in entorno.paredes:\n",
    "                            entorno.paredes.remove(pos)\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posición\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        # Limpiar completamente el entorno\n",
    "                        print(\"--- LIMPIANDO ENTORNO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "\n",
    "        # --- LÓGICA DEL MODO PLAYING ---\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # Obtener estado actual del entorno\n",
    "            estado = entorno.get_state()\n",
    "            \n",
    "            # El agente DQN elige la mejor acción (sin exploración)\n",
    "            # explore=False garantiza que use solo la política aprendida\n",
    "            accion = agente.choose_action(estado, explore=False)\n",
    "            \n",
    "            # Ejecutar la acción en el entorno\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            \n",
    "            # Verificar si el episodio terminó\n",
    "            if terminado:\n",
    "                print(\"Juego terminado. Volviendo a SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "                \n",
    "            # Control de velocidad para observación humana\n",
    "            time.sleep(0.1)  # 10 FPS para visualización clara\n",
    "\n",
    "        # --- SISTEMA DE RENDERIZADO ---\n",
    "        # Crear superficie temporal para composición\n",
    "        pantalla_con_info = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "        pantalla_con_info.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Renderizar el entorno completo en la superficie temporal\n",
    "        entorno.dibujar(\n",
    "            pantalla_con_info,\n",
    "            modo_juego,\n",
    "            tuple(cursor_pos),  # Convertir lista a tupla\n",
    "            img_fruta,\n",
    "            img_veneno,\n",
    "            img_pared,\n",
    "            img_agente,\n",
    "        )\n",
    "        \n",
    "        # Transferir superficie temporal a pantalla principal\n",
    "        pantalla.blit(pantalla_con_info, (0, 0))\n",
    "        \n",
    "        # Actualizar pantalla y controlar framerate\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(60)  # Limitar a 60 FPS para suavidad\n",
    "\n",
    "    # Limpiar recursos al salir de la aplicación\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa de demostración DQN.\n",
    "    \n",
    "    Ejecuta la función main() cuando el archivo se ejecuta directamente.\n",
    "    Este patrón permite importar clases y funciones de este módulo sin\n",
    "    ejecutar automáticamente la interfaz de demostración.\n",
    "    \n",
    "    Uso típico:\n",
    "        python dqn_agente_comefrutas.py  # Ejecuta la demostración\n",
    "        \n",
    "    La aplicación está diseñada para:\n",
    "    - Demostraciones educativas de algoritmos DQN\n",
    "    - Validación visual del comportamiento del agente\n",
    "    - Experimentación rápida con configuraciones de entorno\n",
    "    - Presentaciones de proyectos de IA/ML\n",
    "    \n",
    "    Diferencias con versión DDQN:\n",
    "    - Utiliza algoritmo DQN clásico (una sola red)\n",
    "    - Compatible con modelos entrenados con DQN simple\n",
    "    - Interfaz idéntica pero agente subyacente diferente\n",
    "    \"\"\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70047265",
   "metadata": {},
   "source": [
    "#### enviroment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645cbfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment.py\n",
    "\"\"\"\n",
    "Entorno de cuadrícula para el entrenamiento de un agente DQN.\n",
    "\n",
    "Este módulo implementa un entorno de juego donde un agente debe navegar\n",
    "por una cuadrícula para recoger frutas mientras evita venenos. El entorno\n",
    "utiliza reward shaping para guiar al agente hacia las frutas.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GridEnvironment:\n",
    "    \"\"\"\n",
    "    Entorno de cuadrícula para un agente que debe recoger frutas y evitar venenos.\n",
    "    \n",
    "    El entorno consiste en una cuadrícula de tamaño configurable donde:\n",
    "    - El agente se mueve en 4 direcciones (arriba, abajo, izquierda, derecha)\n",
    "    - Las frutas proporcionan recompensas positivas\n",
    "    - Los venenos proporcionan recompensas negativas y terminan el juego\n",
    "    - El objetivo es recoger todas las frutas sin tocar venenos\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tamaño de la cuadrícula (size x size)\n",
    "        agent_pos (np.array): Posición actual del agente [fila, columna]\n",
    "        fruit_pos (list): Lista de posiciones de frutas\n",
    "        poison_pos (list): Lista de posiciones de venenos\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de cuadrícula.\n",
    "        \n",
    "        Args:\n",
    "            size (int, optional): Tamaño de la cuadrícula. Por defecto es 5x5.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, agent_pos=(0, 0), fruit_pos=[], poison_pos=[]):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno con una configuración específica.\n",
    "        \n",
    "        Establece las posiciones iniciales del agente, frutas y venenos.\n",
    "        Si no se proporcionan posiciones, se usan listas vacías para frutas y venenos.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (tuple, optional): Posición inicial del agente (fila, columna). \n",
    "                                       Por defecto (0, 0).\n",
    "            fruit_pos (list, optional): Lista de tuplas con posiciones de frutas.\n",
    "                                       Por defecto lista vacía.\n",
    "            poison_pos (list, optional): Lista de tuplas con posiciones de venenos.\n",
    "                                        Por defecto lista vacía.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno como array 3D (3, size, size).\n",
    "        \"\"\"\n",
    "        self.agent_pos = np.array(agent_pos)\n",
    "        self.fruit_pos = [np.array(p) for p in fruit_pos]\n",
    "        self.poison_pos = [np.array(p) for p in poison_pos]\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera la representación del estado actual del entorno.\n",
    "        \n",
    "        El estado se representa como una \"imagen\" de 3 canales que puede ser\n",
    "        procesada por una CNN. Cada canal representa un tipo de elemento:\n",
    "        \n",
    "        - Canal 0: Posición del agente (1.0 donde está el agente, 0.0 en el resto)\n",
    "        - Canal 1: Posiciones de frutas (1.0 donde hay frutas, 0.0 en el resto)\n",
    "        - Canal 2: Posiciones de venenos (1.0 donde hay venenos, 0.0 en el resto)\n",
    "        \n",
    "        Esta representación permite que el agente \"vea\" todo el entorno de una vez\n",
    "        y facilita el procesamiento por redes neuronales convolucionales.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado del entorno como array 3D de forma (3, size, size)\n",
    "                     con valores float32.\n",
    "        \"\"\"\n",
    "        state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posición del agente\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de las frutas\n",
    "        for fruit in self.fruit_pos:\n",
    "            state[1, fruit[0], fruit[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de los venenos\n",
    "        for poison in self.poison_pos:\n",
    "            state[2, poison[0], poison[1]] = 1.0\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción en el entorno y retorna el resultado.\n",
    "        \n",
    "        Esta función implementa la lógica principal del juego, incluyendo:\n",
    "        1. Movimiento del agente\n",
    "        2. Cálculo de recompensas con reward shaping\n",
    "        3. Detección de colisiones con frutas y venenos\n",
    "        4. Determinación de condiciones de terminación\n",
    "        \n",
    "        El sistema de recompensas incluye:\n",
    "        - Recompensa por acercarse a frutas (+0.1)\n",
    "        - Castigo por alejarse de frutas (-0.15)\n",
    "        - Recompensa por recoger frutas (+1.0)\n",
    "        - Castigo por tocar veneno (-1.0, termina el juego)\n",
    "        - Recompensa por completar el nivel (+5.0)\n",
    "        - Castigo base por movimiento (-0.05, fomenta eficiencia)\n",
    "        \n",
    "        Args:\n",
    "            action (int): Acción a realizar:\n",
    "                         0 = Arriba (decrementar fila)\n",
    "                         1 = Abajo (incrementar fila)\n",
    "                         2 = Izquierda (decrementar columna)\n",
    "                         3 = Derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, terminado)\n",
    "                - nuevo_estado (np.array): Estado del entorno después de la acción\n",
    "                - recompensa (float): Recompensa obtenida por la acción\n",
    "                - terminado (bool): True si el episodio ha terminado\n",
    "        \"\"\"\n",
    "        \n",
    "        # FASE 1: REWARD SHAPING - Calcular distancia a fruta más cercana ANTES del movimiento\n",
    "        # Esto permite dar recompensas por acercarse/alejarse de las frutas\n",
    "        old_dist_to_fruit = float('inf')\n",
    "        if self.fruit_pos:\n",
    "            distances = [np.linalg.norm(self.agent_pos - fruit) for fruit in self.fruit_pos]\n",
    "            old_dist_to_fruit = min(distances)\n",
    "\n",
    "        \n",
    "        # FASE 2: MOVIMIENTO DEL AGENTE\n",
    "        # Actualizar la posición del agente basada en la acción seleccionada\n",
    "        if action == 0:      # Arriba\n",
    "            self.agent_pos[0] -= 1\n",
    "        elif action == 1:    # Abajo\n",
    "            self.agent_pos[0] += 1\n",
    "        elif action == 2:    # Izquierda\n",
    "            self.agent_pos[1] -= 1\n",
    "        elif action == 3:    # Derecha\n",
    "            self.agent_pos[1] += 1\n",
    "\n",
    "        # Limitar la posición del agente a los límites del tablero\n",
    "        # np.clip asegura que las coordenadas estén entre 0 y (size-1)\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.size - 1)\n",
    "\n",
    "        \n",
    "        # FASE 3: CÁLCULO DE RECOMPENSAS\n",
    "        \n",
    "        # Recompensa base: pequeño castigo por cada movimiento para fomentar eficiencia\n",
    "        reward = -0.05  \n",
    "        done = False\n",
    "\n",
    "        # REWARD SHAPING: Calcular nueva distancia y recompensar acercamiento a frutas\n",
    "        # Esto ayuda al agente a aprender a navegar hacia las frutas incluso antes de alcanzarlas\n",
    "        new_dist_to_fruit = float('inf')\n",
    "        if self.fruit_pos:\n",
    "            distances = [np.linalg.norm(self.agent_pos - fruit) for fruit in self.fruit_pos]\n",
    "            new_dist_to_fruit = min(distances)\n",
    "\n",
    "            # Recompensar por acercarse, castigar por alejarse\n",
    "            if new_dist_to_fruit < old_dist_to_fruit:\n",
    "                reward += 0.1   # Recompensa por acercarse a una fruta\n",
    "            else:\n",
    "                reward -= 0.15  # Castigo por alejarse (ligeramente mayor para evitar indecisión)\n",
    "\n",
    "        \n",
    "        # FASE 4: DETECCIÓN DE EVENTOS\n",
    "        \n",
    "        # Verificar si el agente recogió una fruta\n",
    "        for i, fruit in enumerate(self.fruit_pos):\n",
    "            if np.array_equal(self.agent_pos, fruit):\n",
    "                reward += 1.0  # Gran recompensa por recoger fruta\n",
    "                self.fruit_pos.pop(i)  # Remover la fruta del entorno\n",
    "                break  # Solo puede recoger una fruta por paso\n",
    "        \n",
    "        # Verificar si el agente tocó veneno (termina el juego)\n",
    "        if any(np.array_equal(self.agent_pos, poison) for poison in self.poison_pos):\n",
    "            reward = -1.0  # Castigo severo y absoluto por tocar veneno\n",
    "            done = True    # Terminar el episodio inmediatamente\n",
    "\n",
    "        # Verificar condición de victoria: no quedan frutas\n",
    "        if not self.fruit_pos:\n",
    "            done = True\n",
    "            reward += 5.0  # Gran recompensa bonus por completar el objetivo\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d7e91",
   "metadata": {},
   "source": [
    "#### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628abc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\"\"\"\n",
    "Interfaz gráfica interactiva para visualizar un agente DQN entrenado.\n",
    "\n",
    "Este módulo implementa una aplicación Pygame que permite:\n",
    "1. Configurar un escenario colocando frutas y venenos manualmente\n",
    "2. Observar cómo el agente DQN entrenado resuelve el escenario\n",
    "3. Reiniciar para probar diferentes configuraciones\n",
    "\n",
    "La aplicación tiene dos modos:\n",
    "- Modo Setup: El usuario coloca elementos en la cuadrícula\n",
    "- Modo Run: El agente toma control y ejecuta su política aprendida\n",
    "\n",
    "Controles:\n",
    "- Click izquierdo: Colocar fruta\n",
    "- Click derecho: Colocar veneno  \n",
    "- Espacio: Iniciar simulación del agente\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "from environment import GridEnvironment\n",
    "from agent import Agent\n",
    "\n",
    "# CONFIGURACIÓN DE PYGAME Y CONSTANTES DEL JUEGO\n",
    "\"\"\"\n",
    "Configuración visual y dimensiones de la aplicación.\n",
    "\"\"\"\n",
    "GRID_SIZE = 5        # Tamaño de la cuadrícula (5x5)\n",
    "CELL_SIZE = 100      # Tamaño de cada celda en píxeles\n",
    "WIDTH, HEIGHT = GRID_SIZE * CELL_SIZE, GRID_SIZE * CELL_SIZE  # Ventana de 500x500 píxeles\n",
    "WIN = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Agente Come-Frutas\")\n",
    "pygame.font.init()   # Inicializar fuentes para texto si se necesita\n",
    "\n",
    "# INICIALIZACIÓN DEL AGENTE DQN ENTRENADO\n",
    "\"\"\"\n",
    "Carga el agente DQN previamente entrenado desde archivo.\n",
    "El agente utilizará su política aprendida para navegar por el entorno.\n",
    "\"\"\"\n",
    "env = GridEnvironment(size=GRID_SIZE)\n",
    "action_size = 4  # 4 acciones posibles: arriba, abajo, izquierda, derecha\n",
    "state_shape = (3, GRID_SIZE, GRID_SIZE)  # Forma del estado: 3 canales x 5x5 grid\n",
    "agent = Agent(state_shape, action_size)  # Crear instancia del agente\n",
    "agent.load(\"dqn_model.pth\")              # Cargar pesos del modelo entrenado\n",
    "\n",
    "# DEFINICIÓN DE COLORES\n",
    "\"\"\"\n",
    "Paleta de colores para los elementos visuales del juego.\n",
    "Utiliza sistema RGB (Red, Green, Blue) con valores 0-255.\n",
    "\"\"\"\n",
    "COLOR_GRID = (200, 200, 200)   # Gris claro para las líneas de la cuadrícula\n",
    "COLOR_AGENT = (0, 0, 255)      # Azul para el agente\n",
    "COLOR_FRUIT = (0, 255, 0)      # Verde para las frutas\n",
    "COLOR_POISON = (255, 0, 0)     # Rojo para los venenos\n",
    "\n",
    "def draw_grid():\n",
    "    \"\"\"\n",
    "    Dibuja las líneas de la cuadrícula en la ventana.\n",
    "    \n",
    "    Crea una cuadrícula visual de 5x5 dibujando líneas verticales y horizontales\n",
    "    separadas por CELL_SIZE píxeles. Esto ayuda a visualizar las celdas donde\n",
    "    se pueden colocar elementos y donde se mueve el agente.\n",
    "    \"\"\"\n",
    "    # Líneas verticales\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        pygame.draw.line(WIN, COLOR_GRID, (x, 0), (x, HEIGHT))\n",
    "    # Líneas horizontales  \n",
    "    for y in range(0, HEIGHT, CELL_SIZE):\n",
    "        pygame.draw.line(WIN, COLOR_GRID, (0, y), (WIDTH, y))\n",
    "\n",
    "def draw_elements(agent_pos, fruits, poisons):\n",
    "    \"\"\"\n",
    "    Dibuja todos los elementos del juego en sus posiciones actuales.\n",
    "    \n",
    "    Renderiza visualmente:\n",
    "    - Agente: Como un cuadrado azul que ocupa toda la celda\n",
    "    - Frutas: Como círculos verdes centrados en sus celdas\n",
    "    - Venenos: Como cuadrados rojos más pequeños centrados en sus celdas\n",
    "    \n",
    "    Args:\n",
    "        agent_pos (np.array): Posición del agente [fila, columna]\n",
    "        fruits (list): Lista de posiciones de frutas [(fila, col), ...]\n",
    "        poisons (list): Lista de posiciones de venenos [(fila, col), ...]\n",
    "    \n",
    "    Note:\n",
    "        Las coordenadas se invierten para Pygame: agent_pos[1] es X, agent_pos[0] es Y\n",
    "    \"\"\"\n",
    "    # Dibujar agente como cuadrado azul completo\n",
    "    if agent_pos[0] >= 0:  # Solo dibujar si el agente está en el tablero\n",
    "        pygame.draw.rect(WIN, COLOR_AGENT, \n",
    "                        (agent_pos[1] * CELL_SIZE, agent_pos[0] * CELL_SIZE, \n",
    "                         CELL_SIZE, CELL_SIZE))\n",
    "    \n",
    "    # Dibujar frutas como círculos verdes\n",
    "    for f in fruits:\n",
    "        center_x = f[1] * CELL_SIZE + CELL_SIZE // 2\n",
    "        center_y = f[0] * CELL_SIZE + CELL_SIZE // 2\n",
    "        radius = CELL_SIZE // 3\n",
    "        pygame.draw.circle(WIN, COLOR_FRUIT, (center_x, center_y), radius)\n",
    "    \n",
    "    # Dibujar venenos como cuadrados rojos más pequeños\n",
    "    for p in poisons:\n",
    "        margin = 20  # Margen para hacer el cuadrado más pequeño\n",
    "        pygame.draw.rect(WIN, COLOR_POISON, \n",
    "                        (p[1] * CELL_SIZE + margin, p[0] * CELL_SIZE + margin, \n",
    "                         CELL_SIZE - 2*margin, CELL_SIZE - 2*margin))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que maneja el bucle de la aplicación.\n",
    "    \n",
    "    Implementa una máquina de estados con dos modos:\n",
    "    \n",
    "    MODO SETUP:\n",
    "    - Permite al usuario colocar frutas y venenos con clics del mouse\n",
    "    - Click izquierdo: Colocar fruta\n",
    "    - Click derecho: Colocar veneno\n",
    "    - Presionar ESPACIO: Iniciar simulación\n",
    "    \n",
    "    MODO RUN:\n",
    "    - El agente DQN toma control del juego\n",
    "    - Ejecuta acciones basadas en su política aprendida\n",
    "    - Visualiza el comportamiento del agente en tiempo real\n",
    "    - Se reinicia automáticamente al terminar\n",
    "    \n",
    "    La aplicación se ejecuta hasta que el usuario cierre la ventana.\n",
    "    \"\"\"\n",
    "    # Variables de estado del juego\n",
    "    fruits = []      # Lista de posiciones de frutas colocadas por el usuario\n",
    "    poisons = []     # Lista de posiciones de venenos colocadas por el usuario  \n",
    "    mode = \"setup\"   # Modo actual: \"setup\" (configuración) o \"run\" (simulación)\n",
    "\n",
    "    # Configuración del bucle principal\n",
    "    clock = pygame.time.Clock()  # Para controlar FPS\n",
    "    run = True                   # Flag de control del bucle principal\n",
    "    # BUCLE PRINCIPAL DE LA APLICACIÓN\n",
    "    while run:\n",
    "        # Limpiar pantalla con fondo negro\n",
    "        WIN.fill((0, 0, 0))\n",
    "        # Dibujar cuadrícula base\n",
    "        draw_grid()\n",
    "\n",
    "        # MANEJO DE EVENTOS DE USUARIO\n",
    "        for event in pygame.event.get():\n",
    "            # Evento de cierre de ventana\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "\n",
    "            # EVENTOS EN MODO SETUP (Configuración manual)\n",
    "            if mode == \"setup\":\n",
    "                # Manejo de clics del mouse para colocar elementos\n",
    "                if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                    pos = pygame.mouse.get_pos()\n",
    "                    # Convertir coordenadas de píxeles a coordenadas de cuadrícula\n",
    "                    col = pos[0] // CELL_SIZE\n",
    "                    row = pos[1] // CELL_SIZE\n",
    "                    \n",
    "                    # Click izquierdo (botón 1): Colocar fruta\n",
    "                    if event.button == 1 and (row, col) not in fruits:\n",
    "                        fruits.append((row, col))\n",
    "                        print(f\"Fruta colocada en ({row}, {col})\")\n",
    "                    \n",
    "                    # Click derecho (botón 3): Colocar veneno  \n",
    "                    elif event.button == 3 and (row, col) not in poisons:\n",
    "                        poisons.append((row, col))\n",
    "                        print(f\"Veneno colocado en ({row}, {col})\")\n",
    "\n",
    "                # Manejo de teclas para cambiar de modo\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_SPACE:\n",
    "                        mode = \"run\"\n",
    "                        # Inicializar el entorno con la configuración del usuario\n",
    "                        state = env.reset(agent_pos=(0, 0), fruit_pos=fruits, poison_pos=poisons)\n",
    "                        print(\"=== INICIANDO SIMULACIÓN DEL AGENTE ===\")\n",
    "                        print(f\"Frutas: {len(fruits)}, Venenos: {len(poisons)}\")\n",
    "\n",
    "        # RENDERIZADO SEGÚN EL MODO ACTUAL\n",
    "        \n",
    "        if mode == \"setup\":\n",
    "            # MODO CONFIGURACIÓN: Mostrar elementos colocados por el usuario\n",
    "            # Usar posición (-1,-1) para que el agente no aparezca en pantalla\n",
    "            draw_elements(np.array([-1, -1]), fruits, poisons)\n",
    "        \n",
    "        elif mode == \"run\":\n",
    "            # MODO SIMULACIÓN: El agente DQN ejecuta su política\n",
    "            \n",
    "            # Obtener estado actual del entorno\n",
    "            state = env.get_state()\n",
    "            \n",
    "            # El agente decide la acción usando su política entrenada\n",
    "            # explore=False significa que usa solo explotación, no exploración\n",
    "            action = agent.choose_action(state, explore=False)\n",
    "            \n",
    "            # Ejecutar la acción en el entorno\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Renderizar estado actual del juego\n",
    "            draw_elements(env.agent_pos, env.fruit_pos, env.poison_pos)\n",
    "\n",
    "            # Verificar si el episodio terminó\n",
    "            if done:\n",
    "                if not env.fruit_pos:  # Victoria: todas las frutas recogidas\n",
    "                    print(\"🎉 ¡ÉXITO! El agente recogió todas las frutas\")\n",
    "                else:  # Derrota: tocó veneno\n",
    "                    print(\"💀 DERROTA: El agente tocó veneno\")\n",
    "                \n",
    "                print(\"=== SIMULACIÓN TERMINADA ===\")\n",
    "                \n",
    "                # Reiniciar para permitir nueva configuración\n",
    "                fruits = []\n",
    "                poisons = []\n",
    "                mode = \"setup\"\n",
    "                \n",
    "                # Pausa dramática antes de reiniciar\n",
    "                pygame.time.delay(2000)\n",
    "\n",
    "            # Pausa entre movimientos para visualización clara\n",
    "            pygame.time.delay(300)\n",
    "\n",
    "        # Actualizar pantalla con todos los cambios\n",
    "        pygame.display.update()\n",
    "\n",
    "    # Limpieza al cerrar la aplicación\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa.\n",
    "    \n",
    "    Ejecuta la función main() solo si este archivo se ejecuta directamente\n",
    "    (no si se importa como módulo).\n",
    "    \"\"\"\n",
    "    print(\"=== AGENTE DQN COME-FRUTAS ===\")\n",
    "    print(\"CONTROLES:\")\n",
    "    print(\"• Click izquierdo: Colocar fruta\")\n",
    "    print(\"• Click derecho: Colocar veneno\")\n",
    "    print(\"• ESPACIO: Iniciar simulación\")\n",
    "    print(\"• Cerrar ventana: Salir\")\n",
    "    print(\"\\n¡Configura un escenario y observa al agente!\")\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a901f",
   "metadata": {},
   "source": [
    "### DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f3b84",
   "metadata": {},
   "source": [
    "#### agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\"\"\"\n",
    "Implementación completa del agente DDQN (Double Deep Q-Network).\n",
    "\n",
    "Este módulo contiene la implementación del algoritmo DDQN, una mejora del DQN clásico\n",
    "que aborda el problema de sobreestimación de valores Q mediante el uso de dos redes\n",
    "neuronales: una para selección de acciones y otra para evaluación de valores.\n",
    "\n",
    "Características principales:\n",
    "- Red neuronal convolucional optimizada para entornos de grilla\n",
    "- Algoritmo DDQN con separación de selección y evaluación\n",
    "- Memoria de replay extendida (50,000 experiencias)\n",
    "- Técnicas de estabilización avanzadas\n",
    "- Sistema robusto de guardado/carga de modelos\n",
    "\n",
    "Algoritmo DDQN:\n",
    "La innovación clave es el uso de dos redes para calcular targets:\n",
    "1. Red principal: Selecciona la mejor acción del siguiente estado\n",
    "2. Red objetivo: Evalúa el valor Q de esa acción seleccionada\n",
    "\n",
    "Esto reduce significativamente la sobreestimación de valores Q que sufre DQN clásico,\n",
    "resultando en un aprendizaje más estable y políticas más robustas.\n",
    "\n",
    "Referencias:\n",
    "- van Hasselt et al. (2016): \"Deep Reinforcement Learning with Double Q-learning\"\n",
    "- Mnih et al. (2015): \"Human-level control through deep reinforcement learning\"\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. RED NEURONAL CONVOLUCIONAL PARA DDQN ---\n",
    "class CNN_DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal convolucional especializada para DDQN en entornos de grilla.\n",
    "    \n",
    "    Esta arquitectura está optimizada para procesar estados representados como\n",
    "    imágenes multi-canal, típicos en problemas de navegación espacial donde\n",
    "    el estado se puede visualizar como una cuadrícula con diferentes tipos\n",
    "    de elementos (agente, objetivos, obstáculos).\n",
    "    \n",
    "    Diseño arquitectónico:\n",
    "    \n",
    "    **Etapa Convolucional (Extracción de características):**\n",
    "    - Conv1: 3→16 canales, kernel 3x3 → Detecta patrones básicos locales\n",
    "    - Conv2: 16→32 canales, kernel 3x3 → Combina patrones en características complejas\n",
    "    - ReLU en cada capa para introducir no-linealidad\n",
    "    - Padding=1 preserva dimensiones espaciales\n",
    "    \n",
    "    **Etapa Completamente Conectada (Toma de decisiones):**\n",
    "    - FC1: Procesa características extraídas (256 neuronas)\n",
    "    - FC2: Genera valores Q para cada acción posible\n",
    "    \n",
    "    **Ventajas de esta arquitectura:**\n",
    "    - Invarianza a traslaciones locales (convoluciones)\n",
    "    - Reducción progresiva de parámetros vs redes totalmente conectadas\n",
    "    - Capacidad de detectar patrones espaciales complejos\n",
    "    - Escalabilidad a entornos de diferentes tamaños\n",
    "    \n",
    "    Args:\n",
    "        h (int): Altura de la grilla de entrada\n",
    "        w (int): Anchura de la grilla de entrada\n",
    "        outputs (int): Número de acciones posibles (valores Q de salida)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(CNN_DQN, self).__init__()\n",
    "        \n",
    "        # --- CAPAS CONVOLUCIONALES PARA EXTRACCIÓN DE CARACTERÍSTICAS ---\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        # Entrada: 3 canales (agente, frutas, venenos)\n",
    "        # Salida: 16 mapas de características\n",
    "        # Kernel 3x3: Ventana de percepción local óptima para grillas pequeñas\n",
    "        # Padding=1: Preserva dimensiones espaciales de entrada\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Entrada: 16 mapas de características de la capa anterior\n",
    "        # Salida: 32 mapas de características más abstractas\n",
    "        # Mayor profundidad permite detectar patrones más complejos\n",
    "        \n",
    "        # --- CÁLCULO DINÁMICO DE DIMENSIONES ---\n",
    "        \"\"\"\n",
    "        Función auxiliar para calcular dimensiones después de convoluciones.\n",
    "        Esencial para conectar correctamente las capas convolucionales\n",
    "        con las capas completamente conectadas.\n",
    "        \n",
    "        Fórmula: output_size = (input_size + 2*padding - kernel_size) // stride + 1\n",
    "        \"\"\"\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        # Aplicar la función de cálculo a ambas dimensiones espaciales\n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32  # 32 canales de la última conv\n",
    "        \n",
    "        # --- CAPAS COMPLETAMENTE CONECTADAS PARA TOMA DE DECISIONES ---\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        # Capa oculta densa que procesa las características extraídas\n",
    "        # 256 neuronas: Balance entre capacidad expresiva y eficiencia computacional\n",
    "        # Suficiente para capturar relaciones complejas entre características espaciales\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "        # Capa de salida que produce valores Q para cada acción\n",
    "        # Sin función de activación (los valores Q pueden ser negativos)\n",
    "        # Número de neuronas = número de acciones posibles\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante de la red neuronal.\n",
    "        \n",
    "        Implementa el flujo completo de información desde el estado de entrada\n",
    "        hasta los valores Q de salida, aplicando las transformaciones necesarias\n",
    "        para extraer características espaciales y generar estimaciones de valor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Estado de entrada con forma (batch_size, 3, height, width)\n",
    "                             - batch_size: Número de estados en el lote\n",
    "                             - 3: Canales (agente, frutas, venenos)\n",
    "                             - height, width: Dimensiones espaciales de la grilla\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Valores Q para cada acción con forma (batch_size, num_actions)\n",
    "        \n",
    "        Flujo de procesamiento:\n",
    "        1. Convolución 1 + ReLU: Detección de patrones básicos\n",
    "        2. Convolución 2 + ReLU: Extracción de características complejas\n",
    "        3. Aplanamiento: Conversión de 2D a 1D para capas densas\n",
    "        4. FC1 + ReLU: Procesamiento de alto nivel de características\n",
    "        5. FC2: Generación de valores Q finales (sin activación)\n",
    "        \"\"\"\n",
    "        # Primera capa convolucional con activación ReLU\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        \n",
    "        # Segunda capa convolucional con activación ReLU\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        \n",
    "        # Aplanar características espaciales para capas densas\n",
    "        # Transforma tensor 4D (batch, canales, alto, ancho) → 2D (batch, características)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Primera capa completamente conectada con activación ReLU\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        \n",
    "        # Capa de salida sin activación (valores Q pueden ser negativos)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "# --- 2. AGENTE DDQN CON EXPERIENCE REPLAY Y TARGET NETWORK ---\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agente de Deep Q-Learning con arquitectura CNN para navegación en grilla.\n",
    "    \n",
    "    Implementa un agente de aprendizaje por refuerzo que utiliza una red neuronal\n",
    "    convolucional para procesar estados espaciales y aprender una política óptima\n",
    "    para navegar en un entorno de grilla, evitando venenos y recolectando frutas.\n",
    "    \n",
    "    Características principales:\n",
    "    - CNN para procesamiento de estados espaciales (grilla 5x5)\n",
    "    - Experience replay con buffer de memoria para estabilidad\n",
    "    - Target network para cálculos de valores Q objetivo\n",
    "    - Estrategia epsilon-greedy con decaimiento para exploración\n",
    "    - Optimización Adam para entrenamiento eficiente\n",
    "    \n",
    "    Arquitectura del agente:\n",
    "    1. Red principal: Entrenamiento y selección de acciones\n",
    "    2. Red objetivo: Cálculos estables de valores Q futuro\n",
    "    3. Buffer de experiencias: Almacena transiciones para replay\n",
    "    4. Optimizador: Adam para actualización de pesos\n",
    "    \n",
    "    El agente mejora mediante:\n",
    "    - Exploración inicial alta (epsilon=1.0) para descubrir el entorno\n",
    "    - Decaimiento gradual hacia explotación (epsilon_min=0.01)\n",
    "    - Entrenamiento con experiencias pasadas (experience replay)\n",
    "    - Actualización periódica de la red objetivo para estabilidad\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_shape, action_size):\n",
    "        \"\"\"\n",
    "        Inicializa el agente DDQN con configuración optimizada para el entorno.\n",
    "        \n",
    "        Args:\n",
    "            state_shape (tuple): Forma del estado (canales, altura, ancho)\n",
    "                                Típicamente (3, 5, 5) para grilla con agente/frutas/venenos\n",
    "            action_size (int): Número de acciones posibles (4: arriba, abajo, izq, der)\n",
    "        \n",
    "        Configuración de hiperparámetros:\n",
    "        - memory: 50,000 experiencias para diversidad y estabilidad\n",
    "        - gamma: 0.99 (alta importancia a recompensas futuras)\n",
    "        - epsilon: 1.0→0.01 (exploración total a mínima)\n",
    "        - epsilon_decay: 0.9995 (decaimiento gradual)\n",
    "        - learning_rate: 0.0001 (ajuste fino y estable)\n",
    "        - update_target_every: 5 (frecuencia de actualización de red objetivo)\n",
    "        \"\"\"\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # --- CONFIGURACIÓN DE EXPERIENCE REPLAY ---\n",
    "        self.memory = deque(maxlen=50000)      \n",
    "        # Buffer circular que almacena hasta 50,000 experiencias\n",
    "        # Tamaño grande permite mayor diversidad de experiencias\n",
    "        # Memoria circular: experiencias antiguas se eliminan automáticamente\n",
    "        \n",
    "        # --- PARÁMETROS DE APRENDIZAJE ---\n",
    "        self.gamma = 0.99                     \n",
    "        # Factor de descuento alto para valorar recompensas futuras\n",
    "        # 0.99 significa que recompensas 100 pasos adelante valen ~37% del valor actual\n",
    "        \n",
    "        # --- ESTRATEGIA DE EXPLORACIÓN EPSILON-GREEDY ---\n",
    "        self.epsilon = 1.0                    \n",
    "        # Exploración inicial: 100% acciones aleatorias para descubrir entorno\n",
    "        \n",
    "        self.epsilon_min = 0.01               \n",
    "        # Exploración mínima: siempre mantener 1% de acciones aleatorias\n",
    "        # Evita quedar atrapado en mínimos locales\n",
    "        \n",
    "        self.epsilon_decay = 0.9995           \n",
    "        # Decaimiento gradual: epsilon *= 0.9995 cada episodio\n",
    "        # Transición suave de exploración a explotación\n",
    "        \n",
    "        # --- OPTIMIZACIÓN ---\n",
    "        self.learning_rate = 0.0001           \n",
    "        # Tasa de aprendizaje baja para entrenamiento estable y convergencia suave\n",
    "        # Evita oscilaciones en la función de pérdida\n",
    "        \n",
    "        # --- ACTUALIZACIÓN DE RED OBJETIVO ---\n",
    "        self.update_target_every = 5          \n",
    "        # Frecuencia de actualización de la red objetivo (cada 5 entrenamientos)\n",
    "        # Balance entre estabilidad y adaptación a nuevos pesos\n",
    "        \n",
    "        # --- INICIALIZACIÓN DE REDES NEURONALES ---\n",
    "        h, w = state_shape[1], state_shape[2]  # Dimensiones de la grilla\n",
    "        \n",
    "        # Red principal: Se entrena continuamente con nuevas experiencias\n",
    "        self.model = CNN_DQN(h, w, action_size)\n",
    "        \n",
    "        # Red objetivo: Proporciona valores Q estables para cálculos de objetivo\n",
    "        self.target_model = CNN_DQN(h, w, action_size)\n",
    "        \n",
    "        # Inicializar red objetivo con mismos pesos que red principal\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # --- CONFIGURACIÓN DE ENTRENAMIENTO ---\n",
    "        # Optimizador Adam: Adaptativo, eficiente para redes neuronales\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Función de pérdida: Error cuadrático medio para regresión de valores Q\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Contador de pasos para tracking de actualizaciones\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Actualiza la red objetivo copiando pesos de la red principal.\n",
    "        \n",
    "        La red objetivo es fundamental para la estabilidad del entrenamiento:\n",
    "        - Proporciona valores Q estables para calcular objetivos\n",
    "        - Se actualiza menos frecuentemente que la red principal\n",
    "        - Evita que los objetivos cambien constantemente durante entrenamiento\n",
    "        \n",
    "        Proceso:\n",
    "        1. Copia completa de todos los parámetros de la red principal\n",
    "        2. La red objetivo permanece fija hasta la próxima actualización\n",
    "        3. Garantiza consistencia en los cálculos de valores Q objetivo\n",
    "        \"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Almacena una experiencia en el buffer de memory replay.\n",
    "        \n",
    "        Experience replay es una técnica fundamental en Deep Q-Learning que:\n",
    "        - Rompe correlaciones temporales entre experiencias consecutivas\n",
    "        - Permite reutilizar experiencias valiosas múltiples veces\n",
    "        - Mejora la eficiencia de uso de datos\n",
    "        - Estabiliza el entrenamiento de la red neuronal\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado actual del agente en la grilla\n",
    "                             Forma (3, 5, 5) con canales para agente/frutas/venenos\n",
    "            action (int): Acción tomada (0=arriba, 1=abajo, 2=izq, 3=der)\n",
    "            reward (float): Recompensa recibida por la acción\n",
    "                           +10 por fruta, -10 por veneno, -1 por movimiento\n",
    "            next_state (np.array): Estado resultante después de la acción\n",
    "            done (bool): True si el episodio terminó (todas frutas recogidas)\n",
    "        \n",
    "        El buffer circular (deque) gestiona automáticamente:\n",
    "        - Eliminación de experiencias antiguas cuando se alcanza el límite\n",
    "        - Mantener diversidad de experiencias para entrenamiento robusto\n",
    "        - Acceso eficiente para muestreo aleatorio durante replay\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, explore=True):\n",
    "        \"\"\"\n",
    "        Selecciona una acción usando estrategia epsilon-greedy.\n",
    "        \n",
    "        Implementa el balance crítico entre exploración y explotación:\n",
    "        - Exploración: Necesaria para descubrir nuevas estrategias\n",
    "        - Explotación: Usar conocimiento actual para maximizar recompensas\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado actual del entorno (3, 5, 5)\n",
    "            explore (bool): Si False, siempre usa la mejor acción conocida\n",
    "                           Útil para evaluación sin exploración aleatoria\n",
    "        \n",
    "        Returns:\n",
    "            int: Índice de acción seleccionada (0-3)\n",
    "        \n",
    "        Estrategia epsilon-greedy:\n",
    "        - Probabilidad epsilon: Acción aleatoria (exploración)\n",
    "        - Probabilidad (1-epsilon): Mejor acción según red neuronal (explotación)\n",
    "        \n",
    "        Progresión de epsilon:\n",
    "        - Inicio: ε=1.0 → 100% exploración para mapear el entorno\n",
    "        - Medio: ε~0.5 → Balance exploración/explotación\n",
    "        - Final: ε=0.01 → 99% explotación, 1% exploración residual\n",
    "        \"\"\"\n",
    "        self.steps_done += 1  # Contador para tracking de progreso\n",
    "        \n",
    "        # Exploración: acción aleatoria si epsilon lo determina y explore=True\n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Explotación: usar red neuronal para encontrar mejor acción\n",
    "        # Convertir estado a tensor PyTorch y agregar dimensión de batch\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Inferencia sin calcular gradientes (más eficiente)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state_tensor)\n",
    "        \n",
    "        # Seleccionar acción con mayor valor Q predicho\n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Entrena la red neuronal usando experiencias pasadas con Double DQN.\n",
    "        \n",
    "        Double DQN mejora el algoritmo DQN clásico al separar la selección\n",
    "        y evaluación de acciones, reduciendo la sobrestimación sistemática\n",
    "        de valores Q que puede llevar a políticas subóptimas.\n",
    "        \n",
    "        Diferencias DQN vs Double DQN:\n",
    "        \n",
    "        DQN Clásico:\n",
    "        target = reward + gamma * max(target_network(next_state))\n",
    "        Problema: La misma red selecciona y evalúa → sobrestimación\n",
    "        \n",
    "        Double DQN:\n",
    "        best_action = argmax(main_network(next_state))     # Selección\n",
    "        target = reward + gamma * target_network(next_state)[best_action]  # Evaluación\n",
    "        Ventaja: Separación reduce sesgo de sobrestimación\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Tamaño del lote de experiencias para entrenamiento\n",
    "                             Típicamente 32-64 para balance eficiencia/estabilidad\n",
    "        \n",
    "        Proceso de entrenamiento:\n",
    "        1. Verificar que hay suficientes experiencias en memoria\n",
    "        2. Muestrear batch aleatorio de experiencias\n",
    "        3. Calcular valores Q actuales para estados del batch\n",
    "        4. Aplicar lógica Double DQN para calcular objetivos\n",
    "        5. Computar loss (MSE entre predicciones y objetivos)\n",
    "        6. Backpropagation y actualización de pesos\n",
    "        7. Decrecer epsilon (menos exploración)\n",
    "        8. Aplicar clipping de gradientes para estabilidad\n",
    "        \"\"\"\n",
    "        # No entrenar si memoria insuficiente\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        # --- MUESTREO ALEATORIO DE EXPERIENCIAS ---\n",
    "        # Rompe correlaciones temporales y mejora generalización\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Separar componentes de las experiencias en tensores\n",
    "        states = torch.FloatTensor(np.array([e[0] for e in minibatch]))\n",
    "        actions = torch.LongTensor([e[1] for e in minibatch]).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor([e[2] for e in minibatch]).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(np.array([e[3] for e in minibatch]))\n",
    "        dones = torch.BoolTensor([e[4] for e in minibatch]).unsqueeze(1)\n",
    "\n",
    "        # --- VALORES Q ACTUALES ---\n",
    "        # Calcular Q-values para estados actuales usando red principal\n",
    "        current_q_values = self.model(states).gather(1, actions)\n",
    "        \n",
    "        # --- LÓGICA DOUBLE DQN ---\n",
    "        with torch.no_grad():  # No calcular gradientes para eficiencia\n",
    "            # 1. Red principal SELECCIONA mejor acción para siguiente estado\n",
    "            #    Usa conocimiento más actualizado para selección\n",
    "            best_next_actions = self.model(next_states).max(1)[1].unsqueeze(1)\n",
    "            \n",
    "            # 2. Red objetivo EVALÚA el valor de la acción seleccionada\n",
    "            #    Usa pesos más estables para evaluación consistente\n",
    "            next_q_values_target = self.target_model(next_states).gather(1, best_next_actions)\n",
    "        \n",
    "        # --- CÁLCULO DE OBJETIVOS Q ---\n",
    "        # Si episodio terminó (done=True), no hay valor futuro\n",
    "        # target = reward + descuento * valor_futuro * (no_terminado)\n",
    "        target_q_values = rewards + (self.gamma * next_q_values_target * (~dones))\n",
    "        \n",
    "        # --- ENTRENAMIENTO DE LA RED ---\n",
    "        # Error cuadrático medio entre predicciones y objetivos\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimización con backpropagation\n",
    "        self.optimizer.zero_grad()  # Limpiar gradientes previos\n",
    "        loss.backward()             # Calcular gradientes\n",
    "        \n",
    "        # Clipping de gradientes para prevenir explosión\n",
    "        # Limita gradientes a [-1, 1] para estabilidad numérica\n",
    "        torch.nn.utils.clip_grad_value_(self.model.parameters(), 1)\n",
    "        \n",
    "        self.optimizer.step()       # Actualizar pesos\n",
    "        \n",
    "        # --- DECAIMIENTO DE EXPLORACIÓN ---\n",
    "        # Reducir epsilon gradualmente para transición exploración→explotación\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        \"\"\"\n",
    "        Carga un modelo pre-entrenado desde archivo.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Ruta al archivo .pth con los pesos del modelo\n",
    "        \n",
    "        Funcionalidad:\n",
    "        - Carga pesos de la red principal desde archivo\n",
    "        - Actualiza red objetivo para mantener consistencia\n",
    "        - Permite continuar entrenamiento o hacer inferencia\n",
    "        - Preserva arquitectura de red definida en __init__\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "        self.update_target_network()  # Sincronizar red objetivo\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Guarda el modelo entrenado en un archivo.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Ruta donde guardar el archivo .pth\n",
    "        \n",
    "        Funcionalidad:\n",
    "        - Guarda solo los pesos de la red principal (más compacto)\n",
    "        - La red objetivo se puede reconstruir al cargar\n",
    "        - Formato PyTorch estándar para compatibilidad\n",
    "        - Permite reutilizar modelos entrenados\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe144e4d",
   "metadata": {},
   "source": [
    "#### enviroment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment.py\n",
    "\"\"\"\n",
    "Entorno de grilla para el entrenamiento de agentes de aprendizaje por refuerzo.\n",
    "Este módulo implementa un entorno de grilla donde un agente debe recolectar frutas \n",
    "mientras evita venenos, diseñado específicamente para algoritmos DDQN.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GridEnvironment:\n",
    "    \"\"\"\n",
    "    Entorno de grilla 2D para simulación de agentes que recolectan frutas y evitan venenos.\n",
    "    \n",
    "    El entorno consiste en una grilla cuadrada donde:\n",
    "    - El agente se mueve en 4 direcciones (arriba, abajo, izquierda, derecha)\n",
    "    - Las frutas otorgan recompensas positivas cuando son recolectadas\n",
    "    - Los venenos causan penalizaciones y resetean la posición del agente\n",
    "    - El objetivo es recolectar todas las frutas minimizando las penalizaciones\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tamaño de la grilla (size x size)\n",
    "        start_pos (tuple): Posición inicial del agente en cada episodio\n",
    "        agent_pos (np.array): Posición actual del agente\n",
    "        fruit_pos (list): Lista de posiciones de las frutas\n",
    "        poison_pos (list): Lista de posiciones de los venenos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de grilla.\n",
    "        \n",
    "        Args:\n",
    "            size (int, optional): Tamaño de la grilla cuadrada. Por defecto es 5x5.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.start_pos = (0, 0)  # Posición inicial por defecto\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, agent_pos=(0, 0), fruit_pos=[], poison_pos=[]):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno con una configuración específica.\n",
    "        \n",
    "        Este método prepara el entorno para un nuevo episodio, estableciendo las posiciones\n",
    "        iniciales del agente, frutas y venenos. Es crucial para el entrenamiento ya que\n",
    "        permite configurar diferentes escenarios de aprendizaje.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (tuple, optional): Posición inicial del agente (x, y). Por defecto (0, 0).\n",
    "            fruit_pos (list, optional): Lista de tuplas con posiciones de frutas. Por defecto vacía.\n",
    "            poison_pos (list, optional): Lista de tuplas con posiciones de venenos. Por defecto vacía.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno como tensor 3D (canales, altura, anchura).\n",
    "        \"\"\"\n",
    "        self.start_pos = np.array(agent_pos)  # Guardamos la posición inicial del episodio\n",
    "        self.agent_pos = np.array(agent_pos)\n",
    "        self.fruit_pos = [np.array(p) for p in fruit_pos]\n",
    "        self.poison_pos = [np.array(p) for p in poison_pos]\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Obtiene el estado actual del entorno como una representación tensorial.\n",
    "        \n",
    "        El estado se representa como un tensor 3D de forma (3, size, size) donde:\n",
    "        - Canal 0: Posición del agente (1.0 en la posición actual, 0.0 en el resto)\n",
    "        - Canal 1: Posiciones de las frutas (1.0 donde hay frutas, 0.0 en el resto)\n",
    "        - Canal 2: Posiciones de los venenos (1.0 donde hay venenos, 0.0 en el resto)\n",
    "        \n",
    "        Esta representación permite que las redes neuronales procesen eficientemente\n",
    "        la información espacial del entorno usando convoluciones.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Tensor 3D de forma (3, size, size) representando el estado actual.\n",
    "        \"\"\"\n",
    "        state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posición del agente\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de las frutas\n",
    "        for fruit in self.fruit_pos:\n",
    "            state[1, fruit[0], fruit[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de los venenos\n",
    "        for poison in self.poison_pos:\n",
    "            state[2, poison[0], poison[1]] = 1.0\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción en el entorno y devuelve el resultado.\n",
    "        \n",
    "        Este método implementa la lógica principal del entorno, procesando las acciones\n",
    "        del agente y calculando las recompensas correspondientes. Incluye manejo especial\n",
    "        para venenos que resetean la posición del agente sin terminar el episodio.\n",
    "        \n",
    "        Args:\n",
    "            action (int): Acción a ejecutar\n",
    "                - 0: Mover hacia arriba (decrementar fila)\n",
    "                - 1: Mover hacia abajo (incrementar fila)\n",
    "                - 2: Mover hacia la izquierda (decrementar columna)\n",
    "                - 3: Mover hacia la derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, episodio_terminado)\n",
    "                - nuevo_estado (np.array): Estado resultante después de la acción\n",
    "                - recompensa (float): Recompensa obtenida por la acción\n",
    "                - episodio_terminado (bool): True si el episodio ha terminado\n",
    "        \n",
    "        Lógica de recompensas:\n",
    "            - Movimiento básico: -0.05 (costo de vida)\n",
    "            - Tocar veneno: -10.0 (penalización fuerte + reset a posición inicial)\n",
    "            - Recolectar fruta: +1.0 (recompensa por objetivo)\n",
    "            - Completar nivel: +10.0 (bonus por recolectar todas las frutas)\n",
    "        \"\"\"\n",
    "        # Ejecutar el movimiento según la acción seleccionada\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] -= 1    # Mover hacia arriba\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] += 1    # Mover hacia abajo\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] -= 1    # Mover hacia la izquierda\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] += 1    # Mover hacia la derecha\n",
    "        \n",
    "        # Asegurar que el agente permanezca dentro de los límites de la grilla\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.size - 1)\n",
    "\n",
    "        # Recompensa base por cada movimiento (costo de vida)\n",
    "        reward = -0.05\n",
    "        done = False\n",
    "\n",
    "        # --- LÓGICA DE MANEJO DE VENENOS ---\n",
    "        # Verificar si el agente tocó algún veneno\n",
    "        if any(np.array_equal(self.agent_pos, p) for p in self.poison_pos):\n",
    "            reward = -10.0  # Penalización severa por tocar veneno\n",
    "            self.agent_pos = np.copy(self.start_pos)  # Resetear a posición inicial\n",
    "            # IMPORTANTE: done NO es True. El episodio continúa después del reset.\n",
    "        else:\n",
    "            # --- LÓGICA DE RECOLECCIÓN DE FRUTAS ---\n",
    "            # Esta lógica solo se ejecuta si NO se tocó un veneno\n",
    "            eaten_fruit_this_step = False\n",
    "            \n",
    "            # Verificar si el agente recolectó alguna fruta\n",
    "            for i, fruit in enumerate(self.fruit_pos):\n",
    "                if np.array_equal(self.agent_pos, fruit):\n",
    "                    reward += 1.0  # Recompensa por recolectar fruta\n",
    "                    self.fruit_pos.pop(i)  # Remover la fruta recolectada\n",
    "                    eaten_fruit_this_step = True\n",
    "                    break\n",
    "\n",
    "            # Opcional: Aquí se puede agregar reward shaping basado en distancia\n",
    "            if not eaten_fruit_this_step and self.fruit_pos:\n",
    "                # Ejemplo: reward += -0.01 * distancia_a_fruta_más_cercana\n",
    "                pass  # Actualmente no implementado\n",
    "\n",
    "            # --- CONDICIÓN DE VICTORIA ---\n",
    "            # Si no quedan frutas, el episodio termina exitosamente\n",
    "            if not self.fruit_pos:\n",
    "                done = True\n",
    "                reward += 10.0  # Bonus por completar el nivel\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94c721",
   "metadata": {},
   "source": [
    "#### interfaz.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420bcad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn_agente_comefrutas.py (versión integrada con interfaz gráfica completa)\n",
    "\"\"\"\n",
    "Interfaz gráfica para el entrenamiento y visualización de agentes DDQN.\n",
    "\n",
    "Este módulo implementa una interfaz gráfica completa usando Pygame que permite:\n",
    "- Configurar entornos de manera interactiva\n",
    "- Visualizar el comportamiento del agente entrenado\n",
    "- Alternar entre modo setup y modo juego\n",
    "- Gestionar elementos del entorno (frutas, venenos, paredes)\n",
    "\n",
    "El sistema está diseñado para facilitar la experimentación con diferentes\n",
    "configuraciones de entorno y la evaluación visual del rendimiento del agente.\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from agent import Agent\n",
    "\n",
    "# --- CONFIGURACIÓN GENERAL ---\n",
    "\"\"\"Constantes de configuración para la interfaz gráfica.\"\"\"\n",
    "GRID_WIDTH = 5          # Ancho de la grilla en celdas\n",
    "GRID_HEIGHT = 5         # Alto de la grilla en celdas\n",
    "CELL_SIZE = 120         # Tamaño de cada celda en píxeles\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la pantalla\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto total de la pantalla\n",
    "\n",
    "# Paleta de colores para la interfaz\n",
    "COLOR_FONDO = (25, 25, 25)        # Fondo oscuro\n",
    "COLOR_LINEAS = (40, 40, 40)       # Líneas de la grilla\n",
    "COLOR_CURSOR = (255, 255, 0)      # Cursor amarillo en modo setup\n",
    "COLOR_TEXTO = (230, 230, 230)     # Texto claro\n",
    "\n",
    "\n",
    "# --- ENTORNO PARA DDQN (misma estructura visual que Q-learning) ---\n",
    "class EntornoGrid:\n",
    "    \"\"\"\n",
    "    Entorno de grilla con interfaz gráfica para agentes DDQN.\n",
    "    \n",
    "    Esta clase maneja tanto la lógica del entorno como su representación visual,\n",
    "    proporcionando una interfaz interactiva para configurar y visualizar el\n",
    "    comportamiento del agente. Compatible con la arquitectura DDQN.\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tamaño de la grilla\n",
    "        agent_pos (tuple): Posición actual del agente (x, y)\n",
    "        frutas (set): Conjunto de posiciones con frutas\n",
    "        venenos (set): Conjunto de posiciones con venenos\n",
    "        paredes (set): Conjunto de posiciones con paredes (obstáculos)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de grilla con configuración por defecto.\n",
    "        \n",
    "        El agente comienza en la posición (0,0) y todos los conjuntos de\n",
    "        elementos están vacíos inicialmente.\n",
    "        \"\"\"\n",
    "        self.size = GRID_WIDTH\n",
    "        self.agent_pos = (0, 0)\n",
    "        self.frutas = set()\n",
    "        self.venenos = set()\n",
    "        self.paredes = set()\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"\n",
    "        Resetea el agente a su posición inicial sin modificar el entorno.\n",
    "        \n",
    "        Esta función es útil para reiniciar episodios manteniendo la misma\n",
    "        configuración de frutas, venenos y paredes establecida en modo setup.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno después del reset.\n",
    "        \"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno excepto el agente.\n",
    "        \n",
    "        Esta función es útil para limpiar completamente el entorno y comenzar\n",
    "        una nueva configuración desde cero en modo setup.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción del agente y actualiza el estado del entorno.\n",
    "        \n",
    "        Este método implementa la lógica de movimiento y las reglas del juego,\n",
    "        incluyendo colisiones con paredes, recolección de frutas y penalizaciones\n",
    "        por venenos.\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Acción a ejecutar\n",
    "                - 0: Mover hacia arriba (y-1)\n",
    "                - 1: Mover hacia abajo (y+1)\n",
    "                - 2: Mover hacia la izquierda (x-1)\n",
    "                - 3: Mover hacia la derecha (x+1)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (estado, recompensa, terminado)\n",
    "                - estado (np.array): Nuevo estado del entorno\n",
    "                - recompensa (float): Recompensa obtenida por la acción\n",
    "                - terminado (bool): True si el episodio ha terminado\n",
    "        \n",
    "        Lógica de recompensas:\n",
    "            - Colisión con pared/límite: -0.1 (sin movimiento)\n",
    "            - Movimiento válido: -0.05 (costo de vida)\n",
    "            - Tocar veneno: -10.0 (penalización + reset a origen)\n",
    "            - Recolectar fruta: +1.0\n",
    "            - Completar nivel: +10.0 adicional\n",
    "        \"\"\"\n",
    "        x, y = self.agent_pos\n",
    "        \n",
    "        # Calcular nueva posición según la acción\n",
    "        if accion == 0:\n",
    "            y -= 1    # Mover hacia arriba\n",
    "        elif accion == 1:\n",
    "            y += 1    # Mover hacia abajo\n",
    "        elif accion == 2:\n",
    "            x -= 1    # Mover hacia la izquierda\n",
    "        elif accion == 3:\n",
    "            x += 1    # Mover hacia la derecha\n",
    "\n",
    "        # Verificar colisiones con límites de la grilla o paredes\n",
    "        if (\n",
    "            x < 0\n",
    "            or x >= GRID_WIDTH\n",
    "            or y < 0\n",
    "            or y >= GRID_HEIGHT\n",
    "            or (x, y) in self.paredes\n",
    "        ):\n",
    "            # Movimiento inválido: penalización menor y no se mueve\n",
    "            return self.get_state(), -0.1, False\n",
    "\n",
    "        # Movimiento válido: actualizar posición del agente\n",
    "        self.agent_pos = (x, y)\n",
    "        recompensa = -0.05  # Costo base por movimiento\n",
    "        terminado = False\n",
    "\n",
    "        # Verificar interacciones con elementos del entorno\n",
    "        if self.agent_pos in self.venenos:\n",
    "            # Penalización por tocar veneno y reset a posición inicial\n",
    "            recompensa = -10.0\n",
    "            self.agent_pos = (0, 0)\n",
    "        elif self.agent_pos in self.frutas:\n",
    "            # Recompensa por recolectar fruta\n",
    "            recompensa = 1.0\n",
    "            self.frutas.remove(self.agent_pos)\n",
    "            \n",
    "            # Verificar si se completó el nivel (no quedan frutas)\n",
    "            if not self.frutas:\n",
    "                recompensa += 10.0  # Bonus por completar\n",
    "                terminado = True\n",
    "\n",
    "        return self.get_state(), recompensa, terminado\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Obtiene la representación del estado actual como tensor 3D.\n",
    "        \n",
    "        Convierte el estado del entorno en un formato compatible con redes\n",
    "        neuronales convolucionales, usando 3 canales para representar\n",
    "        diferentes tipos de elementos.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Tensor 3D de forma (3, size, size) donde:\n",
    "                - Canal 0: Posición del agente (1.0 donde está el agente)\n",
    "                - Canal 1: Posiciones de frutas (1.0 donde hay frutas)\n",
    "                - Canal 2: Posiciones de venenos (1.0 donde hay venenos)\n",
    "                \n",
    "        Nota: Las paredes no se incluyen en el estado ya que son estáticas\n",
    "              y se manejan a través de las restricciones de movimiento.\n",
    "        \"\"\"\n",
    "        estado = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posición del agente\n",
    "        estado[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruta in self.frutas:\n",
    "            estado[1, fruta[0], fruta[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de venenos\n",
    "        for veneno in self.venenos:\n",
    "            estado[2, veneno[0], veneno[1]] = 1.0\n",
    "            \n",
    "        return estado\n",
    "\n",
    "    def dibujar(\n",
    "        self,\n",
    "        pantalla,\n",
    "        modo_juego,\n",
    "        cursor_pos,\n",
    "        img_fruta,\n",
    "        img_veneno,\n",
    "        img_pared,\n",
    "        img_agente,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Renderiza el entorno completo en la pantalla de Pygame.\n",
    "        \n",
    "        Este método se encarga de dibujar todos los elementos visuales del juego,\n",
    "        incluyendo la grilla, elementos del entorno, el agente, el cursor (en modo setup)\n",
    "        y la información de controles.\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde dibujar\n",
    "            modo_juego (str): Modo actual (\"SETUP\" o \"PLAYING\")\n",
    "            cursor_pos (tuple): Posición del cursor en modo setup\n",
    "            img_fruta (pygame.Surface): Imagen de la fruta\n",
    "            img_veneno (pygame.Surface): Imagen del veneno\n",
    "            img_pared (pygame.Surface): Imagen de la pared\n",
    "            img_agente (pygame.Surface): Imagen del agente\n",
    "        \n",
    "        Elementos visuales renderizados:\n",
    "            1. Fondo y grilla\n",
    "            2. Paredes (obstáculos estáticos)\n",
    "            3. Frutas (objetivos a recolectar)\n",
    "            4. Venenos (elementos a evitar)\n",
    "            5. Agente (jugador controlado por IA)\n",
    "            6. Cursor (solo en modo setup)\n",
    "            7. Información de controles y modo actual\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar líneas de la grilla (verticales)\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "            \n",
    "        # Dibujar líneas de la grilla (horizontales)\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibujar elementos del entorno en orden de capas\n",
    "        # 1. Paredes (fondo)\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "            \n",
    "        # 2. Frutas (objetivos)\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "            \n",
    "        # 3. Venenos (peligros)\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "\n",
    "        # 4. Agente (primer plano)\n",
    "        pantalla.blit(\n",
    "            img_agente, (self.agent_pos[0] * CELL_SIZE, self.agent_pos[1] * CELL_SIZE)\n",
    "        )\n",
    "\n",
    "        # 5. Cursor (solo en modo setup)\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar información textual\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        \n",
    "        # Información del modo actual\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Controles para modo setup\n",
    "        controles1 = font.render(\n",
    "            \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\", True, COLOR_TEXTO\n",
    "        )\n",
    "        \n",
    "        # Controles generales\n",
    "        controles2 = font.render(\"P=Jugar, S=Setup\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Posicionar textos en la parte inferior\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles1, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles2, (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "\n",
    "# --- MAIN CON INTERFAZ COMPLETA ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta la interfaz gráfica completa del sistema DDQN.\n",
    "    \n",
    "    Esta función implementa el bucle principal del programa, manejando:\n",
    "    - Inicialización de Pygame y carga de recursos\n",
    "    - Gestión de eventos de teclado para ambos modos\n",
    "    - Alternancia entre modo setup y modo juego\n",
    "    - Renderizado continuo de la interfaz\n",
    "    - Ejecución automática del agente en modo juego\n",
    "    \n",
    "    Modos de operación:\n",
    "        SETUP: Permite configurar el entorno interactivamente\n",
    "            - Flechas: Mover cursor\n",
    "            - F: Añadir/quitar fruta\n",
    "            - V: Añadir/quitar veneno  \n",
    "            - W: Añadir/quitar pared\n",
    "            - C: Limpiar entorno\n",
    "            \n",
    "        PLAYING: El agente entrenado juega automáticamente\n",
    "            - Usa el modelo DDQN cargado para tomar decisiones\n",
    "            - Visualiza el comportamiento del agente en tiempo real\n",
    "            - Termina automáticamente y vuelve a setup al completar\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y crear ventana\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente DDQN - Come Frutas 🍓☠️\")\n",
    "\n",
    "    def cargar_img(nombre, color_fallback):\n",
    "        \"\"\"\n",
    "        Carga una imagen desde archivo con fallback a color sólido.\n",
    "        \n",
    "        Esta función auxiliar intenta cargar una imagen desde el directorio\n",
    "        del script. Si falla, crea una superficie de color sólido como respaldo.\n",
    "        \n",
    "        Args:\n",
    "            nombre (str): Nombre del archivo de imagen\n",
    "            color_fallback (tuple): Color RGB de respaldo si falla la carga\n",
    "            \n",
    "        Returns:\n",
    "            pygame.Surface: Superficie escalada al tamaño de celda\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "            img = pygame.image.load(ruta).convert_alpha()\n",
    "            return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "        except:\n",
    "            # Crear superficie de color sólido como fallback\n",
    "            surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "            surf.fill(color_fallback)\n",
    "            return surf\n",
    "\n",
    "    # Cargar imágenes con colores de respaldo\n",
    "    img_fruta = cargar_img(\"fruta.png\", (0, 255, 0))      # Verde si falla\n",
    "    img_veneno = cargar_img(\"veneno.png\", (255, 0, 0))     # Rojo si falla\n",
    "    img_pared = cargar_img(\"pared.png\", (100, 100, 100))   # Gris si falla\n",
    "    img_agente = cargar_img(\"agente.png\", (0, 0, 255))     # Azul si falla\n",
    "\n",
    "    # Inicializar componentes principales\n",
    "    entorno = EntornoGrid()\n",
    "    agente = Agent(state_shape=(3, GRID_HEIGHT, GRID_WIDTH), action_size=4)\n",
    "    agente.load(\"dqn_model.pth\")  # Cargar modelo entrenado\n",
    "\n",
    "    # Variables de control de la interfaz\n",
    "    cursor_pos = [0, 0]      # Posición del cursor en modo setup\n",
    "    modo_juego = \"SETUP\"     # Modo inicial\n",
    "    reloj = pygame.time.Clock()  # Control de FPS\n",
    "    corriendo = True\n",
    "\n",
    "    # Bucle principal del programa\n",
    "    while corriendo:\n",
    "        # Procesar eventos de Pygame\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # Cambios de modo (disponibles en cualquier momento)\n",
    "                if evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO JUEGO ---\")\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    time.sleep(0.5)  # Pausa para evitar acciones inmediatas\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # Controles específicos del modo SETUP\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Movimiento del cursor\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Gestión de elementos en la posición del cursor\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    \n",
    "                    if evento.key == pygame.K_f:\n",
    "                        # Alternar fruta en posición actual\n",
    "                        if pos in entorno.frutas:\n",
    "                            entorno.frutas.remove(pos)\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos)\n",
    "                            # Remover otros elementos de la misma posición\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        # Alternar veneno en posición actual\n",
    "                        if pos in entorno.venenos:\n",
    "                            entorno.venenos.remove(pos)\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos)\n",
    "                            # Remover otros elementos de la misma posición\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        # Alternar pared en posición actual\n",
    "                        if pos in entorno.paredes:\n",
    "                            entorno.paredes.remove(pos)\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos)\n",
    "                            # Remover otros elementos de la misma posición\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        # Limpiar todo el entorno\n",
    "                        print(\"--- LIMPIANDO ENTORNO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "\n",
    "        # Lógica del modo PLAYING (agente automático)\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # Obtener estado actual del entorno\n",
    "            estado = entorno.get_state()\n",
    "            \n",
    "            # El agente elige una acción usando el modelo entrenado\n",
    "            # explore=False significa que usa solo explotación (sin exploración)\n",
    "            accion = agente.choose_action(estado, explore=False)\n",
    "            \n",
    "            # Ejecutar la acción en el entorno\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            \n",
    "            # Si el episodio terminó, volver al modo setup\n",
    "            if terminado:\n",
    "                print(\"Juego terminado. Volviendo a SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "                \n",
    "            # Controlar velocidad de visualización (10 FPS para el agente)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # Renderizado de la interfaz\n",
    "        # Crear superficie completa incluyendo espacio para texto\n",
    "        pantalla_con_info = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "        pantalla_con_info.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar el entorno en la superficie\n",
    "        entorno.dibujar(\n",
    "            pantalla_con_info,\n",
    "            modo_juego,\n",
    "            tuple(cursor_pos),\n",
    "            img_fruta,\n",
    "            img_veneno,\n",
    "            img_pared,\n",
    "            img_agente,\n",
    "        )\n",
    "        \n",
    "        # Copiar la superficie completa a la pantalla principal\n",
    "        pantalla.blit(pantalla_con_info, (0, 0))\n",
    "        \n",
    "        # Actualizar la pantalla y controlar FPS\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(60)  # Limitar a 60 FPS para suavidad visual\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa.\n",
    "    \n",
    "    Ejecuta la función main() solo cuando el archivo se ejecuta directamente,\n",
    "    no cuando se importa como módulo. Esto permite reutilizar las clases\n",
    "    y funciones en otros scripts sin ejecutar automáticamente la interfaz.\n",
    "    \"\"\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf36f8",
   "metadata": {},
   "source": [
    "#### interfaztrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn_agente_comefrutas.py\n",
    "\"\"\"\n",
    "Interfaz de entrenamiento interactiva para agentes DDQN.\n",
    "\n",
    "Este módulo proporciona una interfaz gráfica completa que permite:\n",
    "- Configurar entornos de entrenamiento de manera interactiva\n",
    "- Entrenar agentes DDQN con visualización en tiempo real\n",
    "- Evaluar el rendimiento del agente después del entrenamiento\n",
    "- Gestionar el ciclo completo de desarrollo de IA: setup → entrenamiento → evaluación\n",
    "\n",
    "La interfaz integra tres modos principales:\n",
    "1. SETUP: Configuración interactiva del entorno\n",
    "2. TRAINING: Entrenamiento automático del agente DDQN\n",
    "3. PLAYING: Evaluación visual del agente entrenado\n",
    "\n",
    "Diseñado para facilitar la experimentación y el desarrollo iterativo de agentes\n",
    "de aprendizaje por refuerzo en entornos de grilla.\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from agent import Agent\n",
    "from environment import GridEnvironment\n",
    "\n",
    "# --- CONFIGURACIÓN DEL ENTORNO Y VISUALIZACIÓN ---\n",
    "\"\"\"Parámetros principales del sistema de entrenamiento.\"\"\"\n",
    "GRID_SIZE = 5           # Tamaño de la grilla (5x5)\n",
    "CELL_SIZE = 120         # Tamaño en píxeles de cada celda\n",
    "SCREEN_WIDTH = GRID_SIZE * CELL_SIZE    # Ancho total de la ventana\n",
    "SCREEN_HEIGHT = GRID_SIZE * CELL_SIZE   # Alto total de la ventana\n",
    "\n",
    "# Esquema de colores para la interfaz\n",
    "COLOR_FONDO = (25, 25, 25)        # Fondo oscuro para mejor contraste\n",
    "COLOR_LINEAS = (40, 40, 40)       # Líneas sutiles de la grilla\n",
    "COLOR_CURSOR = (255, 255, 0)      # Cursor amarillo brillante\n",
    "COLOR_TEXTO = (230, 230, 230)     # Texto claro y legible\n",
    "\n",
    "# --- PARÁMETROS DE ENTRENAMIENTO ---\n",
    "\"\"\"Configuración del proceso de entrenamiento DDQN.\"\"\"\n",
    "NUM_EPISODIOS_ENTRENAMIENTO = 3000    # Número total de episodios de entrenamiento\n",
    "BATCH_SIZE = 128                      # Tamaño del lote para replay de experiencias\n",
    "\n",
    "\n",
    "def cargar_imagen(ruta, color_si_falla):\n",
    "    \"\"\"\n",
    "    Carga una imagen desde archivo con sistema de fallback robusto.\n",
    "    \n",
    "    Esta función implementa un mecanismo de carga de imágenes que garantiza\n",
    "    que el programa funcione incluso si los archivos de imagen no están\n",
    "    disponibles, creando superficies de color como respaldo.\n",
    "    \n",
    "    Args:\n",
    "        ruta (str): Ruta relativa o absoluta al archivo de imagen\n",
    "        color_si_falla (tuple): Color RGB (r, g, b) a usar si falla la carga\n",
    "        \n",
    "    Returns:\n",
    "        pygame.Surface: Superficie escalada al tamaño de celda, ya sea la\n",
    "                       imagen cargada o una superficie de color sólido\n",
    "                       \n",
    "    Características:\n",
    "        - Manejo automático de transparencia (convert_alpha)\n",
    "        - Escalado automático al tamaño de celda\n",
    "        - Fallback graceful a color sólido\n",
    "        - Compatible con todos los formatos soportados por Pygame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Intentar cargar la imagen desde archivo\n",
    "        img = pygame.image.load(ruta).convert_alpha()\n",
    "        # Escalar al tamaño exacto de celda para consistencia visual\n",
    "        return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        # Si falla la carga, crear superficie de color sólido\n",
    "        surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "        surf.fill(color_si_falla)\n",
    "        return surf\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta la interfaz de entrenamiento DDQN.\n",
    "    \n",
    "    Esta función implementa un sistema completo de desarrollo de agentes IA que incluye:\n",
    "    \n",
    "    1. **Configuración Interactiva (Modo SETUP)**:\n",
    "       - Diseño visual del entorno usando cursor\n",
    "       - Colocación de frutas, venenos y paredes\n",
    "       - Validación de configuraciones\n",
    "    \n",
    "    2. **Entrenamiento Automatizado (Modo TRAINING)**:\n",
    "       - Ejecución de algoritmo DDQN completo\n",
    "       - Actualización de redes objetivo\n",
    "       - Monitoreo de progreso en tiempo real\n",
    "       - Gestión de memoria de experiencias\n",
    "    \n",
    "    3. **Evaluación Visual (Modo PLAYING)**:\n",
    "       - Visualización del comportamiento aprendido\n",
    "       - Modo sin exploración (solo explotación)\n",
    "       - Análisis cualitativo del rendimiento\n",
    "    \n",
    "    Flujo de trabajo típico:\n",
    "        SETUP → TRAINING → PLAYING → [iteración]\n",
    "    \n",
    "    La interfaz permite experimentación rápida con diferentes configuraciones\n",
    "    de entorno y hiperparámetros de entrenamiento.\n",
    "    \"\"\"\n",
    "    # Inicialización del sistema gráfico\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente Come-Frutas DDQN 🍓☠️\")\n",
    "\n",
    "    # Cargar recursos visuales con colores de fallback específicos\n",
    "    img_fruta = cargar_imagen(\"fruta.png\", (40, 200, 40))    # Verde si falla\n",
    "    img_veneno = cargar_imagen(\"veneno.png\", (200, 40, 40))   # Rojo si falla\n",
    "    img_pared = cargar_imagen(\"pared.png\", (100, 100, 100))   # Gris si falla\n",
    "    img_agente = cargar_imagen(\"agente.png\", (40, 200, 40))   # Verde si falla\n",
    "\n",
    "    # Inicialización de componentes principales del sistema\n",
    "    entorno = GridEnvironment(size=GRID_SIZE)  # Entorno de simulación\n",
    "    agente = Agent(state_shape=(3, GRID_SIZE, GRID_SIZE), action_size=4)  # Agente DDQN\n",
    "\n",
    "    # Variables de control de la interfaz\n",
    "    cursor_pos = [0, 0]        # Posición del cursor en modo setup\n",
    "    modo_juego = \"SETUP\"       # Estado inicial del sistema\n",
    "    reloj = pygame.time.Clock()  # Control de framerate\n",
    "    corriendo = True           # Flag principal del bucle\n",
    "\n",
    "    # Conjuntos para gestionar elementos del entorno configurables\n",
    "    frutas = set()    # Posiciones de objetivos (recompensa positiva)\n",
    "    venenos = set()   # Posiciones de peligros (penalización)\n",
    "    paredes = set()   # Posiciones de obstáculos (bloqueo de movimiento)\n",
    "\n",
    "    # Bucle principal del sistema de entrenamiento\n",
    "    while corriendo:\n",
    "        # Procesamiento de eventos del usuario\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # --- CONTROL DE ENTRENAMIENTO (Tecla T) ---\n",
    "                if evento.key == pygame.K_t:\n",
    "                    if modo_juego != \"TRAINING\":\n",
    "                        print(\"--- ENTRENANDO DDQN ---\")\n",
    "                        modo_juego = \"TRAINING\"\n",
    "                        \n",
    "                        # Bucle principal de entrenamiento DDQN\n",
    "                        for episodio in range(NUM_EPISODIOS_ENTRENAMIENTO):\n",
    "                            # Reiniciar entorno con configuración actual\n",
    "                            estado = entorno.reset(\n",
    "                                agent_pos=(0, 0),\n",
    "                                fruit_pos=list(frutas),\n",
    "                                poison_pos=list(venenos),\n",
    "                            )\n",
    "                            \n",
    "                            # Variables de control del episodio\n",
    "                            terminado = False\n",
    "                            total_reward = 0\n",
    "                            \n",
    "                            # Bucle del episodio individual\n",
    "                            while not terminado:\n",
    "                                # El agente elige acción con exploración activa\n",
    "                                accion = agente.choose_action(estado, explore=True)\n",
    "                                \n",
    "                                # Ejecutar acción y observar resultado\n",
    "                                nuevo_estado, recompensa, terminado = entorno.step(accion)\n",
    "                                \n",
    "                                # Almacenar experiencia en memoria de replay\n",
    "                                agente.remember(\n",
    "                                    estado, accion, recompensa, nuevo_estado, terminado\n",
    "                                )\n",
    "                                \n",
    "                                # Entrenar la red con experiencias pasadas\n",
    "                                agente.replay(BATCH_SIZE)\n",
    "                                \n",
    "                                # Actualización periódica de la red objetivo\n",
    "                                if agente.steps_done % agente.update_target_every == 0:\n",
    "                                    agente.update_target_network()\n",
    "                                \n",
    "                                # Preparar para siguiente paso\n",
    "                                estado = nuevo_estado\n",
    "                                total_reward += recompensa\n",
    "                            \n",
    "                            # Reporte de progreso cada 100 episodios\n",
    "                            if (episodio + 1) % 100 == 0:\n",
    "                                print(\n",
    "                                    f\"Ep {episodio+1}, Reward: {total_reward:.2f}, Epsilon: {agente.epsilon:.3f}\"\n",
    "                                )\n",
    "                        \n",
    "                        print(\"--- ENTRENAMIENTO COMPLETO ---\")\n",
    "                        modo_juego = \"PLAYING\"  # Cambiar automáticamente a evaluación\n",
    "\n",
    "                # --- CONTROL DE MODOS (Teclas P y S) ---\n",
    "                elif evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO PLAYING ---\")\n",
    "                    # Reiniciar entorno para evaluación\n",
    "                    entorno.reset(\n",
    "                        agent_pos=(0, 0),\n",
    "                        fruit_pos=list(frutas),\n",
    "                        poison_pos=list(venenos),\n",
    "                    )\n",
    "                    modo_juego = \"PLAYING\"\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # --- CONTROLES DEL MODO SETUP ---\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Control de navegación del cursor\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_SIZE - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_SIZE - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Conversión de coordenadas (cursor usa x,y pero entorno usa y,x)\n",
    "                    pos = tuple(cursor_pos[::-1])\n",
    "                    \n",
    "                    # Gestión de elementos en la posición del cursor\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        # Alternar fruta en posición actual\n",
    "                        if pos in frutas:\n",
    "                            frutas.remove(pos)\n",
    "                        else:\n",
    "                            frutas.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posición\n",
    "                            venenos.discard(pos)\n",
    "                            paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        # Alternar veneno en posición actual\n",
    "                        if pos in venenos:\n",
    "                            venenos.remove(pos)\n",
    "                        else:\n",
    "                            venenos.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posición\n",
    "                            frutas.discard(pos)\n",
    "                            paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        # Alternar pared en posición actual\n",
    "                        if pos in paredes:\n",
    "                            paredes.remove(pos)\n",
    "                        else:\n",
    "                            paredes.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posición\n",
    "                            frutas.discard(pos)\n",
    "                            venenos.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        # Limpiar completamente el entorno\n",
    "                        frutas.clear()\n",
    "                        venenos.clear()\n",
    "                        paredes.clear()\n",
    "\n",
    "        # --- LÓGICA DEL MODO PLAYING ---\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # Obtener estado actual del entorno\n",
    "            estado = entorno.get_state()\n",
    "            \n",
    "            # El agente toma decisiones sin exploración (solo explotación)\n",
    "            accion = agente.choose_action(estado, explore=False)\n",
    "            \n",
    "            # Ejecutar acción y verificar si terminó el episodio\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            \n",
    "            if terminado:\n",
    "                print(\"Juego terminado. Volviendo a SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "            \n",
    "            # Control de velocidad de visualización\n",
    "            time.sleep(0.1)  # 10 FPS para observar mejor el comportamiento\n",
    "\n",
    "        # --- SISTEMA DE RENDERIZADO COMPLETO ---\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar grilla de referencia\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Renderizar elementos del entorno (orden de capas importante)\n",
    "        # Nota: Las coordenadas se invierten para coincidir con el sistema visual\n",
    "        \n",
    "        # 1. Paredes (capa de fondo)\n",
    "        for pared in paredes:\n",
    "            pantalla.blit(img_pared, (pared[1] * CELL_SIZE, pared[0] * CELL_SIZE))\n",
    "            \n",
    "        # 2. Frutas (objetivos)\n",
    "        for fruta in frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[1] * CELL_SIZE, fruta[0] * CELL_SIZE))\n",
    "            \n",
    "        # 3. Venenos (peligros)\n",
    "        for veneno in venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[1] * CELL_SIZE, veneno[0] * CELL_SIZE))\n",
    "\n",
    "        # 4. Agente (solo visible fuera del modo setup)\n",
    "        if modo_juego != \"SETUP\":\n",
    "            pos = entorno.agent_pos\n",
    "            pantalla.blit(img_agente, (pos[1] * CELL_SIZE, pos[0] * CELL_SIZE))\n",
    "\n",
    "        # 5. Cursor (solo visible en modo setup)\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # --- INTERFAZ DE INFORMACIÓN Y CONTROLES ---\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        \n",
    "        # Mostrar modo actual\n",
    "        pantalla.blit(\n",
    "            font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO),\n",
    "            (10, SCREEN_HEIGHT + 5),\n",
    "        )\n",
    "        \n",
    "        # Controles para modo setup\n",
    "        pantalla.blit(\n",
    "            font.render(\n",
    "                \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\",\n",
    "                True,\n",
    "                COLOR_TEXTO,\n",
    "            ),\n",
    "            (10, SCREEN_HEIGHT + 30),\n",
    "        )\n",
    "        \n",
    "        # Controles generales del sistema\n",
    "        pantalla.blit(\n",
    "            font.render(\"T=Entrenar, P=Jugar, S=Setup\", True, COLOR_TEXTO),\n",
    "            (10, SCREEN_HEIGHT + 55),\n",
    "        )\n",
    "\n",
    "        # Actualizar pantalla y controlar framerate\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(60)  # Limitar a 60 FPS para suavidad visual\n",
    "\n",
    "    # Limpiar recursos al finalizar\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa de entrenamiento.\n",
    "    \n",
    "    Ejecuta la función main() cuando el archivo se ejecuta directamente.\n",
    "    Este patrón permite importar las funciones y clases de este módulo\n",
    "    en otros scripts sin ejecutar automáticamente la interfaz de entrenamiento.\n",
    "    \n",
    "    Uso típico:\n",
    "        python interfaztrain.py  # Ejecuta la interfaz completa\n",
    "        \n",
    "    O desde otro script:\n",
    "        from interfaztrain import cargar_imagen  # Solo importa funciones\n",
    "    \"\"\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e6105",
   "metadata": {},
   "source": [
    "#### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e278b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\"\"\"\n",
    "Script de entrenamiento principal para el agente DDQN (Double Deep Q-Network).\n",
    "\n",
    "Este módulo implementa el proceso completo de entrenamiento del agente DDQN para\n",
    "el problema de recolección de frutas evitando venenos. El entrenamiento utiliza\n",
    "generación aleatoria de escenarios para garantizar la generalización del agente.\n",
    "\n",
    "Características principales del entrenamiento:\n",
    "- Generación aleatoria de entornos para cada episodio\n",
    "- Implementación completa del algoritmo DDQN\n",
    "- Actualización periódica de la red objetivo\n",
    "- Guardado automático del modelo durante el entrenamiento\n",
    "- Monitoreo del progreso con métricas de rendimiento\n",
    "\n",
    "El sistema está diseñado para entrenar un agente robusto capaz de manejar\n",
    "una amplia variedad de configuraciones de entorno, desde escenarios simples\n",
    "hasta configuraciones complejas con múltiples obstáculos y objetivos.\n",
    "\n",
    "Algoritmo implementado:\n",
    "- Double Deep Q-Network (DDQN) con replay buffer\n",
    "- Exploración epsilon-greedy con decaimiento\n",
    "- Actualización periódica de red objetivo\n",
    "- Entrenamiento continuo con experiencias almacenadas\n",
    "\"\"\"\n",
    "\n",
    "from environment import GridEnvironment\n",
    "from agent import Agent\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- CONFIGURACIÓN DE ENTRENAMIENTO ---\n",
    "\"\"\"Hiperparámetros principales del proceso de entrenamiento.\"\"\"\n",
    "EPISODES = 25000    # Número total de episodios de entrenamiento (juegos completos)\n",
    "GRID_SIZE = 5       # Tamaño de la grilla del entorno (5x5 celdas)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta el proceso completo de entrenamiento DDQN.\n",
    "    \n",
    "    Este bloque implementa el algoritmo de entrenamiento completo, incluyendo:\n",
    "    - Inicialización del entorno y agente\n",
    "    - Generación aleatoria de escenarios de entrenamiento\n",
    "    - Bucle principal de entrenamiento con DDQN\n",
    "    - Gestión de experiencias y actualización de redes\n",
    "    - Monitoreo y guardado del progreso\n",
    "    \n",
    "    El entrenamiento utiliza curriculum learning implícito a través de la\n",
    "    variabilidad aleatoria de escenarios, exponiendo al agente a una amplia\n",
    "    gama de situaciones para mejorar la generalización.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- INICIALIZACIÓN DE COMPONENTES ---\n",
    "    env = GridEnvironment(size=GRID_SIZE)              # Entorno de simulación\n",
    "    state_shape = (3, GRID_SIZE, GRID_SIZE)           # Forma del estado: 3 canales x 5x5\n",
    "    action_size = 4                                    # Número de acciones posibles (4 direcciones)\n",
    "    agent = Agent(state_shape, action_size)           # Agente DDQN con arquitectura CNN\n",
    "    \n",
    "    # --- CONFIGURACIÓN DE HIPERPARÁMETROS ---\n",
    "    batch_size = 128    # Tamaño del lote para entrenamiento de la red neural\n",
    "                       # Un batch size mayor proporciona gradientes más estables\n",
    "\n",
    "    # --- BUCLE PRINCIPAL DE ENTRENAMIENTO ---\n",
    "    for e in range(EPISODES):\n",
    "        # --- GENERACIÓN ALEATORIA DE ESCENARIOS ---\n",
    "        \"\"\"\n",
    "        Cada episodio utiliza una configuración completamente aleatoria del entorno.\n",
    "        Esta estrategia es FUNDAMENTAL para la generalización del agente, ya que\n",
    "        evita el sobreajuste a configuraciones específicas y fuerza al agente\n",
    "        a aprender estrategias robustas que funcionen en cualquier escenario.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Determinar número aleatorio de elementos en el entorno\n",
    "        num_fruits = np.random.randint(1, 5)    # Entre 1 y 4 frutas\n",
    "        num_poisons = np.random.randint(1, 4)   # Entre 1 y 3 venenos\n",
    "        \n",
    "        # Generar posiciones únicas para todos los elementos\n",
    "        # Esto previene superposiciones y garantiza configuraciones válidas\n",
    "        all_pos = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)]\n",
    "        random.shuffle(all_pos)  # Mezclar aleatoriamente todas las posiciones\n",
    "        \n",
    "        # Asignar posiciones únicas para cada elemento\n",
    "        agent_pos = all_pos.pop()                           # Posición inicial del agente\n",
    "        fruit_pos = [all_pos.pop() for _ in range(num_fruits)]   # Posiciones de frutas\n",
    "        poison_pos = [all_pos.pop() for _ in range(num_poisons)] # Posiciones de venenos\n",
    "\n",
    "        # Reiniciar entorno con la configuración generada\n",
    "        state = env.reset(agent_pos=agent_pos, fruit_pos=fruit_pos, poison_pos=poison_pos)\n",
    "        \n",
    "        # --- EJECUCIÓN DEL EPISODIO ---\n",
    "        \"\"\"\n",
    "        Cada episodio simula un juego completo donde el agente debe recolectar\n",
    "        todas las frutas mientras evita los venenos. El límite de 50 pasos\n",
    "        previene episodios infinitos y fuerza al agente a ser eficiente.\n",
    "        \"\"\"\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Bucle de pasos dentro del episodio (máximo 50 pasos)\n",
    "        for time in range(50):\n",
    "            # El agente elige una acción usando la política epsilon-greedy\n",
    "            # Durante el entrenamiento, explora aleatoriamente con probabilidad epsilon\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Ejecutar la acción en el entorno\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Almacenar la experiencia en el buffer de replay\n",
    "            # Esta experiencia se usará más tarde para entrenar la red\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Actualizar estado y acumular recompensa\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # --- ACTUALIZACIÓN DE LA RED OBJETIVO ---\n",
    "            \"\"\"\n",
    "            La red objetivo se actualiza periódicamente para estabilizar el entrenamiento.\n",
    "            Esto es una característica clave del algoritmo DQN que previene la\n",
    "            divergencia durante el entrenamiento.\n",
    "            \"\"\"\n",
    "            if agent.steps_done % agent.update_target_every == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "            # Terminar episodio si se completó el objetivo\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # --- MONITOREO DEL PROGRESO ---\n",
    "        \"\"\"\n",
    "        Imprimir estadísticas del episodio para monitorear el progreso del entrenamiento.\n",
    "        - Puntuación total: Indica qué tan bien está aprendiendo el agente\n",
    "        - Epsilon: Muestra el balance actual entre exploración y explotación\n",
    "        \"\"\"\n",
    "        print(f\"Episodio: {e+1}/{EPISODES}, Puntuación: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "        # --- ENTRENAMIENTO DE LA RED NEURAL ---\n",
    "        \"\"\"\n",
    "        El entrenamiento se realiza después de cada episodio usando experiencias\n",
    "        almacenadas en el buffer de replay. Esto permite que el agente aprenda\n",
    "        de experiencias pasadas, mejorando la eficiencia del aprendizaje.\n",
    "        \"\"\"\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "        # --- GUARDADO PERIÓDICO DEL MODELO ---\n",
    "        \"\"\"\n",
    "        Guardar el modelo cada 50 episodios para:\n",
    "        - Prevenir pérdida de progreso en caso de interrupciones\n",
    "        - Permitir evaluación de versiones intermedias\n",
    "        - Facilitar la reanudación del entrenamiento si es necesario\n",
    "        \"\"\"\n",
    "        if e % 50 == 0:\n",
    "            agent.save(\"dqn_model.pth\")\n",
    "\n",
    "    # --- FINALIZACIÓN DEL ENTRENAMIENTO ---\n",
    "    print(\"Entrenamiento finalizado. Modelo guardado en 'dqn_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9294a4",
   "metadata": {},
   "source": [
    "#### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6238cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\"\"\"\n",
    "Demostración interactiva del agente DDQN entrenado.\n",
    "\n",
    "Este módulo proporciona una interfaz de demostración simple donde los usuarios pueden:\n",
    "- Configurar entornos personalizados mediante clics del mouse\n",
    "- Observar el comportamiento del agente DDQN entrenado en tiempo real\n",
    "- Experimentar con diferentes configuraciones de frutas y venenos\n",
    "\n",
    "El sistema está diseñado como una demostración pública o para validación\n",
    "del rendimiento del agente en escenarios definidos por el usuario.\n",
    "\n",
    "Características principales:\n",
    "- Interfaz minimalista y fácil de usar\n",
    "- Carga automática del modelo entrenado\n",
    "- Visualización en tiempo real del agente\n",
    "- Reinicio automático para múltiples demostraciones\n",
    "\n",
    "Flujo de trabajo:\n",
    "1. Modo SETUP: El usuario configura frutas y venenos con clics\n",
    "2. Modo RUN: El agente ejecuta la solución automáticamente\n",
    "3. Reinicio automático al completar la demostración\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "from environment import GridEnvironment\n",
    "from agent import Agent\n",
    "\n",
    "# --- CONFIGURACIÓN DE LA INTERFAZ GRÁFICA ---\n",
    "\"\"\"Parámetros de visualización y configuración de la ventana.\"\"\"\n",
    "GRID_SIZE = 5           # Tamaño de la grilla (5x5)\n",
    "CELL_SIZE = 100         # Tamaño de cada celda en píxeles\n",
    "WIDTH, HEIGHT = GRID_SIZE * CELL_SIZE, GRID_SIZE * CELL_SIZE  # Dimensiones de ventana\n",
    "WIN = pygame.display.set_mode((WIDTH, HEIGHT))               # Ventana principal\n",
    "pygame.display.set_caption(\"Agente Come-Frutas\")            # Título de la ventana\n",
    "pygame.font.init()                                           # Inicializar sistema de fuentes\n",
    "\n",
    "# --- INICIALIZACIÓN DEL AGENTE ENTRENADO ---\n",
    "\"\"\"\n",
    "Configuración y carga del agente DDQN preentrenado.\n",
    "\n",
    "Este bloque inicializa los componentes necesarios para ejecutar\n",
    "el agente entrenado en modo demostración.\n",
    "\"\"\"\n",
    "env = GridEnvironment(size=GRID_SIZE)                    # Entorno de simulación\n",
    "action_size = 4                                          # Número de acciones posibles (4 direcciones)\n",
    "state_shape = (3, GRID_SIZE, GRID_SIZE)                 # Forma del estado: 3 canales x 5x5\n",
    "agent = Agent(state_shape, action_size)                 # Crear instancia del agente\n",
    "agent.load(\"dqn_model.pth\")                            # Cargar modelo preentrenado\n",
    "\n",
    "# --- ESQUEMA DE COLORES PARA VISUALIZACIÓN ---\n",
    "\"\"\"Colores RGB para los diferentes elementos del juego.\"\"\"\n",
    "COLOR_GRID = (200, 200, 200)    # Gris claro para las líneas de la grilla\n",
    "COLOR_AGENT = (0, 0, 255)       # Azul para el agente\n",
    "COLOR_FRUIT = (0, 255, 0)       # Verde para las frutas (objetivos)\n",
    "COLOR_POISON = (255, 0, 0)      # Rojo para los venenos (peligros)\n",
    "\n",
    "\n",
    "def draw_grid():\n",
    "    \"\"\"\n",
    "    Dibuja las líneas de la grilla en la ventana.\n",
    "    \n",
    "    Crea una cuadrícula visual que ayuda a los usuarios a identificar\n",
    "    las posiciones disponibles para colocar elementos durante el modo setup.\n",
    "    \n",
    "    La grilla se dibuja con líneas verticales y horizontales espaciadas\n",
    "    uniformemente según el tamaño de celda configurado.\n",
    "    \"\"\"\n",
    "    # Dibujar líneas verticales\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        pygame.draw.line(WIN, COLOR_GRID, (x, 0), (x, HEIGHT))\n",
    "    \n",
    "    # Dibujar líneas horizontales  \n",
    "    for y in range(0, HEIGHT, CELL_SIZE):\n",
    "        pygame.draw.line(WIN, COLOR_GRID, (0, y), (WIDTH, y))\n",
    "\n",
    "\n",
    "def draw_elements(agent_pos, fruits, poisons):\n",
    "    \"\"\"\n",
    "    Renderiza todos los elementos del juego en la pantalla.\n",
    "    \n",
    "    Esta función se encarga de dibujar visualmente todos los componentes\n",
    "    del entorno: el agente, las frutas y los venenos, usando formas\n",
    "    geométricas distintivas para cada tipo de elemento.\n",
    "    \n",
    "    Args:\n",
    "        agent_pos (np.array): Posición actual del agente [fila, columna]\n",
    "        fruits (list): Lista de posiciones de frutas [(fila, col), ...]\n",
    "        poisons (list): Lista de posiciones de venenos [(fila, col), ...]\n",
    "    \n",
    "    Representaciones visuales:\n",
    "        - Agente: Rectángulo azul que ocupa toda la celda\n",
    "        - Frutas: Círculos verdes centrados en las celdas\n",
    "        - Venenos: Cuadrados rojos más pequeños dentro de las celdas\n",
    "    \"\"\"\n",
    "    # Dibujar agente como rectángulo azul\n",
    "    # Nota: Se intercambian coordenadas (agent_pos[1], agent_pos[0]) para\n",
    "    # convertir de coordenadas de matriz (fila, columna) a pantalla (x, y)\n",
    "    pygame.draw.rect(\n",
    "        WIN,\n",
    "        COLOR_AGENT,\n",
    "        (agent_pos[1] * CELL_SIZE, agent_pos[0] * CELL_SIZE, CELL_SIZE, CELL_SIZE),\n",
    "    )\n",
    "    \n",
    "    # Dibujar frutas como círculos verdes\n",
    "    for f in fruits:\n",
    "        center_x = f[1] * CELL_SIZE + CELL_SIZE // 2  # Centro horizontal\n",
    "        center_y = f[0] * CELL_SIZE + CELL_SIZE // 2  # Centro vertical\n",
    "        radius = CELL_SIZE // 3                       # Radio del círculo\n",
    "        pygame.draw.circle(WIN, COLOR_FRUIT, (center_x, center_y), radius)\n",
    "    \n",
    "    # Dibujar venenos como cuadrados rojos más pequeños\n",
    "    for p in poisons:\n",
    "        # Crear un margen de 20 píxeles alrededor del cuadrado\n",
    "        x = p[1] * CELL_SIZE + 20\n",
    "        y = p[0] * CELL_SIZE + 20\n",
    "        size = CELL_SIZE - 40  # Tamaño reducido del cuadrado\n",
    "        pygame.draw.rect(WIN, COLOR_POISON, (x, y, size, size))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta la demostración interactiva del agente DDQN.\n",
    "    \n",
    "    Esta función implementa un sistema de dos modos que permite a los usuarios\n",
    "    configurar entornos personalizados y observar el comportamiento del agente.\n",
    "    \n",
    "    Modos de operación:\n",
    "    \n",
    "    1. **MODO SETUP** (Configuración interactiva):\n",
    "       - Permite al usuario colocar elementos usando el mouse\n",
    "       - Clic izquierdo: Añadir frutas (objetivos)\n",
    "       - Clic derecho: Añadir venenos (obstáculos peligrosos)\n",
    "       - Barra espaciadora: Iniciar simulación\n",
    "    \n",
    "    2. **MODO RUN** (Demostración del agente):\n",
    "       - El agente DDQN toma control total\n",
    "       - Ejecuta acciones basadas en el modelo entrenado\n",
    "       - Visualización en tiempo real del comportamiento\n",
    "       - Finalización automática y reinicio\n",
    "    \n",
    "    El sistema está diseñado para demostraciones públicas, permitiendo\n",
    "    múltiples usuarios configurar y probar diferentes escenarios.\n",
    "    \"\"\"\n",
    "    # Variables de estado del sistema\n",
    "    fruits = []           # Lista de posiciones de frutas configuradas por el usuario\n",
    "    poisons = []          # Lista de posiciones de venenos configuradas por el usuario\n",
    "    mode = \"setup\"        # Modo inicial: \"setup\" para configuración, \"run\" para ejecución\n",
    "\n",
    "    # Configuración del bucle principal\n",
    "    clock = pygame.time.Clock()  # Control de framerate\n",
    "    run = True                   # Flag principal del bucle\n",
    "\n",
    "    # Bucle principal de la demostración\n",
    "    while run:\n",
    "        # Limpiar pantalla con fondo negro\n",
    "        WIN.fill((0, 0, 0))\n",
    "        \n",
    "        # Dibujar grilla de referencia\n",
    "        draw_grid()\n",
    "\n",
    "        # Procesar eventos del usuario\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "\n",
    "            # --- LÓGICA DEL MODO SETUP ---\n",
    "            if mode == \"setup\":\n",
    "                # Manejo de clics del mouse para colocar elementos\n",
    "                if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                    pos = pygame.mouse.get_pos()\n",
    "                    col = pos[0] // CELL_SIZE  # Convertir coordenada x a columna\n",
    "                    row = pos[1] // CELL_SIZE  # Convertir coordenada y a fila\n",
    "\n",
    "                    # Clic izquierdo: Añadir fruta (si no existe ya)\n",
    "                    if event.button == 1 and (row, col) not in fruits:\n",
    "                        fruits.append((row, col))\n",
    "                        \n",
    "                    # Clic derecho: Añadir veneno (si no existe ya)\n",
    "                    elif event.button == 3 and (row, col) not in poisons:\n",
    "                        poisons.append((row, col))\n",
    "\n",
    "                # Tecla espaciadora: Iniciar simulación\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_SPACE:\n",
    "                        mode = \"run\"\n",
    "                        # Configurar el entorno con los elementos del usuario\n",
    "                        state = env.reset(\n",
    "                            agent_pos=(0, 0),      # Agente siempre inicia en (0,0)\n",
    "                            fruit_pos=fruits,      # Frutas configuradas por el usuario\n",
    "                            poison_pos=poisons     # Venenos configurados por el usuario\n",
    "                        )\n",
    "                        print(\"Iniciando simulación...\")\n",
    "\n",
    "        # --- RENDERIZADO SEGÚN EL MODO ACTUAL ---\n",
    "        if mode == \"setup\":\n",
    "            # Modo configuración: Mostrar elementos colocados por el usuario\n",
    "            # El agente se dibuja fuera de pantalla (posición inválida) para ocultarlo\n",
    "            draw_elements(\n",
    "                np.array([-1, -1]),  # Posición fuera de pantalla para el agente\n",
    "                fruits,              # Frutas configuradas por el usuario\n",
    "                poisons              # Venenos configurados por el usuario\n",
    "            )\n",
    "\n",
    "        elif mode == \"run\":\n",
    "            # --- LÓGICA DEL AGENTE AUTÓNOMO ---\n",
    "            # Obtener estado actual del entorno\n",
    "            state = env.get_state()\n",
    "            \n",
    "            # El agente elige la mejor acción sin exploración\n",
    "            # explore=False asegura que use solo la política aprendida\n",
    "            action = agent.choose_action(state, explore=False)\n",
    "            \n",
    "            # Ejecutar la acción en el entorno\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Renderizar estado actual con posiciones reales del entorno\n",
    "            draw_elements(\n",
    "                env.agent_pos,    # Posición actual del agente\n",
    "                env.fruit_pos,    # Frutas restantes en el entorno\n",
    "                env.poison_pos    # Venenos en el entorno\n",
    "            )\n",
    "\n",
    "            # Verificar si el episodio terminó\n",
    "            if done:\n",
    "                print(\"¡Simulación terminada!\")\n",
    "                # Reiniciar sistema para nueva demostración\n",
    "                fruits = []        # Limpiar frutas configuradas\n",
    "                poisons = []       # Limpiar venenos configurados\n",
    "                mode = \"setup\"     # Volver al modo configuración\n",
    "                pygame.time.delay(2000)  # Pausa de 2 segundos antes del reinicio\n",
    "\n",
    "            # Control de velocidad de visualización\n",
    "            pygame.time.delay(300)  # Pausa de 300ms para observar movimientos\n",
    "\n",
    "        # Actualizar pantalla para mostrar cambios\n",
    "        pygame.display.update()\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa de demostración.\n",
    "    \n",
    "    Ejecuta la función main() cuando el archivo se ejecuta directamente.\n",
    "    Este patrón permite importar funciones de este módulo en otros scripts\n",
    "    sin ejecutar automáticamente la demostración.\n",
    "    \n",
    "    Uso típico:\n",
    "        python main.py  # Ejecuta la demostración interactiva\n",
    "        \n",
    "    La demostración está diseñada para:\n",
    "    - Presentaciones públicas del proyecto\n",
    "    - Validación rápida del comportamiento del agente\n",
    "    - Experimentación interactiva con diferentes configuraciones\n",
    "    - Evaluación cualitativa del rendimiento del modelo\n",
    "    \"\"\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4021b",
   "metadata": {},
   "source": [
    "### Agente Genético"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a90e4b",
   "metadata": {},
   "source": [
    "#### agente_ga.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_ga.py\n",
    "\"\"\"\n",
    "Implementación de un agente basado en algoritmos genéticos para el entorno de recolección de frutas.\n",
    "\n",
    "Este módulo define la arquitectura de red neuronal y la clase agente utilizados en el enfoque\n",
    "de algoritmos genéticos. A diferencia del DQN que aprende mediante gradientes, este agente\n",
    "evoluciona sus pesos mediante selección natural, mutación y cruzamiento.\n",
    "\n",
    "Componentes principales:\n",
    "- AgentNetwork: Red neuronal convolucional para procesar el estado visual\n",
    "- Agent: Wrapper que contiene la red y maneja la evaluación de fitness\n",
    "\n",
    "El agente procesa el estado del entorno (representado como una imagen de 3 canales)\n",
    "y produce directamente acciones sin necesidad de aprendizaje por refuerzo.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# La arquitectura de la red puede ser la misma CNN que ya teníamos.\n",
    "# Es una buena forma de procesar la \"visión\" del agente.\n",
    "class AgentNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal convolucional para el agente genético.\n",
    "    \n",
    "    Esta red procesa la representación visual del entorno (estado como imagen de 3 canales)\n",
    "    y produce valores de acción para las 4 direcciones posibles. La arquitectura utiliza\n",
    "    capas convolucionales para extraer características espaciales, seguidas de capas\n",
    "    densas para la toma de decisiones.\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Conv2D (3→16 canales) + ReLU\n",
    "    - Conv2D (16→32 canales) + ReLU  \n",
    "    - Flatten\n",
    "    - Dense (→256) + ReLU\n",
    "    - Dense (→4 acciones)\n",
    "    \n",
    "    Args:\n",
    "        h (int): Altura de la cuadrícula de entrada (default: 5)\n",
    "        w (int): Ancho de la cuadrícula de entrada (default: 5)\n",
    "        outputs (int): Número de acciones posibles (default: 4)\n",
    "    \"\"\"\n",
    "    def __init__(self, h=5, w=5, outputs=4):\n",
    "        super(AgentNetwork, self).__init__()\n",
    "        \n",
    "        # Capas convolucionales para procesamiento espacial del estado visual\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            \"\"\"\n",
    "            Calcula el tamaño de salida después de una operación de convolución.\n",
    "            Formula: (entrada + 2*padding - kernel_size) // stride + 1\n",
    "            \"\"\"\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        # Calcular dimensiones para la capa lineal después de las convoluciones\n",
    "        # Como usamos padding=1 y kernel=3, las dimensiones se mantienen iguales\n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32  # 32 es el número de canales de salida de conv2\n",
    "        \n",
    "        # Capas densas para la toma de decisiones\n",
    "        # Capas densas para la toma de decisiones\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)  # Capa oculta con 256 neuronas\n",
    "        self.fc2 = nn.Linear(256, outputs)            # Capa de salida con 4 acciones\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante de la red neuronal.\n",
    "        \n",
    "        Procesa el estado visual del entorno a través de las capas convolucionales\n",
    "        y densas para producir valores de acción.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Estado del entorno de forma (batch_size, 3, h, w)\n",
    "                            - Canal 0: Posición del agente\n",
    "                            - Canal 1: Posiciones de frutas  \n",
    "                            - Canal 2: Posiciones de venenos\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Valores de acción de forma (batch_size, 4)\n",
    "                         Cada valor representa la \"utilidad\" de una acción:\n",
    "                         - Índice 0: Arriba\n",
    "                         - Índice 1: Abajo\n",
    "                         - Índice 2: Izquierda\n",
    "                         - Índice 3: Derecha\n",
    "        \"\"\"\n",
    "        # Primera capa convolucional + activación ReLU\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        \n",
    "        # Segunda capa convolucional + activación ReLU\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        \n",
    "        # Aplanar tensor para capas densas: (batch, channels*h*w)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Primera capa densa + activación ReLU\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        \n",
    "        # Capa de salida (sin activación, valores raw para argmax)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# El Agente es ahora solo una cáscara con su red y una puntuación de fitness.\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Wrapper del agente para algoritmos genéticos.\n",
    "    \n",
    "    Esta clase encapsula la red neuronal y proporciona la interfaz necesaria\n",
    "    para el algoritmo genético. A diferencia de los agentes de RL, este agente\n",
    "    no aprende durante la ejecución; su comportamiento está completamente\n",
    "    determinado por los pesos de la red neuronal (sus \"genes\").\n",
    "    \n",
    "    El agente se evalúa mediante su fitness (rendimiento en el entorno),\n",
    "    y los mejores agentes se seleccionan para reproducirse y crear la\n",
    "    siguiente generación mediante:\n",
    "    - Selección: Los mejores agentes tienen mayor probabilidad de reproducirse\n",
    "    - Cruzamiento: Combinación de genes de dos padres\n",
    "    - Mutación: Cambios aleatorios en los genes\n",
    "    \n",
    "    Attributes:\n",
    "        network (AgentNetwork): Red neuronal que define el comportamiento del agente\n",
    "        fitness (float): Puntuación de rendimiento en el entorno (mayor = mejor)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa un nuevo agente con red neuronal y fitness en cero.\n",
    "        \n",
    "        Los pesos de la red se inicializan aleatoriamente según la \n",
    "        inicialización por defecto de PyTorch. Estos pesos representan\n",
    "        los \"genes\" del agente que evolucionarán con el tiempo.\n",
    "        \"\"\"\n",
    "        self.network = AgentNetwork()\n",
    "        self.fitness = 0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Selecciona una acción basada en el estado actual del entorno.\n",
    "        \n",
    "        El agente utiliza su red neuronal para evaluar el estado y selecciona\n",
    "        la acción con el valor más alto (estrategia greedy). No hay exploración\n",
    "        ya que el comportamiento del agente está completamente determinado por\n",
    "        sus genes (pesos de la red).\n",
    "        \n",
    "        Este método es determinístico: dado el mismo estado y los mismos pesos,\n",
    "        siempre producirá la misma acción. Esto es importante para la evaluación\n",
    "        consistente del fitness durante la evolución.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado del entorno de forma (3, h, w)\n",
    "                             - Canal 0: Posición del agente (1.0 donde está, 0.0 resto)\n",
    "                             - Canal 1: Posiciones de frutas (1.0 donde hay frutas)\n",
    "                             - Canal 2: Posiciones de venenos (1.0 donde hay venenos)\n",
    "        \n",
    "        Returns:\n",
    "            int: Acción seleccionada:\n",
    "                 - 0: Mover arriba (decrementar fila)\n",
    "                 - 1: Mover abajo (incrementar fila)\n",
    "                 - 2: Mover izquierda (decrementar columna)\n",
    "                 - 3: Mover derecha (incrementar columna)\n",
    "        \"\"\"\n",
    "        # Convertir estado NumPy a tensor PyTorch y agregar dimensión de batch\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Evaluación sin gradientes (no hay backpropagation)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.network(state_tensor)\n",
    "        \n",
    "        # Seleccionar acción con mayor valor Q (estrategia greedy)\n",
    "        return torch.argmax(action_values).item()\n",
    "\n",
    "    def load_genes(self, filepath):\n",
    "        \"\"\"\n",
    "        Carga los \"genes\" (pesos de la red) desde un archivo.\n",
    "        \n",
    "        Utilizado para cargar agentes previamente evolucionados y demostrar\n",
    "        su comportamiento. Los pesos representan el \"ADN\" del agente que\n",
    "        determina completamente su comportamiento en el entorno.\n",
    "        \n",
    "        Este método es útil para:\n",
    "        - Cargar el mejor agente de una evolución anterior\n",
    "        - Demostrar el comportamiento de agentes elite\n",
    "        - Continuar la evolución desde una generación guardada\n",
    "        - Análisis y visualización del comportamiento aprendido\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Ruta al archivo con los pesos del modelo\n",
    "                           (normalmente un archivo .pth de PyTorch)\n",
    "                           \n",
    "        Raises:\n",
    "            FileNotFoundError: Si el archivo no existe\n",
    "            RuntimeError: Si los pesos no coinciden con la arquitectura\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.network.load_state_dict(torch.load(filepath))\n",
    "            print(f\"✅ Genes cargados exitosamente desde: {filepath}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ Error: No se encontró el archivo {filepath}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error cargando genes: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277ac25",
   "metadata": {},
   "source": [
    "#### genetico_agente.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demostrador interactivo para agentes entrenados con algoritmos genéticos.\n",
    "\n",
    "Este módulo implementa una interfaz gráfica completa que permite:\n",
    "1. Configurar escenarios personalizados con frutas, venenos y paredes\n",
    "2. Observar el comportamiento de un agente genético entrenado\n",
    "3. Interactuar en tiempo real con controles de teclado\n",
    "4. Visualizar el rendimiento del agente en diferentes configuraciones\n",
    "\n",
    "Características principales:\n",
    "- Modo Setup: Configuración manual del entorno\n",
    "- Modo Playing: Demostración del agente en acción\n",
    "- Controles intuitivos con teclado\n",
    "- Gráficos mejorados con sprites\n",
    "- Interfaz informativa con instrucciones\n",
    "\n",
    "El agente carga pesos previamente evolucionados y demuestra su comportamiento\n",
    "determinístico en los escenarios configurados por el usuario.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from agent_ga import Agent\n",
    "\n",
    "# CONFIGURACIÓN VISUAL Y DIMENSIONES\n",
    "\"\"\"\n",
    "Constantes que definen la apariencia y dimensiones de la interfaz gráfica.\n",
    "\"\"\"\n",
    "GRID_WIDTH = 5          # Ancho de la cuadrícula en celdas\n",
    "GRID_HEIGHT = 5         # Alto de la cuadrícula en celdas  \n",
    "CELL_SIZE = 120         # Tamaño de cada celda en píxeles (más grande que en DQN)\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la ventana (600px)\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto total del área de juego (600px)\n",
    "\n",
    "# PALETA DE COLORES PROFESIONAL\n",
    "\"\"\"\n",
    "Esquema de colores dark theme para una interfaz moderna y profesional.\n",
    "\"\"\"\n",
    "COLOR_FONDO = (25, 25, 25)      # Gris muy oscuro para el fondo\n",
    "COLOR_LINEAS = (40, 40, 40)     # Gris oscuro para líneas de cuadrícula\n",
    "COLOR_CURSOR = (255, 255, 0)    # Amarillo brillante para el cursor de selección\n",
    "COLOR_TEXTO = (230, 230, 230)   # Gris claro para texto legible\n",
    "\n",
    "\n",
    "class EntornoGrid:\n",
    "    \"\"\"\n",
    "    Entorno de cuadrícula personalizado para la demostración del agente genético.\n",
    "    \n",
    "    Esta clase maneja la lógica del juego y la configuración del entorno,\n",
    "    incluyendo la colocación de elementos y la simulación de la interacción\n",
    "    del agente. Incluye características adicionales como paredes que no\n",
    "    están presentes en los entornos de entrenamiento básicos.\n",
    "    \n",
    "    Características especiales:\n",
    "    - Soporte para paredes como obstáculos\n",
    "    - Interfaz de configuración manual\n",
    "    - Reset automático en condiciones de terminación\n",
    "    - Estado visual compatible con el agente entrenado\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tamaño de la cuadrícula\n",
    "        agent_pos (tuple): Posición actual del agente (fila, columna)\n",
    "        frutas (set): Conjunto de posiciones con frutas\n",
    "        venenos (set): Conjunto de posiciones con venenos  \n",
    "        paredes (set): Conjunto de posiciones con paredes (obstáculos)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuración vacía.\n",
    "        \n",
    "        Todos los conjuntos de elementos comienzan vacíos, permitiendo\n",
    "        al usuario configurar el escenario manualmente.\n",
    "        \"\"\"\n",
    "        self.size = GRID_WIDTH\n",
    "        self.agent_pos = (0, 0)    # Agente siempre inicia en esquina superior izquierda\n",
    "        self.frutas = set()        # Conjunto de posiciones de frutas\n",
    "        self.venenos = set()       # Conjunto de posiciones de venenos\n",
    "        self.paredes = set()       # Conjunto de posiciones de paredes\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"\n",
    "        Resetea el agente a la posición inicial sin modificar el entorno.\n",
    "        \n",
    "        Utilizado al inicio de cada demostración para colocar al agente\n",
    "        en la posición de partida estándar (0,0) manteniendo la configuración\n",
    "        de frutas, venenos y paredes establecida por el usuario.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno después del reset\n",
    "        \"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno (frutas, venenos, paredes).\n",
    "        \n",
    "        Función de utilidad para resetear completamente el escenario,\n",
    "        permitiendo al usuario comenzar con una cuadrícula vacía.\n",
    "        El agente permanece en su posición actual.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción del agente en el entorno de demostración.\n",
    "        \n",
    "        Implementa la lógica del juego incluyendo movimiento, colisiones con\n",
    "        paredes, interacción con elementos del entorno y cálculo de recompensas.\n",
    "        Incluye características especiales como paredes que bloquean el movimiento.\n",
    "        \n",
    "        Diferencias con el entorno de entrenamiento:\n",
    "        - Incluye paredes como obstáculos\n",
    "        - Movimientos inválidos dan recompensa negativa\n",
    "        - Reset automático al completar nivel\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Acción a ejecutar:\n",
    "                         0 = Arriba (decrementar fila)\n",
    "                         1 = Abajo (incrementar fila) \n",
    "                         2 = Izquierda (decrementar columna)\n",
    "                         3 = Derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (estado, recompensa, terminado)\n",
    "                - estado (np.array): Nuevo estado del entorno\n",
    "                - recompensa (float): Recompensa obtenida\n",
    "                - terminado (bool): Si el episodio ha terminado\n",
    "        \"\"\"\n",
    "        # Calcular nueva posición basada en la acción\n",
    "        fila, col = self.agent_pos\n",
    "        if accion == 0:     # Arriba\n",
    "            fila -= 1\n",
    "        elif accion == 1:   # Abajo\n",
    "            fila += 1\n",
    "        elif accion == 2:   # Izquierda\n",
    "            col -= 1\n",
    "        elif accion == 3:   # Derecha\n",
    "            col += 1\n",
    "\n",
    "        # Verificar colisiones: límites del tablero o paredes\n",
    "        if (\n",
    "            fila < 0\n",
    "            or fila >= GRID_HEIGHT\n",
    "            or col < 0\n",
    "            or col >= GRID_WIDTH\n",
    "            or (fila, col) in self.paredes\n",
    "        ):\n",
    "            # Movimiento inválido: pequeña penalización, posición no cambia\n",
    "            return self.get_state(), -0.1, False\n",
    "\n",
    "        # Movimiento válido: actualizar posición\n",
    "        self.agent_pos = (fila, col)\n",
    "        recompensa = -0.05    # Costo base del movimiento\n",
    "        terminado = False\n",
    "\n",
    "        # Verificar interacciones con elementos del entorno\n",
    "        if self.agent_pos in self.venenos:\n",
    "            # Veneno tocado: penalización severa y reset a inicio\n",
    "            recompensa = -10.0\n",
    "            self.agent_pos = (0, 0)\n",
    "        elif self.agent_pos in self.frutas:\n",
    "            # Fruta recogida: recompensa positiva\n",
    "            recompensa = 1.0\n",
    "            self.frutas.remove(self.agent_pos)\n",
    "            \n",
    "            # Verificar si se completó el nivel\n",
    "            if not self.frutas:\n",
    "                recompensa += 10.0    # Bonus por completar\n",
    "                terminado = True\n",
    "                self.agent_pos = (0, 0)  # Reset para próxima demostración\n",
    "\n",
    "        return self.get_state(), recompensa, terminado\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera la representación del estado compatible con el agente entrenado.\n",
    "        \n",
    "        Crea una representación de 3 canales idéntica a la utilizada durante\n",
    "        el entrenamiento, asegurando compatibilidad con los pesos evolucionados.\n",
    "        Las paredes no se incluyen en el estado ya que el agente original\n",
    "        no fue entrenado con ellas.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado del entorno de forma (3, size, size):\n",
    "                     - Canal 0: Posición del agente\n",
    "                     - Canal 1: Posiciones de frutas\n",
    "                     - Canal 2: Posiciones de venenos\n",
    "        \"\"\"\n",
    "        estado = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posición del agente\n",
    "        estado[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruta in self.frutas:\n",
    "            estado[1, fruta[0], fruta[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de venenos  \n",
    "        for veneno in self.venenos:\n",
    "            estado[2, veneno[0], veneno[1]] = 1.0\n",
    "            \n",
    "        return estado\n",
    "\n",
    "    def dibujar(\n",
    "        self,\n",
    "        pantalla,\n",
    "        modo_juego,\n",
    "        cursor_pos,\n",
    "        img_fruta,\n",
    "        img_veneno,\n",
    "        img_pared,\n",
    "        img_agente,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Renderiza el estado completo del entorno en la pantalla.\n",
    "        \n",
    "        Dibuja todos los elementos visuales incluyendo cuadrícula, sprites,\n",
    "        cursor de selección (en modo setup) e interfaz de usuario con\n",
    "        controles e información del modo actual.\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde dibujar\n",
    "            modo_juego (str): Modo actual (\"SETUP\" o \"PLAYING\")\n",
    "            cursor_pos (tuple): Posición del cursor en modo setup\n",
    "            img_fruta (pygame.Surface): Sprite de la fruta\n",
    "            img_veneno (pygame.Surface): Sprite del veneno\n",
    "            img_pared (pygame.Surface): Sprite de la pared\n",
    "            img_agente (pygame.Surface): Sprite del agente\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar cuadrícula de referencia\n",
    "        # Líneas verticales\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        # Líneas horizontales\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibujar elementos del entorno (orden importante para superposición correcta)\n",
    "        # 1. Paredes (fondo)\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "        \n",
    "        # 2. Frutas\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "        \n",
    "        # 3. Venenos\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "\n",
    "        # 4. Agente (primer plano)\n",
    "        pantalla.blit(\n",
    "            img_agente, (self.agent_pos[0] * CELL_SIZE, self.agent_pos[1] * CELL_SIZE)\n",
    "        )\n",
    "\n",
    "        # 5. Cursor de selección (solo en modo setup)\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Dibujar interfaz de usuario en la parte inferior\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        \n",
    "        # Información del modo actual\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Controles disponibles\n",
    "        controles1 = font.render(\n",
    "            \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\", True, COLOR_TEXTO\n",
    "        )\n",
    "        controles2 = font.render(\"P=Jugar, S=Setup\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Posicionar texto en la parte inferior\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles1, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles2, (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta la aplicación de demostración.\n",
    "    \n",
    "    Inicializa Pygame, carga recursos gráficos, configura el agente genético\n",
    "    entrenado y ejecuta el bucle principal de la aplicación. Maneja dos modos\n",
    "    principales: configuración manual y demostración automática.\n",
    "    \n",
    "    Flujo de ejecución:\n",
    "    1. Inicialización de Pygame y recursos\n",
    "    2. Carga del agente entrenado\n",
    "    3. Bucle principal con manejo de eventos\n",
    "    4. Renderizado continuo\n",
    "    5. Limpieza al salir\n",
    "    \"\"\"\n",
    "    # INICIALIZACIÓN DE PYGAME\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente Genético - Come Frutas 🍓\")\n",
    "\n",
    "    # FUNCIÓN AUXILIAR PARA CARGA DE IMÁGENES\n",
    "    def cargar_img(nombre, color_fallback):\n",
    "        \"\"\"\n",
    "        Carga una imagen con fallback a color sólido si falla.\n",
    "        \n",
    "        Args:\n",
    "            nombre (str): Nombre del archivo de imagen\n",
    "            color_fallback (tuple): Color RGB de respaldo\n",
    "            \n",
    "        Returns:\n",
    "            pygame.Surface: Superficie escalada al tamaño de celda\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "            img = pygame.image.load(ruta).convert_alpha()\n",
    "            return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "        except:\n",
    "            # Fallback: crear superficie de color sólido\n",
    "            surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "            surf.fill(color_fallback)\n",
    "            return surf\n",
    "\n",
    "    # CARGA DE RECURSOS GRÁFICOS\n",
    "    img_fruta = cargar_img(\"../fruta.png\", (0, 255, 0))      # Verde si no hay imagen\n",
    "    img_veneno = cargar_img(\"../veneno.png\", (255, 0, 0))    # Rojo si no hay imagen  \n",
    "    img_pared = cargar_img(\"../pared.png\", (100, 100, 100)) # Gris si no hay imagen\n",
    "    img_agente = cargar_img(\"../agente.png\", (0, 0, 255))   # Azul si no hay imagen\n",
    "\n",
    "    # INICIALIZACIÓN DEL ENTORNO Y AGENTE\n",
    "    entorno = EntornoGrid()\n",
    "    agente = Agent()\n",
    "    \n",
    "    # Cargar agente entrenado con algoritmos genéticos\n",
    "    agente.load_genes(\"GENETICO/best_agent_genes.pth\")\n",
    "\n",
    "    # VARIABLES DE ESTADO DE LA APLICACIÓN\n",
    "    cursor_pos = [0, 0]        # Posición del cursor en modo setup\n",
    "    modo_juego = \"SETUP\"       # Modo inicial: configuración\n",
    "    reloj = pygame.time.Clock() # Control de FPS\n",
    "    corriendo = True           # Flag de control del bucle principal\n",
    "\n",
    "    # BUCLE PRINCIPAL DE LA APLICACIÓN\n",
    "    while corriendo:\n",
    "        # MANEJO DE EVENTOS\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            # EVENTOS DE TECLADO\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # CONTROLES GLOBALES (disponibles en ambos modos)\n",
    "                if evento.key == pygame.K_p:\n",
    "                    print(\"--- INICIANDO MODO JUEGO ---\")\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    time.sleep(0.5)  # Pausa para transición visual\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- INICIANDO MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # CONTROLES ESPECÍFICOS DEL MODO SETUP\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Navegación con flechas del cursor\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Colocación/eliminación de elementos\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    \n",
    "                    # F = Toggle Fruta\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        if pos in entorno.frutas:\n",
    "                            entorno.frutas.remove(pos)\n",
    "                            print(f\"Fruta eliminada en {pos}\")\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos)\n",
    "                            entorno.venenos.discard(pos)    # Remover otros elementos\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            print(f\"Fruta colocada en {pos}\")\n",
    "                    \n",
    "                    # V = Toggle Veneno\n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        if pos in entorno.venenos:\n",
    "                            entorno.venenos.remove(pos)\n",
    "                            print(f\"Veneno eliminado en {pos}\")\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos)\n",
    "                            entorno.frutas.discard(pos)     # Remover otros elementos\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            print(f\"Veneno colocado en {pos}\")\n",
    "                    \n",
    "                    # W = Toggle Pared\n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        if pos in entorno.paredes:\n",
    "                            entorno.paredes.remove(pos)\n",
    "                            print(f\"Pared eliminada en {pos}\")\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos)\n",
    "                            entorno.frutas.discard(pos)     # Remover otros elementos\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            print(f\"Pared colocada en {pos}\")\n",
    "                    \n",
    "                    # C = Limpiar todo\n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        print(\"--- LIMPIANDO ENTORNO COMPLETO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "\n",
    "        # LÓGICA DEL MODO PLAYING (DEMOSTRACIÓN DEL AGENTE)\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # El agente toma decisiones automáticamente\n",
    "            estado = entorno.get_state()\n",
    "            accion = agente.choose_action(estado)\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            \n",
    "            # Verificar si el episodio terminó\n",
    "            if terminado:\n",
    "                print(\"🏆 ¡Agente completó el nivel! Volviendo a modo SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "            \n",
    "            # Pausa para visualización clara del movimiento\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # RENDERIZADO (COMÚN PARA AMBOS MODOS)\n",
    "        # Crear superficie temporal para el contenido completo\n",
    "        pantalla_con_info = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "        pantalla_con_info.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar entorno y elementos\n",
    "        entorno.dibujar(\n",
    "            pantalla_con_info,\n",
    "            modo_juego,\n",
    "            tuple(cursor_pos),\n",
    "            img_fruta,\n",
    "            img_veneno,\n",
    "            img_pared,\n",
    "            img_agente,\n",
    "        )\n",
    "        \n",
    "        # Copiar a pantalla principal y actualizar\n",
    "        pantalla.blit(pantalla_con_info, (0, 0))\n",
    "        pygame.display.flip()\n",
    "        \n",
    "        # Controlar FPS\n",
    "        reloj.tick(60)\n",
    "\n",
    "    # LIMPIEZA AL SALIR\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa.\n",
    "    \n",
    "    Ejecuta la función main() solo si este archivo se ejecuta directamente.\n",
    "    Incluye mensaje de bienvenida con instrucciones básicas.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🧬 DEMOSTRADOR DE AGENTE GENÉTICO 🧬\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CONTROLES:\")\n",
    "    print(\"🎮 GLOBALES:\")\n",
    "    print(\"  P - Iniciar modo Playing (demostración)\")\n",
    "    print(\"  S - Cambiar a modo Setup (configuración)\")\n",
    "    print()\n",
    "    print(\"⚙️  MODO SETUP:\")\n",
    "    print(\"  ⬆️⬇️⬅️➡️ - Mover cursor\")\n",
    "    print(\"  F - Toggle Fruta\")\n",
    "    print(\"  V - Toggle Veneno\") \n",
    "    print(\"  W - Toggle Pared\")\n",
    "    print(\"  C - Limpiar entorno\")\n",
    "    print()\n",
    "    print(\"🤖 MODO PLAYING:\")\n",
    "    print(\"  El agente toma control automáticamente\")\n",
    "    print(\"  Observa el comportamiento evolucionado\")\n",
    "    print()\n",
    "    print(\"¡Configura un escenario y observa la inteligencia artificial!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7db34f",
   "metadata": {},
   "source": [
    "#### enviroment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entorno de cuadrícula especializado para algoritmos genéticos.\n",
    "\n",
    "Este módulo implementa un entorno modificado para el entrenamiento de agentes mediante\n",
    "algoritmos genéticos. La principal diferencia con el entorno DQN es el manejo de venenos:\n",
    "en lugar de terminar el episodio, el agente es enviado de vuelta a la posición inicial\n",
    "con una penalización, permitiendo episodios más largos y mejor evaluación de fitness.\n",
    "\n",
    "Características específicas para GA:\n",
    "- Venenos no terminan el episodio, sino que resetean la posición\n",
    "- Episodios más largos para mejor evaluación de fitness\n",
    "- Recompensas ajustadas para discriminar mejor entre agentes\n",
    "- Seguimiento de posición inicial para reset de venenos\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GridEnvironment:\n",
    "    \"\"\"\n",
    "    Entorno de cuadrícula optimizado para algoritmos genéticos.\n",
    "    \n",
    "    Este entorno está diseñado específicamente para la evaluación de agentes\n",
    "    mediante algoritmos genéticos. La principal modificación es que tocar venenos\n",
    "    no termina el episodio, sino que envía al agente de vuelta al inicio,\n",
    "    permitiendo episodios más largos y una mejor discriminación entre agentes.\n",
    "    \n",
    "    Características para GA:\n",
    "    - Episodios más largos para mejor evaluación de fitness\n",
    "    - Venenos causan reset de posición en lugar de game over\n",
    "    - Recompensas ajustadas para mejor selección evolutiva\n",
    "    - Seguimiento de posición inicial para mecánica de reset\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tamaño de la cuadrícula (size x size)\n",
    "        start_pos (np.array): Posición inicial del agente en el episodio\n",
    "        agent_pos (np.array): Posición actual del agente\n",
    "        fruit_pos (list): Lista de posiciones de frutas\n",
    "        poison_pos (list): Lista de posiciones de venenos\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de cuadrícula para algoritmos genéticos.\n",
    "        \n",
    "        Args:\n",
    "            size (int, optional): Tamaño de la cuadrícula. Por defecto es 5x5.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.start_pos = (0, 0)  # Guardar posición inicial para reset de venenos\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, agent_pos=(0, 0), fruit_pos=[], poison_pos=[]):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno con una configuración específica.\n",
    "        \n",
    "        Establece las posiciones iniciales y guarda la posición de inicio del agente\n",
    "        para la mecánica de reset por venenos. Esta posición inicial es crucial\n",
    "        en el paradigma de algoritmos genéticos ya que permite que el agente\n",
    "        continúe intentando después de errores.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (tuple, optional): Posición inicial del agente (fila, columna). \n",
    "                                       Por defecto (0, 0).\n",
    "            fruit_pos (list, optional): Lista de tuplas con posiciones de frutas.\n",
    "                                       Por defecto lista vacía.\n",
    "            poison_pos (list, optional): Lista de tuplas con posiciones de venenos.\n",
    "                                        Por defecto lista vacía.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno como array 3D (3, size, size).\n",
    "        \"\"\"\n",
    "        self.start_pos = np.array(agent_pos)  # Guardar posición inicial del episodio\n",
    "        self.agent_pos = np.array(agent_pos)\n",
    "        self.fruit_pos = [np.array(p) for p in fruit_pos]\n",
    "        self.poison_pos = [np.array(p) for p in poison_pos]\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera la representación del estado actual del entorno.\n",
    "        \n",
    "        Idéntica implementación al entorno DQN. El estado se representa como una \n",
    "        \"imagen\" de 3 canales que puede ser procesada por redes convolucionales.\n",
    "        \n",
    "        - Canal 0: Posición del agente (1.0 donde está el agente, 0.0 en el resto)\n",
    "        - Canal 1: Posiciones de frutas (1.0 donde hay frutas, 0.0 en el resto)  \n",
    "        - Canal 2: Posiciones de venenos (1.0 donde hay venenos, 0.0 en el resto)\n",
    "        \n",
    "        Esta representación permite que el agente \"vea\" todo el entorno de una vez\n",
    "        y es compatible con arquitecturas de redes neuronales convolucionales.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado del entorno como array 3D de forma (3, size, size)\n",
    "                     con valores float32.\n",
    "        \"\"\"\n",
    "        state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posición del agente\n",
    "        # Canal 0: Posición del agente\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de las frutas\n",
    "        for fruit in self.fruit_pos:\n",
    "            state[1, fruit[0], fruit[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de los venenos\n",
    "        for poison in self.poison_pos:\n",
    "            state[2, poison[0], poison[1]] = 1.0\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción en el entorno optimizado para algoritmos genéticos.\n",
    "        \n",
    "        Esta función implementa la lógica principal del juego con modificaciones\n",
    "        específicas para algoritmos genéticos. La diferencia clave es el manejo\n",
    "        de venenos: en lugar de terminar el episodio, el agente se resetea a la\n",
    "        posición inicial, permitiendo episodios más largos y mejor evaluación.\n",
    "        \n",
    "        Diferencias con DQN:\n",
    "        - Venenos NO terminan el episodio\n",
    "        - Venenos resetean la posición del agente al inicio\n",
    "        - Penalización mayor por venenos (-10.0 vs -1.0)\n",
    "        - Episodios más largos para mejor discriminación de fitness\n",
    "        \n",
    "        Args:\n",
    "            action (int): Acción a realizar:\n",
    "                         0 = Arriba (decrementar fila)\n",
    "                         1 = Abajo (incrementar fila)  \n",
    "                         2 = Izquierda (decrementar columna)\n",
    "                         3 = Derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, terminado)\n",
    "                - nuevo_estado (np.array): Estado del entorno después de la acción\n",
    "                - recompensa (float): Recompensa obtenida por la acción\n",
    "                - terminado (bool): True solo si todas las frutas fueron recogidas\n",
    "        \"\"\"\n",
    "        \n",
    "        # FASE 1: MOVIMIENTO DEL AGENTE\n",
    "        # Lógica idéntica al entorno DQN\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] -= 1    # Arriba\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] += 1    # Abajo\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] -= 1    # Izquierda\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] += 1    # Derecha\n",
    "            \n",
    "        # Limitar posición a los límites del tablero\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.size - 1)\n",
    "\n",
    "        # FASE 2: INICIALIZACIÓN DE RECOMPENSAS\n",
    "        reward = -0.05  # Pequeño castigo por cada movimiento\n",
    "        done = False\n",
    "\n",
    "        # FASE 2: INICIALIZACIÓN DE RECOMPENSAS\n",
    "        reward = -0.05  # Pequeño castigo por cada movimiento\n",
    "        done = False\n",
    "\n",
    "        # FASE 3: MANEJO ESPECIAL DE VENENOS (DIFERENCIA CLAVE CON DQN)\n",
    "        if any(np.array_equal(self.agent_pos, p) for p in self.poison_pos):\n",
    "            # Veneno tocado: penalización severa pero NO termina el episodio\n",
    "            reward = -10.0\n",
    "            # CARACTERÍSTICA PRINCIPAL: Reset a posición inicial\n",
    "            self.agent_pos = np.copy(self.start_pos)\n",
    "            # CRÍTICO: done permanece False, el episodio continúa\n",
    "            print(\"🔄 Agente tocó veneno, reseteado a posición inicial\")\n",
    "        else:\n",
    "            # FASE 4: LÓGICA NORMAL (SOLO SI NO HAY VENENO)\n",
    "            # Verificar si se recogió una fruta\n",
    "            eaten_fruit_this_step = False\n",
    "            for i, fruit in enumerate(self.fruit_pos):\n",
    "                if np.array_equal(self.agent_pos, fruit):\n",
    "                    reward += 1.0  # Recompensa por fruta\n",
    "                    self.fruit_pos.pop(i)  # Remover fruta del entorno\n",
    "                    eaten_fruit_this_step = True\n",
    "                    print(\"🍎 Fruta recogida!\")\n",
    "                    break  # Solo una fruta por paso\n",
    "\n",
    "            # Reward shaping opcional (sin implementar aquí)\n",
    "            if not eaten_fruit_this_step and self.fruit_pos:\n",
    "                # Aquí se podría agregar lógica de distancia como en DQN\n",
    "                # Dejado como comentario para mantener simplicidad\n",
    "                pass\n",
    "\n",
    "            # FASE 5: CONDICIÓN DE VICTORIA\n",
    "            if not self.fruit_pos:\n",
    "                done = True\n",
    "                reward += 10.0  # Gran recompensa por completar el nivel\n",
    "                print(\"🏆 ¡Todas las frutas recogidas! Episodio completado\")\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d01628",
   "metadata": {},
   "source": [
    "#### train_ga.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefa629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ga.py\n",
    "\"\"\"\n",
    "Implementación completa de algoritmo genético para entrenar agentes de IA.\n",
    "\n",
    "Este módulo implementa un algoritmo genético completo que evoluciona poblaciones\n",
    "de agentes para resolver el problema de recolección de frutas. El algoritmo\n",
    "simula la evolución natural mediante selección, cruzamiento y mutación.\n",
    "\n",
    "Proceso evolutivo:\n",
    "1. Inicialización: Crear población aleatoria de agentes\n",
    "2. Evaluación: Probar cada agente en escenarios aleatorios  \n",
    "3. Selección: Elegir los mejores agentes como padres\n",
    "4. Cruzamiento: Combinar genes de padres para crear hijos\n",
    "5. Mutación: Introducir variación aleatoria en los genes\n",
    "6. Reemplazo: Formar nueva generación con elitismo\n",
    "7. Repetir hasta convergencia\n",
    "\n",
    "Características del algoritmo:\n",
    "- Evaluación en escenarios aleatorios para robustez\n",
    "- Elitismo para preservar mejores soluciones\n",
    "- Mutación gaussiana para exploración controlada\n",
    "- Cruzamiento uniforme para recombinación equilibrada\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from environment import GridEnvironment\n",
    "from agent_ga import Agent, AgentNetwork\n",
    "import random\n",
    "\n",
    "# HIPERPARÁMETROS DEL ALGORITMO GENÉTICO\n",
    "\"\"\"\n",
    "Configuración de parámetros evolutivos que controlan el comportamiento\n",
    "del algoritmo genético. Estos valores han sido ajustados empíricamente\n",
    "para balancear exploración vs. explotación.\n",
    "\"\"\"\n",
    "POPULATION_SIZE = 100    # Tamaño de la población por generación\n",
    "NUM_GENERATIONS = 500    # Número total de generaciones a evolucionar\n",
    "MUTATION_RATE = 0.05     # Probabilidad de mutación por gen (5%)\n",
    "ELITISM_COUNT = 25       # Mejores agentes que pasan directamente (25%)\n",
    "\n",
    "GRID_SIZE = 5           # Tamaño del entorno de evaluación\n",
    "\n",
    "def create_initial_population():\n",
    "    \"\"\"\n",
    "    Crea la población inicial de agentes con genes aleatorios.\n",
    "    \n",
    "    Genera una población de agentes donde cada uno tiene pesos de red neuronal\n",
    "    inicializados aleatoriamente según la inicialización por defecto de PyTorch.\n",
    "    Esta diversidad inicial es crucial para el éxito del algoritmo genético.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de POPULATION_SIZE agentes con genes aleatorios\n",
    "        \n",
    "    Note:\n",
    "        La diversidad genética inicial determina el espacio de búsqueda\n",
    "        que el algoritmo puede explorar durante la evolución.\n",
    "    \"\"\"\n",
    "    return [Agent() for _ in range(POPULATION_SIZE)]\n",
    "\n",
    "def evaluate_fitness(population, env):\n",
    "    \"\"\"\n",
    "    Evalúa el fitness de cada agente en la población.\n",
    "    \n",
    "    Cada agente se prueba en un escenario aleatorio generado dinámicamente.\n",
    "    La variabilidad en los escenarios asegura que los agentes desarrollen\n",
    "    estrategias robustas y generalizables en lugar de sobreajustarse a\n",
    "    configuraciones específicas.\n",
    "    \n",
    "    Proceso de evaluación:\n",
    "    1. Generar escenario aleatorio (posiciones, frutas, venenos)\n",
    "    2. Ejecutar agente por máximo 50 pasos\n",
    "    3. Acumular recompensas totales como fitness\n",
    "    4. Repetir para todos los agentes de la población\n",
    "    \n",
    "    Args:\n",
    "        population (list): Lista de agentes a evaluar\n",
    "        env (GridEnvironment): Entorno de evaluación\n",
    "        \n",
    "    Note:\n",
    "        El fitness se calcula como la suma total de recompensas obtenidas\n",
    "        durante el episodio, incluyendo penalizaciones por movimientos,\n",
    "        recompensas por frutas y penalizaciones por venenos.\n",
    "    \"\"\"\n",
    "    for agent in population:\n",
    "        # GENERACIÓN DE ESCENARIO ALEATORIO\n",
    "        # Número variable de frutas (1-4) para diversidad de dificultad\n",
    "        num_fruits = np.random.randint(1, 5)\n",
    "        \n",
    "        # Crear lista de todas las posiciones posibles\n",
    "        all_pos = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)]\n",
    "        random.shuffle(all_pos)  # Barajar para selección aleatoria\n",
    "        \n",
    "        # Asignar posiciones sin solapamiento\n",
    "        agent_pos = all_pos.pop()   # Posición inicial del agente\n",
    "        fruit_pos = [all_pos.pop() for _ in range(num_fruits)]  # Posiciones de frutas\n",
    "        poison_pos = [all_pos.pop() for _ in range(np.random.randint(1, 4))]  # 1-3 venenos\n",
    "\n",
    "        # EJECUCIÓN DEL EPISODIO\n",
    "        state = env.reset(agent_pos=agent_pos, fruit_pos=fruit_pos, poison_pos=poison_pos)\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Máximo 50 pasos para evitar episodios infinitos\n",
    "        for _ in range(50):\n",
    "            action = agent.choose_action(state)\n",
    "            state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Terminar si el agente completa el objetivo\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Asignar fitness como recompensa total acumulada\n",
    "        agent.fitness = total_reward\n",
    "\n",
    "def selection(population):\n",
    "    \"\"\"\n",
    "    Selecciona los mejores agentes de la población para reproducción.\n",
    "    \n",
    "    Implementa selección elitista donde solo los agentes con mayor fitness\n",
    "    se seleccionan como padres para la siguiente generación. Este método\n",
    "    asegura que las características exitosas se preserven y propaguen.\n",
    "    \n",
    "    Estrategia de selección:\n",
    "    - Ordenar población por fitness (mayor a menor)\n",
    "    - Seleccionar el 20% superior como padres\n",
    "    - Estos padres participarán en cruzamiento y algunos en elitismo\n",
    "    \n",
    "    Args:\n",
    "        population (list): Población de agentes evaluados\n",
    "        \n",
    "    Returns:\n",
    "        list: Los mejores agentes seleccionados para reproducción\n",
    "        \n",
    "    Note:\n",
    "        Un porcentaje del 20% permite suficiente diversidad genética\n",
    "        mientras mantiene presión selectiva hacia mejores soluciones.\n",
    "    \"\"\"\n",
    "    # Ordenar por fitness descendente (mejores primero)\n",
    "    population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "    \n",
    "    # Seleccionar el 20% superior de la población\n",
    "    return population[:int(POPULATION_SIZE * 0.2)]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    \"\"\"\n",
    "    Crea un nuevo agente combinando genes de dos padres.\n",
    "    \n",
    "    Implementa cruzamiento uniforme donde cada parámetro (gen) del hijo\n",
    "    se hereda aleatoriamente de uno de los dos padres. Este método mantiene\n",
    "    bloques funcionales de la red mientras permite recombinación genética.\n",
    "    \n",
    "    Proceso de cruzamiento:\n",
    "    1. Crear nuevo agente hijo\n",
    "    2. Para cada parámetro de la red neuronal:\n",
    "       - Probabilidad 50%: heredar del padre 1\n",
    "       - Probabilidad 50%: heredar del padre 2\n",
    "    3. Cargar genes combinados en el hijo\n",
    "    \n",
    "    Args:\n",
    "        parent1 (Agent): Primer padre seleccionado\n",
    "        parent2 (Agent): Segundo padre seleccionado\n",
    "        \n",
    "    Returns:\n",
    "        Agent: Nuevo agente hijo con genes combinados\n",
    "        \n",
    "    Note:\n",
    "        El cruzamiento uniforme preserva mejor las estructuras funcionales\n",
    "        de las redes neuronales comparado con cruzamiento de un punto.\n",
    "    \"\"\"\n",
    "    # Crear nuevo agente hijo\n",
    "    child = Agent()\n",
    "    \n",
    "    # Obtener diccionarios de parámetros (genes) de los padres\n",
    "    p1_genes = parent1.network.state_dict()\n",
    "    p2_genes = parent2.network.state_dict()\n",
    "    child_genes = child.network.state_dict()\n",
    "\n",
    "    # Cruzamiento uniforme: cada gen se hereda aleatoriamente\n",
    "    for key in child_genes.keys():\n",
    "        # 50% probabilidad de heredar cada gen de cada padre\n",
    "        if random.random() < 0.5:\n",
    "            child_genes[key] = p1_genes[key].clone()  # Heredar del padre 1\n",
    "        else:\n",
    "            child_genes[key] = p2_genes[key].clone()  # Heredar del padre 2\n",
    "    \n",
    "    # Cargar genes combinados en la red del hijo\n",
    "    child.network.load_state_dict(child_genes)\n",
    "    return child\n",
    "\n",
    "def mutate(agent):\n",
    "    \"\"\"\n",
    "    Introduce variación genética aleatoria en un agente.\n",
    "    \n",
    "    Implementa mutación gaussiana donde cada parámetro tiene una probabilidad\n",
    "    MUTATION_RATE de ser alterado con ruido gaussiano. Esta mutación permite\n",
    "    explorar nuevas regiones del espacio de búsqueda y evitar convergencia\n",
    "    prematura a óptimos locales.\n",
    "    \n",
    "    Proceso de mutación:\n",
    "    1. Para cada parámetro de la red neuronal:\n",
    "       - Probabilidad MUTATION_RATE: agregar ruido gaussiano\n",
    "       - Magnitud del ruido: distribución normal σ=0.1\n",
    "    2. Recargar parámetros modificados en la red\n",
    "    \n",
    "    Args:\n",
    "        agent (Agent): Agente a mutar\n",
    "        \n",
    "    Returns:\n",
    "        Agent: El mismo agente con genes posiblemente mutados\n",
    "        \n",
    "    Note:\n",
    "        La mutación gaussiana con σ=0.1 proporciona un balance entre\n",
    "        exploración (nuevas soluciones) y explotación (preservar buenas soluciones).\n",
    "    \"\"\"\n",
    "    child_genes = agent.network.state_dict()\n",
    "    \n",
    "    # Aplicar mutación a cada parámetro independientemente\n",
    "    for key in child_genes.keys():\n",
    "        if random.random() < MUTATION_RATE:\n",
    "            # Agregar ruido gaussiano con desviación estándar 0.1\n",
    "            noise = torch.randn_like(child_genes[key]) * 0.1\n",
    "            child_genes[key] += noise\n",
    "    \n",
    "    # Recargar parámetros mutados en la red\n",
    "    agent.network.load_state_dict(child_genes)\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Bucle principal del algoritmo genético.\n",
    "    \n",
    "    Ejecuta el proceso evolutivo completo a través de múltiples generaciones,\n",
    "    implementando el ciclo: evaluación → selección → cruzamiento → mutación.\n",
    "    Incluye elitismo para preservar las mejores soluciones y logging detallado\n",
    "    del progreso evolutivo.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"🧬 INICIANDO ENTRENAMIENTO CON ALGORITMO GENÉTICO 🧬\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Parámetros de evolución:\")\n",
    "    print(f\"🔹 Tamaño de población: {POPULATION_SIZE}\")\n",
    "    print(f\"🔹 Generaciones: {NUM_GENERATIONS}\")\n",
    "    print(f\"🔹 Tasa de mutación: {MUTATION_RATE*100}%\")\n",
    "    print(f\"🔹 Elitismo: {ELITISM_COUNT} agentes\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # INICIALIZACIÓN\n",
    "    env = GridEnvironment()\n",
    "    population = create_initial_population()\n",
    "    \n",
    "    print(\"🌱 Población inicial creada\")\n",
    "    print(\"🎯 Comenzando evolución...\")\n",
    "    print()\n",
    "\n",
    "    # BUCLE EVOLUTIVO PRINCIPAL\n",
    "    for gen in range(NUM_GENERATIONS):\n",
    "        print(f\"🔄 Generación {gen+1}/{NUM_GENERATIONS}\")\n",
    "        \n",
    "        # FASE 1: EVALUACIÓN DE FITNESS\n",
    "        evaluate_fitness(population, env)\n",
    "        \n",
    "        # FASE 2: SELECCIÓN DE PADRES\n",
    "        parents = selection(population)\n",
    "        \n",
    "        # FASE 3: ANÁLISIS DE PROGRESO\n",
    "        best_agent_of_gen = parents[0]  # El mejor agente de esta generación\n",
    "        best_fitness = best_agent_of_gen.fitness\n",
    "        avg_fitness = np.mean([agent.fitness for agent in population])\n",
    "        \n",
    "        # Logging del progreso evolutivo\n",
    "        print(f\"   📊 Mejor fitness: {best_fitness:.2f}\")\n",
    "        print(f\"   📈 Fitness promedio: {avg_fitness:.2f}\")\n",
    "        print(f\"   🏆 Mejora: {best_fitness - avg_fitness:.2f}\")\n",
    "\n",
    "        # FASE 4: PRESERVACIÓN DEL MEJOR AGENTE\n",
    "        # Guardar genes del mejor agente de esta generación\n",
    "        torch.save(best_agent_of_gen.network.state_dict(), \"best_agent_genes.pth\")\n",
    "        print(f\"   💾 Mejor agente guardado\")\n",
    "\n",
    "        # FASE 5: CREACIÓN DE NUEVA GENERACIÓN\n",
    "        new_population = []\n",
    "        \n",
    "        # ELITISMO: Los mejores agentes pasan directamente\n",
    "        new_population.extend(parents[:ELITISM_COUNT])\n",
    "        print(f\"   👑 {ELITISM_COUNT} elite preservados\")\n",
    "\n",
    "        # REPRODUCCIÓN: Llenar resto con descendencia\n",
    "        offspring_count = 0\n",
    "        while len(new_population) < POPULATION_SIZE:\n",
    "            # Seleccionar dos padres aleatoriamente del pool de elite\n",
    "            parent1, parent2 = random.sample(parents, 2)\n",
    "            \n",
    "            # Cruzamiento: combinar genes de padres\n",
    "            child = crossover(parent1, parent2)\n",
    "            \n",
    "            # Mutación: introducir variación genética\n",
    "            child = mutate(child)\n",
    "            \n",
    "            new_population.append(child)\n",
    "            offspring_count += 1\n",
    "            \n",
    "        print(f\"   👶 {offspring_count} descendientes creados\")\n",
    "        \n",
    "        # Reemplazar población anterior\n",
    "        population = new_population\n",
    "        \n",
    "        print(\"   ✅ Generación completada\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # FINALIZACIÓN\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎉 ¡ENTRENAMIENTO EVOLUTIVO COMPLETADO! 🎉\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"📁 El mejor ADN evolutivo está guardado en 'best_agent_genes.pth'\")\n",
    "    print(\"🚀 Puedes usar este agente en los demostradores para ver su comportamiento\")\n",
    "    print(\"🧬 El agente ha evolucionado a través de\", NUM_GENERATIONS, \"generaciones\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3e3bc",
   "metadata": {},
   "source": [
    "### Imitación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596289e",
   "metadata": {},
   "source": [
    "##### a_star_solver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_star_solver.py\n",
    "\"\"\"\n",
    "Implementación del algoritmo A* para navegación óptima en grid.\n",
    "\n",
    "Este módulo proporciona funcionalidades para encontrar el camino más corto\n",
    "entre dos puntos en una cuadrícula, evitando obstáculos (venenos). Utiliza\n",
    "el algoritmo A* con distancia Manhattan como heurística para garantizar\n",
    "optimalidad en entornos de grid.\n",
    "\n",
    "Funciones:\n",
    "    heuristic(a, b): Calcula distancia Manhattan entre dos puntos\n",
    "    a_star_search(grid_size, agent_pos, goal_pos, poisons): Encuentra camino óptimo\n",
    "\"\"\"\n",
    "import heapq\n",
    "\n",
    "def heuristic(a, b):\n",
    "    \"\"\"\n",
    "    Calcula la distancia heurística entre dos puntos usando distancia Manhattan.\n",
    "    \n",
    "    La distancia Manhattan es la suma de las diferencias absolutas de sus\n",
    "    coordenadas cartesianas. Es una heurística admisible para movimiento\n",
    "    en grid con 4 direcciones, garantizando optimalidad en A*.\n",
    "    \n",
    "    Args:\n",
    "        a (tuple): Coordenadas (x, y) del primer punto\n",
    "        b (tuple): Coordenadas (x, y) del segundo punto\n",
    "    \n",
    "    Returns:\n",
    "        int: Distancia Manhattan entre los puntos a y b\n",
    "    \n",
    "    Example:\n",
    "        >>> heuristic((0, 0), (3, 4))\n",
    "        7\n",
    "        >>> heuristic((1, 1), (1, 1))\n",
    "        0\n",
    "    \"\"\"\n",
    "    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "def a_star_search(grid_size, agent_pos, goal_pos, poisons):\n",
    "    \"\"\"\n",
    "    Implementa el algoritmo A* para encontrar el camino óptimo en una cuadrícula.\n",
    "    \n",
    "    Busca el camino más corto desde la posición del agente hasta el objetivo,\n",
    "    evitando venenos y respetando los límites del grid. Utiliza una heurística\n",
    "    admisible (distancia Manhattan) para garantizar optimalidad.\n",
    "    \n",
    "    Algoritmo A*:\n",
    "        1. Mantiene conjunto abierto (por explorar) y cerrado (explorados)\n",
    "        2. Evalúa nodos usando f(n) = g(n) + h(n):\n",
    "           - g(n): Costo real desde inicio hasta nodo n\n",
    "           - h(n): Heurística desde nodo n hasta objetivo\n",
    "        3. Expande el nodo con menor f(n) hasta encontrar objetivo\n",
    "        4. Reconstruye camino desde objetivo hasta inicio\n",
    "    \n",
    "    Args:\n",
    "        grid_size (int): Tamaño de la cuadrícula (grid_size x grid_size)\n",
    "        agent_pos (tuple): Posición inicial del agente (x, y)\n",
    "        goal_pos (tuple): Posición objetivo a alcanzar (x, y)\n",
    "        poisons (list): Lista de posiciones de venenos [(x, y), ...]\n",
    "    \n",
    "    Returns:\n",
    "        list: Secuencia de posiciones [(x, y), ...] del camino óptimo,\n",
    "              excluyendo posición inicial. None si no existe camino.\n",
    "    \n",
    "    Example:\n",
    "        >>> a_star_search(5, (0, 0), (4, 4), [(2, 2)])\n",
    "        [(1, 0), (2, 0), (3, 0), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]\n",
    "    \n",
    "    Note:\n",
    "        - Movimiento limitado a 4 direcciones (arriba, abajo, izquierda, derecha)\n",
    "        - Venenos son obstáculos impasables\n",
    "        - Retorna None si el objetivo es inalcanzable\n",
    "    \"\"\"\n",
    "    # Convertir posiciones a tuplas para consistencia\n",
    "    start = tuple(agent_pos)\n",
    "    goal = tuple(goal_pos)\n",
    "    \n",
    "    # Inicializar estructuras de datos del algoritmo A*\n",
    "    close_set = set()           # Nodos ya explorados\n",
    "    came_from = {}              # Mapeo para reconstruir camino\n",
    "    g_score = {start: 0}        # Costo real desde inicio\n",
    "    f_score = {start: heuristic(start, goal)}  # Costo estimado total\n",
    "    \n",
    "    # Heap para mantener nodos ordenados por f_score\n",
    "    open_heap = [(f_score[start], start)]\n",
    "\n",
    "    # Bucle principal del algoritmo A*\n",
    "    while open_heap:\n",
    "        # Extraer nodo con menor f_score\n",
    "        current = heapq.heappop(open_heap)[1]\n",
    "\n",
    "        # ¿Hemos llegado al objetivo?\n",
    "        if current == goal:\n",
    "            # Reconstruir camino desde objetivo hasta inicio\n",
    "            path = [current]\n",
    "            while current in came_from:\n",
    "                current = came_from[current]\n",
    "                path.insert(0, current)\n",
    "            # Excluir posición inicial del camino retornado\n",
    "            path.pop(0)\n",
    "            return path\n",
    "\n",
    "        # Marcar nodo actual como explorado\n",
    "        close_set.add(current)\n",
    "        \n",
    "        # Explorar todos los vecinos (4 direcciones)\n",
    "        for i, j in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n",
    "            neighbor = (current[0] + i, current[1] + j)\n",
    "            \n",
    "            # Verificar límites del grid\n",
    "            if not (0 <= neighbor[0] < grid_size and 0 <= neighbor[1] < grid_size):\n",
    "                continue\n",
    "            \n",
    "            # Verificar obstáculos: nodos explorados y venenos\n",
    "            if neighbor in close_set or any(tuple(p) == neighbor for p in poisons):\n",
    "                continue\n",
    "\n",
    "            # Calcular nuevo costo para llegar al vecino\n",
    "            tentative_g_score = g_score[current] + 1\n",
    "            \n",
    "            # ¿Es este un mejor camino al vecino?\n",
    "            if tentative_g_score < g_score.get(neighbor, float('inf')):\n",
    "                # Registrar mejor camino encontrado\n",
    "                came_from[neighbor] = current\n",
    "                g_score[neighbor] = tentative_g_score\n",
    "                f_score[neighbor] = tentative_g_score + heuristic(neighbor, goal)\n",
    "                # Agregar vecino a conjunto de exploración\n",
    "                heapq.heappush(open_heap, (f_score[neighbor], neighbor))\n",
    "    \n",
    "    # No se encontró camino al objetivo\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80e546",
   "metadata": {},
   "source": [
    "#### generate_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_data.py\n",
    "\"\"\"\n",
    "Generador de datos de demostración experta para aprendizaje por imitación.\n",
    "\n",
    "Este módulo crea datasets de pares estado-acción obtenidos de un agente experto\n",
    "que utiliza el algoritmo A* para navegación óptima. Los datos generados se\n",
    "utilizan para entrenar agentes mediante aprendizaje supervisado, imitando\n",
    "comportamiento experto en diferentes configuraciones de complejidad.\n",
    "\n",
    "Funciones:\n",
    "    get_action(from_pos, to_pos): Convierte movimiento posicional a índice de acción\n",
    "    generate_expert_data_for_n_fruits(num_fruits, num_samples, output_file): \n",
    "        Genera dataset para configuración específica de frutas\n",
    "        \n",
    "Constantes:\n",
    "    GRID_SIZE: Tamaño del entorno de cuadrícula (5x5)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from environment import GridEnvironment\n",
    "from a_star_solver import a_star_search\n",
    "\n",
    "# Configuración del entorno\n",
    "GRID_SIZE = 5\n",
    "\n",
    "def get_action(from_pos, to_pos):\n",
    "    \"\"\"\n",
    "    Convierte un movimiento entre posiciones adyacentes en índice de acción.\n",
    "    \n",
    "    Calcula la diferencia vectorial entre posiciones y la mapea al índice\n",
    "    de acción correspondiente. Utilizada para convertir el camino óptimo\n",
    "    de A* en secuencia de acciones ejecutables por el agente.\n",
    "    \n",
    "    Args:\n",
    "        from_pos (tuple/np.ndarray): Posición inicial (x, y)\n",
    "        to_pos (tuple/np.ndarray): Posición objetivo (x, y)\n",
    "    \n",
    "    Returns:\n",
    "        int: Índice de acción correspondiente al movimiento:\n",
    "             0 = Arriba (decrementar x)\n",
    "             1 = Abajo (incrementar x)  \n",
    "             2 = Izquierda (decrementar y)\n",
    "             3 = Derecha (incrementar y)\n",
    "             -1 = Movimiento inválido (no adyacente)\n",
    "    \n",
    "    Example:\n",
    "        >>> get_action((1, 1), (0, 1))  # Movimiento hacia arriba\n",
    "        0\n",
    "        >>> get_action((1, 1), (1, 2))  # Movimiento hacia derecha\n",
    "        3\n",
    "    \n",
    "    Note:\n",
    "        Solo funciona para posiciones adyacentes. Movimientos diagonales\n",
    "        o de múltiples celdas retornan -1.\n",
    "    \"\"\"\n",
    "    # Calcular vector de diferencia entre posiciones\n",
    "    delta = np.array(to_pos) - np.array(from_pos)\n",
    "    \n",
    "    # Mapear diferencia a índice de acción\n",
    "    if delta[0] == -1: return 0    # Arriba\n",
    "    if delta[0] == 1: return 1     # Abajo\n",
    "    if delta[1] == -1: return 2    # Izquierda\n",
    "    if delta[1] == 1: return 3     # Derecha\n",
    "    return -1  # Movimiento inválido\n",
    "\n",
    "def generate_expert_data_for_n_fruits(num_fruits, num_samples, output_file):\n",
    "    \"\"\"\n",
    "    Genera dataset de demostraciones expertas para configuración específica.\n",
    "    \n",
    "    Crea escenarios aleatorios con número fijo de frutas y utiliza A* para\n",
    "    generar comportamiento experto óptimo. Implementa estrategia greedy de\n",
    "    ir siempre a la fruta más cercana, creando datos de entrenamiento para\n",
    "    aprendizaje por imitación con curriculum learning.\n",
    "    \n",
    "    Proceso de generación:\n",
    "        1. Crear escenario aleatorio (agente, frutas, venenos)\n",
    "        2. Calcular fruta más cercana al agente\n",
    "        3. Usar A* para encontrar camino óptimo\n",
    "        4. Ejecutar primer paso y registrar par (estado, acción)\n",
    "        5. Repetir hasta completar episodio o fallar\n",
    "        6. Continuar hasta obtener muestras suficientes\n",
    "    \n",
    "    Args:\n",
    "        num_fruits (int): Número de frutas en cada escenario\n",
    "        num_samples (int): Cantidad objetivo de muestras estado-acción\n",
    "        output_file (str): Archivo pickle donde guardar el dataset\n",
    "    \n",
    "    Raises:\n",
    "        IOError: Si no se puede escribir el archivo de salida\n",
    "    \n",
    "    Example:\n",
    "        >>> generate_expert_data_for_n_fruits(2, 1000, \"data_2_fruits.pkl\")\n",
    "        Generando 1000 muestras para 2 fruta(s)...\n",
    "        Dataset 'data_2_fruits.pkl' creado con 1000 muestras.\n",
    "    \n",
    "    Note:\n",
    "        - Venenos colocados aleatoriamente (2-4 por escenario)\n",
    "        - Máximo 50 pasos por episodio para evitar bucles infinitos\n",
    "        - Estrategia greedy: siempre ir a fruta más cercana (euclidiana)\n",
    "        - Solo se registran acciones válidas (con camino A* factible)\n",
    "    \"\"\"\n",
    "    # Inicializar entorno y contenedor de datos\n",
    "    env = GridEnvironment()\n",
    "    expert_data = []\n",
    "    print(f\"Generando {num_samples} muestras para {num_fruits} fruta(s)...\")\n",
    "    \n",
    "    generated_episodes = 0\n",
    "    while len(expert_data) < num_samples:\n",
    "        generated_episodes += 1\n",
    "        \n",
    "        # Configurar escenario aleatorio\n",
    "        num_poisons = np.random.randint(2, 5)  # 2-4 venenos por escenario\n",
    "        \n",
    "        # Generar posiciones únicas aleatorias\n",
    "        all_pos = [(r, c) for r in range(GRID_SIZE) for c in range(GRID_SIZE)]\n",
    "        random.shuffle(all_pos)\n",
    "        \n",
    "        # Asignar posiciones a elementos del entorno\n",
    "        agent_p = all_pos.pop()\n",
    "        fruit_p = [all_pos.pop() for _ in range(num_fruits)]\n",
    "        poison_p = [all_pos.pop() for _ in range(num_poisons)]\n",
    "        \n",
    "        # Inicializar entorno con configuración generada\n",
    "        env.reset(agent_pos=agent_p, fruit_pos=fruit_p, poison_pos=poison_p)\n",
    "\n",
    "        # Simular episodio con comportamiento experto\n",
    "        for _ in range(50):  # Máximo 50 pasos por episodio\n",
    "            # Verificar condición de terminación por victoria\n",
    "            if not env.fruit_pos: \n",
    "                break\n",
    "            \n",
    "            # Implementar estrategia greedy: ir a fruta más cercana\n",
    "            agent_pos = env.agent_pos\n",
    "            # Calcular distancias euclidianas a todas las frutas\n",
    "            distances = [np.linalg.norm(agent_pos - f) for f in env.fruit_pos]\n",
    "            # Seleccionar fruta más cercana como objetivo\n",
    "            goal_fruit = env.fruit_pos[np.argmin(distances)]\n",
    "            \n",
    "            # Usar A* para encontrar camino óptimo al objetivo\n",
    "            path = a_star_search(GRID_SIZE, agent_pos, goal_fruit, env.poison_pos)\n",
    "\n",
    "            # Verificar si existe camino factible\n",
    "            if path and len(path) > 0:\n",
    "                # Convertir primer paso del camino a acción\n",
    "                action = get_action(agent_pos, path[0])\n",
    "                # Registrar par estado-acción para entrenamiento\n",
    "                state = env.get_state()\n",
    "                expert_data.append((state, action))\n",
    "                # Ejecutar acción en el entorno\n",
    "                env.step(action)\n",
    "            else:\n",
    "                # No hay camino factible: terminar episodio\n",
    "                break\n",
    "        \n",
    "        # Reporte de progreso cada 200 episodios\n",
    "        if generated_episodes % 200 == 0:\n",
    "            print(f\"  Partidas procesadas: {generated_episodes}, Muestras actuales: {len(expert_data)}\")\n",
    "\n",
    "    # Guardar dataset en archivo pickle\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(expert_data[:num_samples], f)\n",
    "    print(f\"Dataset '{output_file}' creado con {len(expert_data[:num_samples])} muestras.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Script principal para generación de curriculum de datasets.\n",
    "    \n",
    "    Genera múltiples datasets con diferentes niveles de complejidad para\n",
    "    implementar curriculum learning en el entrenamiento por imitación.\n",
    "    Los datasets se ordenan por dificultad creciente (número de frutas).\n",
    "    \n",
    "    Curriculum generado:\n",
    "        - 1 fruta: 4000 muestras (nivel básico)\n",
    "        - 2 frutas: 4000 muestras (nivel intermedio bajo)\n",
    "        - 3 frutas: 4000 muestras (nivel intermedio alto)\n",
    "        - 4 frutas: 5000 muestras (nivel avanzado, más muestras)\n",
    "    \n",
    "    Beneficios del curriculum learning:\n",
    "        - Aprendizaje gradual de complejidad creciente\n",
    "        - Mejor convergencia del entrenamiento\n",
    "        - Políticas más robustas y generalizables\n",
    "        - Reducción de overfitting a configuraciones específicas\n",
    "    \"\"\"\n",
    "    # Generar datasets con complejidad creciente\n",
    "    generate_expert_data_for_n_fruits(1, 4000, \"expert_data_1_fruit.pkl\")\n",
    "    generate_expert_data_for_n_fruits(2, 4000, \"expert_data_2_fruits.pkl\")\n",
    "    generate_expert_data_for_n_fruits(3, 4000, \"expert_data_3_fruits.pkl\")\n",
    "    generate_expert_data_for_n_fruits(4, 5000, \"expert_data_4_fruits.pkl\")\n",
    "    print(\"\\nTodos los datasets del currículo han sido generados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e625956",
   "metadata": {},
   "source": [
    "#### agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277310f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\"\"\"\n",
    "Implementación de agente con red neuronal convolucional para aprendizaje por imitación.\n",
    "\n",
    "Este módulo define la arquitectura de red neuronal y la clase agente utilizadas\n",
    "en el aprendizaje por imitación. La red procesa representaciones visuales del\n",
    "entorno (grids 3D) y predice acciones óptimas imitando comportamiento experto.\n",
    "\n",
    "Clases:\n",
    "    AgentNetwork: Red neuronal convolucional para procesamiento de estados visuales\n",
    "    Agent: Interfaz del agente que utiliza la red para toma de decisiones\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AgentNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal convolucional para procesamiento de estados de grid y predicción de acciones.\n",
    "    \n",
    "    Arquitectura diseñada específicamente para entornos de cuadrícula donde el estado\n",
    "    se representa como imágenes de 3 canales (agente, frutas, venenos). Utiliza\n",
    "    capas convolucionales para extracción de características espaciales seguidas\n",
    "    de capas densas para predicción de acciones.\n",
    "    \n",
    "    Arquitectura:\n",
    "        - Conv2D (3->16): Extracción de características básicas\n",
    "        - Conv2D (16->32): Características de nivel medio\n",
    "        - Flatten: Preparación para capas densas\n",
    "        - FC (flattened->256): Representación de alto nivel\n",
    "        - FC (256->4): Predicción de acciones (4 direcciones)\n",
    "    \n",
    "    Args:\n",
    "        h (int): Altura del grid de entrada (default: 5)\n",
    "        w (int): Ancho del grid de entrada (default: 5)\n",
    "        outputs (int): Número de acciones posibles (default: 4)\n",
    "    \"\"\"\n",
    "    def __init__(self, h=5, w=5, outputs=4):\n",
    "        super(AgentNetwork, self).__init__()\n",
    "        \n",
    "        # Capas convolucionales para procesamiento espacial\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Función auxiliar para calcular tamaño de salida convolucional\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            \"\"\"Calcula dimensión de salida después de operación convolucional.\"\"\"\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        # Calcular dimensiones después de capas convolucionales\n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        # Capas densas para predicción final\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante de la red neuronal.\n",
    "        \n",
    "        Procesa el estado visual del entorno a través de las capas convolucionales\n",
    "        y densas para generar valores de acción. Utiliza ReLU como función de\n",
    "        activación para introducir no-linealidad.\n",
    "        \n",
    "        Flujo de procesamiento:\n",
    "            1. Conv1 + ReLU: Extracción de características básicas\n",
    "            2. Conv2 + ReLU: Características de nivel medio\n",
    "            3. Flatten: Conversión a vector 1D\n",
    "            4. FC1 + ReLU: Representación de alto nivel\n",
    "            5. FC2: Valores de acción finales (sin activación)\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Estado del entorno con forma (batch_size, 3, h, w)\n",
    "                             Canales: [agente, frutas, venenos]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Valores de acción con forma (batch_size, 4)\n",
    "                         Índices corresponden a [arriba, abajo, izquierda, derecha]\n",
    "        \"\"\"\n",
    "        # Primera capa convolucional con activación ReLU\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # Segunda capa convolucional con activación ReLU\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # Aplanar para conexión con capas densas\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Primera capa densa con activación ReLU\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        # Capa de salida sin activación (valores de acción)\n",
    "        return self.fc2(x)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agente que utiliza red neuronal para toma de decisiones por imitación.\n",
    "    \n",
    "    Implementa la interfaz de agente que encapsula la red neuronal y proporciona\n",
    "    métodos para selección de acciones y carga de modelos pre-entrenados.\n",
    "    Diseñado para imitar comportamiento experto aprendido de datos de demostración.\n",
    "    \n",
    "    Attributes:\n",
    "        network (AgentNetwork): Red neuronal convolucional para predicción de acciones\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el agente con red neuronal por defecto.\n",
    "        \n",
    "        Crea una instancia de AgentNetwork con parámetros estándar\n",
    "        para entornos de grid 5x5 con 4 acciones posibles.\n",
    "        \"\"\"\n",
    "        self.network = AgentNetwork()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Selecciona la acción óptima basada en el estado actual del entorno.\n",
    "        \n",
    "        Utiliza la red neuronal para evaluar el estado y selecciona la acción\n",
    "        con mayor valor predicho. Implementa una política determinística\n",
    "        (greedy) que siempre elige la mejor acción según el modelo.\n",
    "        \n",
    "        Proceso:\n",
    "            1. Convierte estado a tensor PyTorch\n",
    "            2. Agrega dimensión de batch (unsqueeze)\n",
    "            3. Realiza inferencia sin gradientes\n",
    "            4. Selecciona acción con mayor valor (argmax)\n",
    "        \n",
    "        Args:\n",
    "            state (numpy.ndarray): Estado del entorno con forma (3, h, w)\n",
    "                                  Canales: [agente, frutas, venenos]\n",
    "        \n",
    "        Returns:\n",
    "            int: Índice de la acción seleccionada\n",
    "                 0=Arriba, 1=Abajo, 2=Izquierda, 3=Derecha\n",
    "        \n",
    "        Note:\n",
    "            Utiliza torch.no_grad() para optimizar inferencia y evitar\n",
    "            construcción del grafo computacional durante evaluación.\n",
    "        \"\"\"\n",
    "        # Convertir estado a tensor y agregar dimensión de batch\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Realizar inferencia sin cálculo de gradientes\n",
    "        with torch.no_grad():\n",
    "            action_values = self.network(state_tensor)\n",
    "        \n",
    "        # Seleccionar acción con mayor valor predicho\n",
    "        return torch.argmax(action_values).item()\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Carga pesos pre-entrenados en la red neuronal del agente.\n",
    "        \n",
    "        Permite cargar modelos entrenados mediante aprendizaje por imitación\n",
    "        para utilizar políticas aprendidas de datos de demostración experta.\n",
    "        Los pesos se cargan directamente en la red neuronal existente.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Ruta al archivo de modelo PyTorch (.pth)\n",
    "                           que contiene los state_dict de la red\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: Si el archivo de modelo no existe\n",
    "            RuntimeError: Si hay incompatibilidad en arquitectura de red\n",
    "        \n",
    "        Example:\n",
    "            >>> agent = Agent()\n",
    "            >>> agent.load_model('imitacion_model.pth')\n",
    "            >>> action = agent.choose_action(current_state)\n",
    "        \n",
    "        Note:\n",
    "            El modelo cargado debe tener la misma arquitectura que\n",
    "            AgentNetwork para evitar errores de compatibilidad.\n",
    "        \"\"\"\n",
    "        self.network.load_state_dict(torch.load(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e0b91",
   "metadata": {},
   "source": [
    "#### imitacion_agente.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demostración interactiva de agente entrenado por aprendizaje por imitación.\n",
    "\n",
    "Este módulo proporciona una interfaz gráfica para demostrar el comportamiento\n",
    "de un agente que ha aprendido por imitación de datos expertos. Permite al\n",
    "usuario diseñar niveles y observar cómo el agente navega utilizando la\n",
    "política aprendida mediante redes neuronales convolucionales.\n",
    "\n",
    "Características:\n",
    "    - Modo configuración: Diseño interactivo de niveles\n",
    "    - Modo juego: Demostración automática del agente entrenado\n",
    "    - Interfaz visual: Pygame con sprites y feedback en tiempo real\n",
    "    - Carga de modelos: Integración con modelos PyTorch pre-entrenados\n",
    "\n",
    "Constantes:\n",
    "    GRID_WIDTH, GRID_HEIGHT: Dimensiones del entorno (5x5)\n",
    "    CELL_SIZE: Tamaño de cada celda en píxeles (120px)\n",
    "    SCREEN_WIDTH, SCREEN_HEIGHT: Dimensiones de la ventana\n",
    "    COLOR_*: Esquema de colores para la interfaz\n",
    "\n",
    "Clases:\n",
    "    EntornoGrid: Entorno de demostración con funcionalidades completas\n",
    "\"\"\"\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from agent import Agent\n",
    "\n",
    "# Configuración del entorno y pantalla\n",
    "GRID_WIDTH = 5\n",
    "GRID_HEIGHT = 5\n",
    "CELL_SIZE = 120\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE\n",
    "\n",
    "# Esquema de colores para interfaz oscura\n",
    "COLOR_FONDO = (25, 25, 25)      # Fondo principal oscuro\n",
    "COLOR_LINEAS = (40, 40, 40)     # Líneas de grid sutiles\n",
    "COLOR_CURSOR = (255, 255, 0)    # Cursor amarillo brillante\n",
    "COLOR_TEXTO = (230, 230, 230)   # Texto claro para legibilidad\n",
    "\n",
    "\n",
    "class EntornoGrid:\n",
    "    \"\"\"\n",
    "    Entorno de demostración para agente entrenado por imitación.\n",
    "    \n",
    "    Implementa un entorno de grid completo con capacidades de configuración\n",
    "    interactiva y simulación de episodios. Diseñado para demostrar el\n",
    "    comportamiento aprendido del agente en diferentes escenarios.\n",
    "    \n",
    "    Funcionalidades:\n",
    "        - Configuración manual de elementos (frutas, venenos, paredes)\n",
    "        - Simulación de episodios con agente automático\n",
    "        - Sistema de recompensas completo para feedback\n",
    "        - Renderizado visual con sprites\n",
    "        - Detección de colisiones y condiciones de terminación\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tamaño del grid (5x5)\n",
    "        agent_pos (tuple): Posición actual del agente (fila, columna)\n",
    "        frutas (set): Conjunto de posiciones de frutas\n",
    "        venenos (set): Conjunto de posiciones de venenos\n",
    "        paredes (set): Conjunto de posiciones de paredes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuración por defecto.\n",
    "        \n",
    "        Establece grid vacío con agente en posición (0,0) y\n",
    "        conjuntos vacíos para elementos del entorno.\n",
    "        \"\"\"\n",
    "        self.size = GRID_WIDTH\n",
    "        self.agent_pos = (0, 0)\n",
    "        self.frutas = set()\n",
    "        self.venenos = set()\n",
    "        self.paredes = set()\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"\n",
    "        Reinicia el agente a la posición inicial del episodio.\n",
    "        \n",
    "        Coloca al agente en (0,0) manteniendo la configuración actual\n",
    "        del entorno. Utilizado al iniciar nuevos episodios de demostración.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Estado inicial del entorno con forma (3, size, size)\n",
    "        \"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno.\n",
    "        \n",
    "        Limpia completamente frutas, venenos y paredes, dejando\n",
    "        un grid vacío para nueva configuración. El agente mantiene\n",
    "        su posición actual.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción del agente y actualiza el estado del entorno.\n",
    "        \n",
    "        Procesa el movimiento del agente, verifica colisiones y calcula\n",
    "        recompensas según las interacciones con elementos del entorno.\n",
    "        Implementa la lógica completa de simulación para demostración.\n",
    "        \n",
    "        Sistema de recompensas:\n",
    "            - Movimiento normal: -0.05 (costo por paso)\n",
    "            - Movimiento inválido: -0.1 (penalización)\n",
    "            - Fruta recolectada: +1.0 (objetivo positivo)\n",
    "            - Todas las frutas: +10.0 adicional (victoria)\n",
    "            - Veneno tocado: -10.0 (penalización grave)\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Acción a ejecutar\n",
    "                         0 = Arriba (decrementar fila)\n",
    "                         1 = Abajo (incrementar fila)\n",
    "                         2 = Izquierda (decrementar columna)\n",
    "                         3 = Derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, terminado)\n",
    "                - nuevo_estado (np.ndarray): Estado resultante (3, size, size)\n",
    "                - recompensa (float): Recompensa por la acción ejecutada\n",
    "                - terminado (bool): True si episodio terminó, False en caso contrario\n",
    "        \"\"\"\n",
    "        # Calcular nueva posición basada en la acción\n",
    "        fila, col = self.agent_pos\n",
    "        if accion == 0:\n",
    "            fila -= 1    # Arriba\n",
    "        elif accion == 1:\n",
    "            fila += 1    # Abajo\n",
    "        elif accion == 2:\n",
    "            col -= 1     # Izquierda\n",
    "        elif accion == 3:\n",
    "            col += 1     # Derecha\n",
    "\n",
    "        # Verificar límites del entorno y colisiones con paredes\n",
    "        if (\n",
    "            fila < 0\n",
    "            or fila >= GRID_HEIGHT\n",
    "            or col < 0\n",
    "            or col >= GRID_WIDTH\n",
    "            or (fila, col) in self.paredes\n",
    "        ):\n",
    "            # Movimiento inválido: mantener posición y penalizar\n",
    "            return self.get_state(), -0.1, False\n",
    "\n",
    "        # Movimiento válido: actualizar posición\n",
    "        self.agent_pos = (fila, col)\n",
    "        recompensa = -0.05  # Costo base por movimiento\n",
    "        terminado = False\n",
    "\n",
    "        # Procesar interacciones con elementos del entorno\n",
    "        if self.agent_pos in self.venenos:\n",
    "            # Veneno tocado: penalización grave y reset a inicio\n",
    "            recompensa = -10.0\n",
    "            self.agent_pos = (0, 0)\n",
    "        elif self.agent_pos in self.frutas:\n",
    "            # Fruta recolectada: recompensa positiva\n",
    "            recompensa = 1.0\n",
    "            self.frutas.remove(self.agent_pos)\n",
    "            # Verificar victoria (todas las frutas recolectadas)\n",
    "            if not self.frutas:\n",
    "                recompensa += 10.0  # Bonus por completar nivel\n",
    "                terminado = True\n",
    "                self.agent_pos = (0, 0)  # Reset a posición inicial\n",
    "\n",
    "        return self.get_state(), recompensa, terminado\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera representación visual del estado actual del entorno.\n",
    "        \n",
    "        Crea tensor 3D donde cada canal representa un tipo de elemento,\n",
    "        compatible con la arquitectura CNN del agente entrenado.\n",
    "        \n",
    "        Estructura de canales:\n",
    "            - Canal 0: Posición del agente (binario)\n",
    "            - Canal 1: Posiciones de frutas (binario)\n",
    "            - Canal 2: Posiciones de venenos (binario)\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Estado con forma (3, size, size) y dtype float32\n",
    "                       Valores 1.0 indican presencia, 0.0 ausencia\n",
    "        \n",
    "        Note:\n",
    "            Las paredes no se incluyen en el estado ya que el agente\n",
    "            entrenado no las consideraba en los datos de demostración.\n",
    "        \"\"\"\n",
    "        # Inicializar tensor de estado\n",
    "        estado = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posición del agente\n",
    "        estado[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruta in self.frutas:\n",
    "            estado[1, fruta[0], fruta[1]] = 1.0\n",
    "        \n",
    "        # Canal 2: Posiciones de venenos\n",
    "        for veneno in self.venenos:\n",
    "            estado[2, veneno[0], veneno[1]] = 1.0\n",
    "        \n",
    "        return estado\n",
    "\n",
    "    def dibujar(\n",
    "        self,\n",
    "        pantalla,\n",
    "        modo_juego,\n",
    "        cursor_pos,\n",
    "        img_fruta,\n",
    "        img_veneno,\n",
    "        img_pared,\n",
    "        img_agente,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Renderiza el estado completo del entorno con interfaz de usuario.\n",
    "        \n",
    "        Dibuja todos los elementos visuales del entorno, grid de navegación,\n",
    "        cursor de configuración e información de controles. Proporciona\n",
    "        feedback visual completo para ambos modos de operación.\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde renderizar\n",
    "            modo_juego (str): Modo actual (\"SETUP\" o \"PLAYING\")\n",
    "            cursor_pos (tuple): Posición del cursor en modo configuración\n",
    "            img_fruta (pygame.Surface): Sprite de las frutas\n",
    "            img_veneno (pygame.Surface): Sprite de los venenos\n",
    "            img_pared (pygame.Surface): Sprite de las paredes\n",
    "            img_agente (pygame.Surface): Sprite del agente\n",
    "        \n",
    "        Note:\n",
    "            Renderiza en orden específico para evitar superposiciones:\n",
    "            fondo → grid → paredes → frutas → venenos → agente → cursor → UI\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con fondo oscuro\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar líneas del grid para navegación visual\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Renderizar elementos del entorno (orden: paredes → frutas → venenos)\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "\n",
    "        # Dibujar agente (siempre en primer plano)\n",
    "        pantalla.blit(\n",
    "            img_agente, (self.agent_pos[0] * CELL_SIZE, self.agent_pos[1] * CELL_SIZE)\n",
    "        )\n",
    "\n",
    "        # Mostrar cursor en modo configuración\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar información de interfaz\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        controles1 = font.render(\n",
    "            \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\", True, COLOR_TEXTO\n",
    "        )\n",
    "        controles2 = font.render(\"P=Jugar, S=Setup\", True, COLOR_TEXTO)\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles1, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles2, (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal de la demostración interactiva del agente por imitación.\n",
    "    \n",
    "    Inicializa la interfaz gráfica y gestiona el bucle principal que permite\n",
    "    alternar entre modo configuración (diseño de niveles) y modo demostración\n",
    "    (agente automático). Proporciona una experiencia completa para evaluar\n",
    "    el rendimiento del agente entrenado.\n",
    "    \n",
    "    Flujo de la aplicación:\n",
    "        1. Inicialización de Pygame y recursos\n",
    "        2. Carga del modelo entrenado\n",
    "        3. Bucle principal con dos modos:\n",
    "           - SETUP: Configuración manual de niveles\n",
    "           - PLAYING: Demostración automática del agente\n",
    "        4. Manejo de eventos y renderizado en tiempo real\n",
    "    \n",
    "    Controles disponibles:\n",
    "        Modo SETUP:\n",
    "            - Flechas: Mover cursor de configuración\n",
    "            - F: Colocar/quitar fruta\n",
    "            - V: Colocar/quitar veneno\n",
    "            - W: Colocar/quitar pared\n",
    "            - C: Limpiar entorno completamente\n",
    "            - P: Iniciar demostración automática\n",
    "        \n",
    "        Modo PLAYING:\n",
    "            - S: Volver a modo configuración\n",
    "            - Agente se mueve automáticamente cada 0.1 segundos\n",
    "    \n",
    "    Note:\n",
    "        Requiere modelo entrenado en \"IMITACION/imitacion_model.pth\"\n",
    "        y sprites en directorio padre (../fruta.png, etc.)\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y configurar ventana\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente por Imitación - Come Frutas 🍓\")\n",
    "\n",
    "    def cargar_img(nombre, color_fallback):\n",
    "        \"\"\"\n",
    "        Función auxiliar para carga robusta de sprites.\n",
    "        \n",
    "        Intenta cargar imagen desde archivo, si falla crea superficie\n",
    "        de color sólido como respaldo para mantener funcionalidad.\n",
    "        \n",
    "        Args:\n",
    "            nombre (str): Nombre del archivo de imagen\n",
    "            color_fallback (tuple): Color RGB de respaldo\n",
    "        \n",
    "        Returns:\n",
    "            pygame.Surface: Sprite cargado o superficie de color\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "            img = pygame.image.load(ruta).convert_alpha()\n",
    "            return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "        except:\n",
    "            surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "            surf.fill(color_fallback)\n",
    "            return surf\n",
    "\n",
    "    # Cargar sprites con colores de respaldo\n",
    "    img_fruta = cargar_img(\"../fruta.png\", (0, 255, 0))        # Verde\n",
    "    img_veneno = cargar_img(\"../veneno.png\", (255, 0, 0))      # Rojo\n",
    "    img_pared = cargar_img(\"../pared.png\", (100, 100, 100))    # Gris\n",
    "    img_agente = cargar_img(\"../agente.png\", (0, 0, 255))      # Azul\n",
    "\n",
    "    # Inicializar entorno y agente\n",
    "    entorno = EntornoGrid()\n",
    "    agente = Agent()\n",
    "    agente.load_model(\"IMITACION/imitacion_model.pth\")\n",
    "\n",
    "    # Variables de estado de la aplicación\n",
    "    cursor_pos = [0, 0]\n",
    "    modo_juego = \"SETUP\"\n",
    "    reloj = pygame.time.Clock()\n",
    "    corriendo = True\n",
    "\n",
    "    # Bucle principal de la aplicación\n",
    "    while corriendo:\n",
    "        # Procesar eventos de entrada\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # Cambio de modos globales\n",
    "                if evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO JUEGO ---\")\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    time.sleep(0.5)  # Pausa para visibilidad del cambio\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # Controles específicos del modo SETUP\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Navegación del cursor con flechas\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Colocación/eliminación de elementos\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        # Toggle fruta: agregar/quitar y limpiar otros elementos\n",
    "                        if pos in entorno.frutas:\n",
    "                            entorno.frutas.remove(pos)\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos)\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        # Toggle veneno: agregar/quitar y limpiar otros elementos\n",
    "                        if pos in entorno.venenos:\n",
    "                            entorno.venenos.remove(pos)\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos)\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        # Toggle pared: agregar/quitar y limpiar otros elementos\n",
    "                        if pos in entorno.paredes:\n",
    "                            entorno.paredes.remove(pos)\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos)\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.venenos.discard(pos)\n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        # Limpiar entorno completamente\n",
    "                        print(\"--- LIMPIANDO ENTORNO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "\n",
    "        # Lógica del modo PLAYING (agente automático)\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # Obtener estado actual y decidir acción\n",
    "            estado = entorno.get_state()\n",
    "            accion = agente.choose_action(estado)\n",
    "            # Ejecutar acción y verificar terminación\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            if terminado:\n",
    "                print(\"Juego terminado. Volviendo a SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "            time.sleep(0.1)  # Velocidad de demostración controlada\n",
    "\n",
    "        # Renderizado del estado actual\n",
    "        pantalla_con_info = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "        pantalla_con_info.fill(COLOR_FONDO)\n",
    "        entorno.dibujar(\n",
    "            pantalla_con_info,\n",
    "            modo_juego,\n",
    "            tuple(cursor_pos),\n",
    "            img_fruta,\n",
    "            img_veneno,\n",
    "            img_pared,\n",
    "            img_agente,\n",
    "        )\n",
    "        pantalla.blit(pantalla_con_info, (0, 0))\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(60)  # 60 FPS para fluidez visual\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568fc5b",
   "metadata": {},
   "source": [
    "#### train_curriculum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fe5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_curriculum.py\n",
    "\"\"\"\n",
    "Entrenamiento por curriculum learning para aprendizaje por imitación.\n",
    "\n",
    "Este módulo implementa el entrenamiento de la red neuronal convolucional\n",
    "utilizando curriculum learning con datasets de complejidad creciente.\n",
    "El entrenamiento progresa desde escenarios simples (1 fruta) hasta\n",
    "complejos (4 frutas), mejorando la convergencia y generalización.\n",
    "\n",
    "Características:\n",
    "    - Curriculum learning con 4 niveles de dificultad\n",
    "    - Entrenamiento supervisado con pares estado-acción\n",
    "    - Optimización Adam con learning rate adaptado\n",
    "    - CrossEntropyLoss para clasificación de acciones\n",
    "    - Progresión gradual de épocas por complejidad\n",
    "\n",
    "Constantes:\n",
    "    LEARNING_RATE: Tasa de aprendizaje para optimizador Adam (0.0005)\n",
    "    BATCH_SIZE: Tamaño de lote para entrenamiento (128 muestras)\n",
    "    CURRICULUM: Secuencia de datasets y épocas de entrenamiento\n",
    "\n",
    "Flujo del entrenamiento:\n",
    "    1. Lección 1: 1 fruta → 25 épocas (fundamentos básicos)\n",
    "    2. Lección 2: 2 frutas → 30 épocas (navegación intermedia)\n",
    "    3. Lección 3: 3 frutas → 40 épocas (planificación compleja)\n",
    "    4. Lección 4: 4 frutas → 50 épocas (maestría y refinamiento)\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "from agent import AgentNetwork\n",
    "\n",
    "# Hiperparámetros de entrenamiento\n",
    "LEARNING_RATE = 0.0005  # Tasa de aprendizaje conservadora para estabilidad\n",
    "BATCH_SIZE = 128        # Tamaño de lote balanceado para memoria y convergencia\n",
    "\n",
    "# Curriculum de entrenamiento: (archivo_dataset, num_épocas)\n",
    "CURRICULUM = [\n",
    "    (\"expert_data_1_fruit.pkl\", 25),   # Nivel básico: conceptos fundamentales\n",
    "    (\"expert_data_2_fruits.pkl\", 30),  # Nivel intermedio: decisiones múltiples\n",
    "    (\"expert_data_3_fruits.pkl\", 40),  # Nivel avanzado: planificación compleja\n",
    "    (\"expert_data_4_fruits.pkl\", 50)   # Nivel experto: refinamiento y maestría\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Script principal de entrenamiento por curriculum learning.\n",
    "    \n",
    "    Implementa el entrenamiento secuencial de la red neuronal utilizando\n",
    "    datasets de complejidad creciente. Cada lección del curriculum se\n",
    "    enfoca en un nivel específico de dificultad, permitiendo al modelo\n",
    "    aprender gradualmente conceptos más complejos.\n",
    "    \n",
    "    Proceso de entrenamiento:\n",
    "        1. Inicialización del modelo, optimizador y función de pérdida\n",
    "        2. Para cada lección del curriculum:\n",
    "           a. Cargar dataset correspondiente\n",
    "           b. Preparar DataLoader con batches mezclados\n",
    "           c. Entrenar por número específico de épocas\n",
    "           d. Monitorear pérdida promedio por época\n",
    "        3. Guardar modelo final entrenado\n",
    "    \n",
    "    Beneficios del curriculum learning:\n",
    "        - Convergencia más rápida y estable\n",
    "        - Mejor generalización a nuevos escenarios\n",
    "        - Reducción de overfitting a configuraciones específicas\n",
    "        - Aprendizaje progresivo de conceptos complejos\n",
    "    \"\"\"\n",
    "    # Inicializar componentes del entrenamiento\n",
    "    model = AgentNetwork()                              # Red convolucional para predicción de acciones\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # Optimizador Adam para gradiente adaptativo\n",
    "    criterion = nn.CrossEntropyLoss()                  # Función de pérdida para clasificación multiclase\n",
    "\n",
    "    # Ejecutar curriculum learning secuencial\n",
    "    for i, (dataset_file, num_epochs) in enumerate(CURRICULUM):\n",
    "        print(f\"\\n--- Iniciando Lección {i+1}/{len(CURRICULUM)}: {dataset_file} ---\")\n",
    "        \n",
    "        # Cargar dataset de demostración experta\n",
    "        with open(dataset_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Preparar datos para entrenamiento\n",
    "        # Separar estados (entrada CNN) y acciones (etiquetas de clasificación)\n",
    "        states = torch.FloatTensor(np.array([item[0] for item in data]))   # Estados visuales (3, 5, 5)\n",
    "        actions = torch.LongTensor(np.array([item[1] for item in data]))   # Índices de acciones (0-3)\n",
    "        \n",
    "        # Crear DataLoader para entrenamiento por lotes\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(states, actions),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True  # Mezclar datos para evitar patrones de orden\n",
    "        )\n",
    "\n",
    "        # Entrenar modelo en el dataset actual\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0  # Acumulador de pérdida para la época\n",
    "            \n",
    "            # Procesar todos los lotes del dataset\n",
    "            for batch_states, batch_actions in dataloader:\n",
    "                # Paso hacia adelante: predicción del modelo\n",
    "                optimizer.zero_grad()           # Limpiar gradientes acumulados\n",
    "                outputs = model(batch_states)   # Inferencia: estados → valores de acción\n",
    "                \n",
    "                # Calcular pérdida de clasificación\n",
    "                loss = criterion(outputs, batch_actions)  # CrossEntropy entre predicción y etiqueta\n",
    "                \n",
    "                # Retropropagación y optimización\n",
    "                loss.backward()    # Calcular gradientes por backpropagation\n",
    "                optimizer.step()   # Actualizar pesos de la red\n",
    "                \n",
    "                # Acumular pérdida para monitoreo\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            # Reporte de progreso por época\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            print(f\"  Época {epoch+1}/{num_epochs}, Pérdida: {avg_loss:.4f}\")\n",
    "\n",
    "    # Guardar modelo entrenado final\n",
    "    torch.save(model.state_dict(), \"imitacion_model.pth\")\n",
    "    print(\"\\n¡Entrenamiento por currículo completado! Modelo final guardado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8bdd4",
   "metadata": {},
   "source": [
    "#### environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment.py\n",
    "\"\"\"\n",
    "Entorno de cuadrícula para aprendizaje por imitación de agentes.\n",
    "\n",
    "Este módulo implementa un entorno de grid simplificado donde un agente\n",
    "debe navegar para recolectar frutas mientras evita venenos. Está diseñado\n",
    "específicamente para generar datos de demostración experta y entrenar\n",
    "agentes mediante aprendizaje por imitación.\n",
    "\n",
    "Clases:\n",
    "    GridEnvironment: Entorno de cuadrícula con estados visuales 3D\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class GridEnvironment:\n",
    "    \"\"\"\n",
    "    Entorno de cuadrícula para simulación de navegación y recolección.\n",
    "    \n",
    "    Implementa un mundo de grid 2D donde el agente debe recolectar todas\n",
    "    las frutas evitando venenos. El estado se representa como una imagen\n",
    "    de 3 canales (agente, frutas, venenos) ideal para redes convolucionales.\n",
    "    \n",
    "    Características:\n",
    "        - Grid cuadrado de tamaño configurable\n",
    "        - Estados visuales como tensores 3D\n",
    "        - Movimiento con límites del entorno\n",
    "        - Detección automática de colisiones\n",
    "        - Condiciones de terminación por victoria/derrota\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tamaño del grid (size x size)\n",
    "        agent_pos (np.ndarray): Posición actual del agente [x, y]\n",
    "        fruit_pos (list): Lista de posiciones de frutas [np.ndarray, ...]\n",
    "        poison_pos (list): Lista de posiciones de venenos [np.ndarray, ...]\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de grid con tamaño especificado.\n",
    "        \n",
    "        Args:\n",
    "            size (int): Dimensión del grid cuadrado (default: 5)\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, agent_pos=(0, 0), fruit_pos=[], poison_pos=[]):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno con configuración específica de elementos.\n",
    "        \n",
    "        Establece posiciones iniciales del agente, frutas y venenos.\n",
    "        Utilizado para crear escenarios específicos para generación\n",
    "        de datos de demostración o evaluación de políticas.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (tuple): Posición inicial del agente (x, y) (default: (0,0))\n",
    "            fruit_pos (list): Lista de posiciones de frutas [(x,y), ...] (default: [])\n",
    "            poison_pos (list): Lista de posiciones de venenos [(x,y), ...] (default: [])\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Estado inicial del entorno con forma (3, size, size)\n",
    "        \n",
    "        Note:\n",
    "            Las listas de posiciones se convierten a arrays numpy para\n",
    "            operaciones vectorizadas eficientes durante la simulación.\n",
    "        \"\"\"\n",
    "        self.agent_pos = np.array(agent_pos)\n",
    "        self.fruit_pos = [np.array(p) for p in fruit_pos]\n",
    "        self.poison_pos = [np.array(p) for p in poison_pos]\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera representación visual del estado actual como tensor 3D.\n",
    "        \n",
    "        Crea una imagen de 3 canales donde cada canal representa un tipo\n",
    "        de elemento del entorno. Esta representación es ideal para redes\n",
    "        convolucionales que procesan información espacial.\n",
    "        \n",
    "        Estructura de canales:\n",
    "            - Canal 0: Posición del agente (binario)\n",
    "            - Canal 1: Posiciones de frutas (binario)\n",
    "            - Canal 2: Posiciones de venenos (binario)\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Estado con forma (3, size, size) y dtype float32\n",
    "                       Valores: 1.0 para presencia de elemento, 0.0 para ausencia\n",
    "        \n",
    "        Example:\n",
    "            Para grid 3x3 con agente en (0,0) y fruta en (1,1):\n",
    "            Canal 0: [[1, 0, 0],    Canal 1: [[0, 0, 0],    Canal 2: [[0, 0, 0],\n",
    "                      [0, 0, 0],              [0, 1, 0],              [0, 0, 0],\n",
    "                      [0, 0, 0]]              [0, 0, 0]]              [0, 0, 0]]\n",
    "        \"\"\"\n",
    "        # Inicializar tensor de estado con ceros\n",
    "        state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posición del agente\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruit in self.fruit_pos:\n",
    "            state[1, fruit[0], fruit[1]] = 1.0\n",
    "        \n",
    "        # Canal 2: Posiciones de venenos\n",
    "        for poison in self.poison_pos:\n",
    "            state[2, poison[0], poison[1]] = 1.0\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción en el entorno y actualiza el estado.\n",
    "        \n",
    "        Procesa el movimiento del agente, maneja colisiones con límites,\n",
    "        detecta recolección de frutas y verifica condiciones de terminación.\n",
    "        Implementa la lógica core del entorno para simulación de episodios.\n",
    "        \n",
    "        Flujo de ejecución:\n",
    "            1. Actualizar posición según acción\n",
    "            2. Aplicar límites del entorno\n",
    "            3. Procesar recolección de frutas\n",
    "            4. Verificar colisiones con venenos\n",
    "            5. Evaluar condiciones de terminación\n",
    "        \n",
    "        Args:\n",
    "            action (int): Acción a ejecutar\n",
    "                         0 = Arriba (decrementar x)\n",
    "                         1 = Abajo (incrementar x)\n",
    "                         2 = Izquierda (decrementar y)\n",
    "                         3 = Derecha (incrementar y)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, reward, done)\n",
    "                - nuevo_estado (np.ndarray): Estado resultante (3, size, size)\n",
    "                - reward (float): Recompensa por la acción (-0.1 por defecto)\n",
    "                - done (bool): True si episodio terminó, False en caso contrario\n",
    "        \n",
    "        Note:\n",
    "            El reward no se utiliza en aprendizaje por imitación pero se\n",
    "            mantiene para compatibilidad con interfaces de RL estándar.\n",
    "        \"\"\"\n",
    "        # Actualizar posición del agente según la acción\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] -= 1  # Arriba\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] += 1  # Abajo\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] -= 1  # Izquierda\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] += 1  # Derecha\n",
    "        \n",
    "        # Aplicar límites del entorno (clipping)\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.size - 1)\n",
    "\n",
    "        # Inicializar variables de terminación\n",
    "        done = False\n",
    "        reward = -0.1  # Penalización por paso (no usado en imitación)\n",
    "\n",
    "        # Verificar recolección de frutas\n",
    "        for i, fruit in enumerate(self.fruit_pos):\n",
    "            if np.array_equal(self.agent_pos, fruit):\n",
    "                # Fruta recolectada: eliminar de la lista\n",
    "                self.fruit_pos.pop(i)\n",
    "                break\n",
    "        \n",
    "        # Verificar colisión con venenos (derrota)\n",
    "        if any(np.array_equal(self.agent_pos, p) for p in self.poison_pos):\n",
    "            done = True\n",
    "\n",
    "        # Verificar victoria (todas las frutas recolectadas)\n",
    "        if not self.fruit_pos:\n",
    "            done = True\n",
    "        \n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4dd8ba",
   "metadata": {},
   "source": [
    "#### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\"\"\"\n",
    "Demostración simple del agente entrenado por aprendizaje por imitación.\n",
    "\n",
    "Este módulo proporciona una interfaz minimalista para configurar escenarios\n",
    "y observar el comportamiento del agente entrenado. Utiliza formas geométricas\n",
    "simples para representar elementos, enfocándose en la funcionalidad core\n",
    "sin distracciones visuales complejas.\n",
    "\n",
    "Características:\n",
    "    - Configuración interactiva con mouse (clic izquierdo=fruta, clic derecho=veneno)\n",
    "    - Demostración automática del agente entrenado\n",
    "    - Representación visual simple con formas geométricas\n",
    "    - Ciclo continuo configuración → demostración → reset\n",
    "\n",
    "Constantes:\n",
    "    GRID_SIZE: Tamaño del entorno (5x5)\n",
    "    CELL_SIZE: Tamaño de cada celda en píxeles (100px)\n",
    "    WIDTH, HEIGHT: Dimensiones de la ventana (500x500)\n",
    "    COLOR_*: Esquema de colores para elementos visuales\n",
    "\n",
    "Funciones:\n",
    "    draw_elements: Renderizado de elementos con formas geométricas\n",
    "    main: Bucle principal con modos configuración y demostración\n",
    "\"\"\"\n",
    "import pygame\n",
    "import numpy as np\n",
    "from environment import GridEnvironment\n",
    "from agent import Agent\n",
    "\n",
    "# Configuración del entorno y ventana\n",
    "GRID_SIZE = 5\n",
    "CELL_SIZE = 100\n",
    "WIDTH, HEIGHT = GRID_SIZE * CELL_SIZE, GRID_SIZE * CELL_SIZE\n",
    "\n",
    "# Esquema de colores simple y claro\n",
    "COLOR_GRID = (200, 200, 200)    # Gris claro para grid\n",
    "COLOR_AGENT = (0, 0, 255)       # Azul para agente\n",
    "COLOR_FRUIT = (0, 255, 0)       # Verde para frutas\n",
    "COLOR_POISON = (255, 0, 0)      # Rojo para venenos\n",
    "\n",
    "def draw_elements(win, agent_pos, fruits, poisons):\n",
    "    \"\"\"\n",
    "    Renderiza todos los elementos del entorno usando formas geométricas simples.\n",
    "    \n",
    "    Dibuja el grid de navegación y representa cada elemento del entorno\n",
    "    con formas distintivas: rectángulos para agente, círculos para frutas\n",
    "    y cuadrados pequeños para venenos. Diseño minimalista para claridad.\n",
    "    \n",
    "    Representación visual:\n",
    "        - Agente: Rectángulo azul de celda completa\n",
    "        - Frutas: Círculos verdes centrados (1/3 del tamaño de celda)\n",
    "        - Venenos: Cuadrados rojos con margen (80% del tamaño de celda)\n",
    "        - Grid: Líneas grises para delimitación de celdas\n",
    "    \n",
    "    Args:\n",
    "        win (pygame.Surface): Superficie donde renderizar\n",
    "        agent_pos (np.ndarray): Posición del agente [fila, columna]\n",
    "        fruits (list): Lista de posiciones de frutas [(fila, col), ...]\n",
    "        poisons (list): Lista de posiciones de venenos [(fila, col), ...]\n",
    "    \n",
    "    Note:\n",
    "        Convierte coordenadas (fila, columna) a píxeles (x, y) para Pygame.\n",
    "        Agente en posición (-1, -1) no se dibuja (modo configuración).\n",
    "    \"\"\"\n",
    "    # Limpiar pantalla con fondo negro\n",
    "    win.fill((0,0,0))\n",
    "    \n",
    "    # Dibujar grid de navegación\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        pygame.draw.line(win, COLOR_GRID, (x, 0), (x, HEIGHT))\n",
    "    for y in range(0, HEIGHT, CELL_SIZE):\n",
    "        pygame.draw.line(win, COLOR_GRID, (0, y), (WIDTH, y))\n",
    "    # Dibujar agente (solo si posición válida)\n",
    "    if agent_pos[0] >= 0 and agent_pos[1] >= 0:\n",
    "        pygame.draw.rect(win, COLOR_AGENT, \n",
    "                        (agent_pos[1] * CELL_SIZE, agent_pos[0] * CELL_SIZE, \n",
    "                         CELL_SIZE, CELL_SIZE))\n",
    "    \n",
    "    # Dibujar frutas como círculos verdes\n",
    "    for f in fruits:\n",
    "        center_x = f[1] * CELL_SIZE + CELL_SIZE//2\n",
    "        center_y = f[0] * CELL_SIZE + CELL_SIZE//2\n",
    "        radius = CELL_SIZE//3\n",
    "        pygame.draw.circle(win, COLOR_FRUIT, (center_x, center_y), radius)\n",
    "    \n",
    "    # Dibujar venenos como cuadrados rojos con margen\n",
    "    for p in poisons:\n",
    "        margin = 20  # Margen de 20px para distinguir de agente\n",
    "        rect_x = p[1] * CELL_SIZE + margin\n",
    "        rect_y = p[0] * CELL_SIZE + margin\n",
    "        rect_size = CELL_SIZE - 2 * margin\n",
    "        pygame.draw.rect(win, COLOR_POISON, (rect_x, rect_y, rect_size, rect_size))\n",
    "    \n",
    "    # Actualizar display para mostrar cambios\n",
    "    pygame.display.update()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal de la demostración simple del agente por imitación.\n",
    "    \n",
    "    Implementa un ciclo de dos modos: configuración interactiva donde el usuario\n",
    "    coloca elementos con el mouse, y demostración automática donde el agente\n",
    "    entrenado navega el escenario. Diseñado para evaluación rápida y directa\n",
    "    del rendimiento del modelo.\n",
    "    \n",
    "    Flujo de la aplicación:\n",
    "        1. Modo \"setup\": Usuario configura escenario con mouse\n",
    "           - Clic izquierdo: Colocar fruta\n",
    "           - Clic derecho: Colocar veneno\n",
    "           - Espacio: Iniciar demostración\n",
    "        \n",
    "        2. Modo \"run\": Agente navega automáticamente\n",
    "           - Inferencia con modelo entrenado\n",
    "           - Movimiento automático cada 300ms\n",
    "           - Terminación por victoria/derrota\n",
    "           - Reset automático a configuración\n",
    "    \n",
    "    Controles:\n",
    "        - Clic izquierdo: Agregar fruta en posición del mouse\n",
    "        - Clic derecho: Agregar veneno en posición del mouse\n",
    "        - Espacio: Iniciar demostración (solo si hay frutas)\n",
    "        - Automático: Reset a configuración al terminar episodio\n",
    "    \n",
    "    Note:\n",
    "        Requiere modelo entrenado \"imitacion_model.pth\" en directorio actual.\n",
    "        El agente siempre inicia en posición (0,0) del grid.\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y configurar ventana\n",
    "    pygame.init()\n",
    "    win = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "    pygame.display.set_caption(\"Agente Come-Frutas (IA)\")\n",
    "    \n",
    "    # Inicializar entorno y agente con modelo pre-entrenado\n",
    "    env = GridEnvironment(size=GRID_SIZE)\n",
    "    agent = Agent()\n",
    "    agent.load_model(\"imitacion_model.pth\")\n",
    "\n",
    "    # Variables de estado de la aplicación\n",
    "    fruits, poisons = [], []  # Listas de posiciones de elementos\n",
    "    mode = \"setup\"           # Modo inicial: configuración\n",
    "    run = True              # Control del bucle principal\n",
    "    # Bucle principal de la aplicación\n",
    "    while run:\n",
    "        # Procesar eventos de entrada\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "\n",
    "            # Lógica específica del modo configuración\n",
    "            if mode == \"setup\":\n",
    "                if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                    # Convertir posición del mouse a coordenadas de grid\n",
    "                    pos = pygame.mouse.get_pos()\n",
    "                    col, row = pos[0] // CELL_SIZE, pos[1] // CELL_SIZE\n",
    "                    \n",
    "                    # Clic izquierdo: Agregar fruta (si no existe)\n",
    "                    if event.button == 1 and (row, col) not in fruits:\n",
    "                        fruits.append((row, col))\n",
    "                    # Clic derecho: Agregar veneno (si no existe)\n",
    "                    elif event.button == 3 and (row, col) not in poisons:\n",
    "                        poisons.append((row, col))\n",
    "                \n",
    "                # Espacio: Iniciar demostración si hay frutas configuradas\n",
    "                if event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE:\n",
    "                    if fruits:  # Solo si hay al menos una fruta\n",
    "                        mode = \"run\"\n",
    "                        # Inicializar entorno con configuración actual\n",
    "                        env.reset(agent_pos=(0,0), fruit_pos=fruits, poison_pos=poisons)\n",
    "\n",
    "        # Renderizado según el modo actual\n",
    "        if mode == \"setup\":\n",
    "            # Modo configuración: mostrar elementos sin agente\n",
    "            draw_elements(win, np.array([-1,-1]), fruits, poisons)\n",
    "            \n",
    "        elif mode == \"run\":\n",
    "            # Modo demostración: agente automático\n",
    "            \n",
    "            # Obtener estado actual y generar acción\n",
    "            state = env.get_state()\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Ejecutar acción y verificar terminación\n",
    "            _, _, done = env.step(action)\n",
    "            \n",
    "            # Renderizar estado actualizado\n",
    "            draw_elements(win, env.agent_pos, env.fruit_pos, env.poison_pos)\n",
    "            \n",
    "            # Procesar terminación del episodio\n",
    "            if done:\n",
    "                print(\"¡Simulación terminada!\")\n",
    "                pygame.time.delay(2000)  # Pausa para observar resultado final\n",
    "                # Reset automático a modo configuración\n",
    "                fruits, poisons = [], []\n",
    "                mode = \"setup\"\n",
    "            \n",
    "            # Controlar velocidad de demostración\n",
    "            pygame.time.delay(300)  # 300ms entre acciones para visibilidad\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a9c28",
   "metadata": {},
   "source": [
    "### Jugador humano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee67918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modo de juego humano con controles aleatorios - Come Frutas.\n",
    "\n",
    "Este módulo implementa una versión jugable del entorno donde un humano puede\n",
    "controlar el agente directamente. La característica única es que los controles\n",
    "de movimiento se asignan aleatoriamente cada vez que se inicia una partida,\n",
    "añadiendo un elemento de desafío y adaptabilidad.\n",
    "\n",
    "Características principales:\n",
    "- Modo Setup: Configuración manual del escenario\n",
    "- Modo Humano: Control directo del agente por el jugador\n",
    "- Controles aleatorios: Mapeo aleatorio de teclas a movimientos\n",
    "- Interfaz intuitiva: Gráficos y feedback visual\n",
    "- Desafío adaptativo: Cada partida requiere aprender nuevos controles\n",
    "\n",
    "Propósito educativo:\n",
    "- Comparar rendimiento humano vs. IA\n",
    "- Experimentar la dificultad de adaptación a controles cambiantes\n",
    "- Entender la importancia de la consistencia en interfaces\n",
    "- Apreciar la flexibilidad del aprendizaje humano\n",
    "\n",
    "Autor: [Tu nombre]\n",
    "Fecha: Agosto 2025\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "# CONFIGURACIÓN DEL ENTORNO VISUAL\n",
    "\"\"\"\n",
    "Parámetros visuales y dimensiones de la interfaz de juego.\n",
    "Utiliza celdas más grandes (120px) para mejor visibilidad durante el juego manual.\n",
    "\"\"\"\n",
    "GRID_WIDTH = 5              # Ancho de la cuadrícula en celdas\n",
    "GRID_HEIGHT = 5             # Alto de la cuadrícula en celdas\n",
    "CELL_SIZE = 120             # Tamaño de cada celda en píxeles (mayor para juego manual)\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la ventana (600px)\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto del área de juego (600px)\n",
    "\n",
    "# PALETA DE COLORES CONSISTENTE\n",
    "\"\"\"\n",
    "Esquema de colores oscuro profesional, consistente con otros módulos del proyecto.\n",
    "\"\"\"\n",
    "COLOR_FONDO = (25, 25, 25)      # Gris muy oscuro para el fondo\n",
    "COLOR_LINEAS = (40, 40, 40)     # Gris oscuro para líneas de cuadrícula\n",
    "COLOR_CURSOR = (255, 255, 0)    # Amarillo brillante para cursor de selección\n",
    "COLOR_TEXTO = (230, 230, 230)   # Gris claro para texto legible\n",
    "\n",
    "# SISTEMA DE CONTROLES ALEATORIOS\n",
    "\"\"\"\n",
    "Genera un conjunto de teclas válidas para asignación aleatoria de controles.\n",
    "Se evitan teclas especiales para prevenir conflictos con funciones del sistema.\n",
    "\"\"\"\n",
    "TECLAS_VALIDAS = [getattr(pygame, f\"K_{c}\") for c in string.ascii_lowercase + string.digits]\n",
    "\n",
    "class EntornoHumano:\n",
    "    \"\"\"\n",
    "    Entorno de juego optimizado para control humano directo.\n",
    "    \n",
    "    Esta clase maneja la lógica del juego cuando un humano controla el agente,\n",
    "    incluyendo movimiento, colisiones, recolección de objetos y condiciones\n",
    "    de victoria/derrota. Se enfoca en proporcionar feedback inmediato y\n",
    "    una experiencia de juego fluida.\n",
    "    \n",
    "    Diferencias con entornos de IA:\n",
    "    - Feedback inmediato con mensajes en consola\n",
    "    - Lógica de juego simplificada (sin recompensas numéricas)\n",
    "    - Terminación inmediata en victoria/derrota\n",
    "    - Controles responsivos para jugabilidad humana\n",
    "    \n",
    "    Attributes:\n",
    "        agente_pos (tuple): Posición actual del agente (x, y)\n",
    "        frutas (set): Conjunto de posiciones con frutas\n",
    "        venenos (set): Conjunto de posiciones con venenos\n",
    "        paredes (set): Conjunto de posiciones con paredes/obstáculos\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuración vacía.\n",
    "        \n",
    "        El agente comienza en la esquina superior izquierda (0,0) y todos\n",
    "        los conjuntos de elementos están vacíos, permitiendo configuración manual.\n",
    "        \"\"\"\n",
    "        self.agente_pos = (0, 0)    # Posición inicial del agente\n",
    "        self.frutas = set()         # Conjunto de posiciones de frutas\n",
    "        self.venenos = set()        # Conjunto de posiciones de venenos\n",
    "        self.paredes = set()        # Conjunto de posiciones de paredes\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resetea la posición del agente al inicio del juego.\n",
    "        \n",
    "        Coloca al agente en la posición inicial (0,0) sin modificar\n",
    "        la configuración del escenario. Utilizado al comenzar una nueva partida.\n",
    "        \"\"\"\n",
    "        self.agente_pos = (0, 0)\n",
    "\n",
    "    def limpiar(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno.\n",
    "        \n",
    "        Limpia frutas, venenos y paredes del escenario, dejando una\n",
    "        cuadrícula vacía para configuración desde cero.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción del jugador humano en el entorno.\n",
    "        \n",
    "        Procesa el movimiento del agente, verifica colisiones y maneja\n",
    "        las interacciones con elementos del entorno. Proporciona feedback\n",
    "        inmediato al jugador mediante mensajes en consola.\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Dirección de movimiento:\n",
    "                         0 = Arriba (decrementar y)\n",
    "                         1 = Abajo (incrementar y)\n",
    "                         2 = Izquierda (decrementar x)\n",
    "                         3 = Derecha (incrementar x)\n",
    "        \n",
    "        Returns:\n",
    "            bool: True si el juego terminó (victoria o derrota), False si continúa\n",
    "        \"\"\"\n",
    "        # Calcular nueva posición basada en la acción\n",
    "        x, y = self.agente_pos\n",
    "        if accion == 0:     # Arriba\n",
    "            y -= 1\n",
    "        elif accion == 1:   # Abajo\n",
    "            y += 1\n",
    "        elif accion == 2:   # Izquierda\n",
    "            x -= 1\n",
    "        elif accion == 3:   # Derecha\n",
    "            x += 1\n",
    "\n",
    "        # Verificar colisiones: límites del tablero o paredes\n",
    "        if x < 0 or x >= GRID_WIDTH or y < 0 or y >= GRID_HEIGHT or (x, y) in self.paredes:\n",
    "            # Movimiento inválido: no actualizar posición\n",
    "            return False\n",
    "\n",
    "        # Movimiento válido: actualizar posición del agente\n",
    "        self.agente_pos = (x, y)\n",
    "        \n",
    "        # Verificar interacciones con elementos del entorno\n",
    "        if self.agente_pos in self.frutas:\n",
    "            # Fruta recogida: eliminar del conjunto\n",
    "            self.frutas.remove(self.agente_pos)\n",
    "            \n",
    "            # Verificar condición de victoria\n",
    "            if not self.frutas:\n",
    "                print(\"\\n✨ ¡Ganaste! Recolectaste todas las frutas.\\n\")\n",
    "                return True  # Juego terminado con éxito\n",
    "                \n",
    "        elif self.agente_pos in self.venenos:\n",
    "            # Veneno tocado: derrota inmediata\n",
    "            print(\"\\n☠️ ¡Oh no! Tocaste un veneno.\\n\")\n",
    "            return True  # Juego terminado con fallo\n",
    "            \n",
    "        # Continuar juego\n",
    "        return False\n",
    "\n",
    "    def dibujar(self, pantalla, modo, cursor_pos, img_fruta, img_veneno, img_pared, img_agente, _):\n",
    "        \"\"\"\n",
    "        Renderiza el estado completo del entorno con interfaz interactiva.\n",
    "        \n",
    "        Dibuja todos los elementos visuales del juego incluyendo grid, objetos\n",
    "        del entorno y cursor de selección. Proporciona feedback visual para\n",
    "        la interacción del jugador en diferentes modos (colocación/juego).\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde renderizar\n",
    "            modo (str): Modo actual de la interfaz ('frutas', 'venenos', 'paredes', 'jugar')\n",
    "            cursor_pos (tuple): Posición (x,y) del cursor en coordenadas de grid\n",
    "            img_fruta (pygame.Surface): Sprite de las frutas\n",
    "            img_veneno (pygame.Surface): Sprite de los venenos\n",
    "            img_pared (pygame.Surface): Sprite de las paredes\n",
    "            img_agente (pygame.Surface): Sprite del agente\n",
    "            _ : Parámetro no utilizado (compatibilidad de interfaz)\n",
    "        \n",
    "        Note:\n",
    "            Renderiza en orden específico: fondo, grid, objetos, agente, cursor.\n",
    "            El cursor cambia de color según el modo de colocación activo.\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "\n",
    "        # Dibujar líneas del grid para guía visual\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibujar elementos del entorno\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0]*CELL_SIZE, fruta[1]*CELL_SIZE))\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0]*CELL_SIZE, veneno[1]*CELL_SIZE))\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0]*CELL_SIZE, pared[1]*CELL_SIZE))\n",
    "\n",
    "        # Dibujar agente (jugador) - siempre visible en primer plano\n",
    "        pantalla.blit(img_agente, (self.agente_pos[0]*CELL_SIZE, self.agente_pos[1]*CELL_SIZE))\n",
    "\n",
    "        # Dibujar cursor de selección en modo configuración\n",
    "        if modo == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(cursor_pos[0]*CELL_SIZE, cursor_pos[1]*CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar información de interfaz\n",
    "        font = pygame.font.Font(None, 30)\n",
    "        pantalla.blit(font.render(f\"Modo: {modo}\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(font.render(\"F: Fruta, V: Veneno, W: Pared, C: Limpiar, H: Jugar\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(font.render(\"Descubre los controles ocultos usando letras/números\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "def cargar_imagen(nombre, fallback_color):\n",
    "    \"\"\"\n",
    "    Carga una imagen desde archivo con sistema de respaldo.\n",
    "    \n",
    "    Intenta cargar una imagen sprite desde el directorio actual.\n",
    "    Si la carga falla, crea una superficie de color sólido como respaldo.\n",
    "    Escala automáticamente al tamaño de celda definido.\n",
    "    \n",
    "    Args:\n",
    "        nombre (str): Nombre del archivo de imagen a cargar\n",
    "        fallback_color (tuple): Color RGB (r,g,b) para superficie de respaldo\n",
    "    \n",
    "    Returns:\n",
    "        pygame.Surface: Superficie cargada y escalada, o superficie de color\n",
    "                       si la carga falló\n",
    "    \n",
    "    Note:\n",
    "        Todas las imágenes se escalan a CELL_SIZE x CELL_SIZE píxeles.\n",
    "        Utiliza convert_alpha() para optimizar el renderizado con transparencia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir ruta completa al archivo de imagen\n",
    "        ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "        # Cargar imagen con soporte de transparencia\n",
    "        img = pygame.image.load(ruta).convert_alpha()\n",
    "        # Escalar a tamaño de celda estándar\n",
    "        return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "    except:\n",
    "        # Crear superficie de respaldo con color sólido si falla la carga\n",
    "        s = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "        s.fill(fallback_color)\n",
    "        return s\n",
    "\n",
    "def generar_controles_aleatorios():\n",
    "    \"\"\"\n",
    "    Genera un mapeo aleatorio de teclas para controles de movimiento.\n",
    "    \n",
    "    Crea una asignación aleatoria entre teclas del teclado y direcciones\n",
    "    de movimiento para añadir un elemento de desafío y descubrimiento\n",
    "    al juego. Los jugadores deben encontrar qué teclas controlan cada dirección.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapeo de códigos de tecla pygame a acciones de movimiento:\n",
    "              {tecla_pygame: accion_int}\n",
    "              donde accion_int es 0=Arriba, 1=Abajo, 2=Izquierda, 3=Derecha\n",
    "    \n",
    "    Note:\n",
    "        Utiliza teclas alfanuméricas (A-Z, 0-9) para máxima compatibilidad.\n",
    "        Garantiza que cada dirección tenga exactamente una tecla asignada.\n",
    "    \"\"\"\n",
    "    # Seleccionar 4 teclas aleatorias del conjunto disponible\n",
    "    teclas = random.sample(TECLAS_VALIDAS, 4)\n",
    "    # Crear lista de acciones de movimiento\n",
    "    acciones = [0, 1, 2, 3]  # Arriba, abajo, izquierda, derecha\n",
    "    # Mezclar aleatoriamente las acciones\n",
    "    random.shuffle(acciones)\n",
    "    # Crear diccionario de mapeo tecla->acción\n",
    "    return dict(zip(teclas, acciones))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal del juego en modo humano.\n",
    "    \n",
    "    Inicializa Pygame, configura la ventana de juego y ejecuta el bucle\n",
    "    principal que maneja dos modos: configuración del entorno y juego\n",
    "    con controles aleatorios. Proporciona una experiencia interactiva\n",
    "    donde el jugador puede diseñar niveles y luego jugarlos.\n",
    "    \n",
    "    Flujo del juego:\n",
    "        1. Modo SETUP: Colocar frutas, venenos y paredes con el mouse\n",
    "        2. Modo JUGAR: Controlar agente con teclas aleatorias descubiertas\n",
    "        3. Victoria: Recolectar todas las frutas\n",
    "        4. Derrota: Tocar veneno\n",
    "    \n",
    "    Controles SETUP:\n",
    "        - Mouse: Mover cursor\n",
    "        - F: Colocar fruta\n",
    "        - V: Colocar veneno  \n",
    "        - W: Colocar pared\n",
    "        - C: Limpiar todo\n",
    "        - H: Iniciar juego\n",
    "    \n",
    "    Controles JUGAR:\n",
    "        - Teclas aleatorias para movimiento (descubrir experimentando)\n",
    "        - ESC: Volver a configuración\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y configurar ventana\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 100))\n",
    "    pygame.display.set_caption(\"Modo Humano Aleatorio - Come Frutas\")\n",
    "\n",
    "    # Inicializar entorno y variables de estado\n",
    "    entorno = EntornoHumano()\n",
    "    cursor_pos = [0, 0]\n",
    "    modo = \"SETUP\"  # Modo inicial: configuración del entorno\n",
    "    mapeo_controles = {}  # Mapeo de teclas aleatorias (generado al jugar)\n",
    "\n",
    "    # Cargar sprites con colores de respaldo\n",
    "    img_fruta = cargar_imagen(\"fruta.png\", (40, 200, 40))\n",
    "    img_veneno = cargar_imagen(\"veneno.png\", (255, 50, 50))\n",
    "    img_pared = cargar_imagen(\"pared.jpg\", (80, 80, 80))\n",
    "    img_agente = cargar_imagen(\"agente.png\", (60, 100, 255))\n",
    "\n",
    "    # Variables de control del juego\n",
    "    reloj = pygame.time.Clock()\n",
    "    corriendo = True\n",
    "\n",
    "    # Bucle principal del juego\n",
    "    while corriendo:\n",
    "        # Procesar eventos de entrada\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "            elif evento.type == pygame.KEYDOWN:\n",
    "                if evento.key == pygame.K_s:\n",
    "                    modo = \"SETUP\"\n",
    "                elif evento.key == pygame.K_h:\n",
    "                    modo = \"HUMANO\"\n",
    "                    entorno.reset()\n",
    "                    mapeo_controles = generar_controles_aleatorios()\n",
    "\n",
    "                if modo == \"SETUP\":\n",
    "                    if evento.key == pygame.K_UP: cursor_pos[1] = max(0, cursor_pos[1]-1)\n",
    "                    elif evento.key == pygame.K_DOWN: cursor_pos[1] = min(GRID_HEIGHT-1, cursor_pos[1]+1)\n",
    "                    elif evento.key == pygame.K_LEFT: cursor_pos[0] = max(0, cursor_pos[0]-1)\n",
    "                    elif evento.key == pygame.K_RIGHT: cursor_pos[0] = min(GRID_WIDTH-1, cursor_pos[0]+1)\n",
    "                    # Colocación de elementos con teclas específicas\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    if evento.key == pygame.K_f: \n",
    "                        # F: Colocar/quitar fruta (toggle)\n",
    "                        entorno.frutas.symmetric_difference_update({pos})\n",
    "                        entorno.venenos.discard(pos)\n",
    "                        entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_v: \n",
    "                        # V: Colocar/quitar veneno (toggle)\n",
    "                        entorno.venenos.symmetric_difference_update({pos})\n",
    "                        entorno.frutas.discard(pos)\n",
    "                        entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_w: \n",
    "                        # W: Colocar/quitar pared (toggle)\n",
    "                        entorno.paredes.symmetric_difference_update({pos})\n",
    "                        entorno.frutas.discard(pos)\n",
    "                        entorno.venenos.discard(pos)\n",
    "                    elif evento.key == pygame.K_c: \n",
    "                        # C: Limpiar todo el entorno\n",
    "                        entorno.limpiar()\n",
    "\n",
    "                # Controles específicos del modo HUMANO\n",
    "                elif modo == \"HUMANO\":\n",
    "                    if evento.key in mapeo_controles:\n",
    "                        # Ejecutar acción de movimiento con tecla aleatoria\n",
    "                        accion = mapeo_controles[evento.key]\n",
    "                        terminado = entorno.step(accion)\n",
    "                        if terminado:\n",
    "                            # Volver a configuración al terminar el juego\n",
    "                            modo = \"SETUP\"\n",
    "\n",
    "        # Renderizar estado actual del juego\n",
    "        entorno.dibujar(pantalla, modo, cursor_pos, img_fruta, img_veneno, img_pared, img_agente, mapeo_controles)\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(30)\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
