{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803cfcb5",
   "metadata": {},
   "source": [
    "# Taller sobre clustering aglomerativo y DBSCAN\n",
    "# Agente Come Frutas\n",
    "\n",
    "## Integrantes: Ayala Ivonne, Cumbal Mateo, Garcés Boris, Morales David, Pereira Alicia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512b684",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### 1. Introducción\n",
    "El presente informe detalla el proceso de diseño, implementación y experimentación para el desarrollo de un agente autónomo de Inteligencia Artificial en el marco del proyecto \"Agente Come-Frutas\". El desafío se centra en un problema clásico de toma de decisiones secuenciales en un entorno dinámico y con riesgos, sirviendo como un caso de estudio práctico para la aplicación de técnicas avanzadas de Machine Learning.\n",
    "\n",
    "El problema consiste en un entorno de rejilla de 5x5 en el que un agente debe aprender a navegar de manera eficiente. El objetivo principal es maximizar una puntuación recolectando \"frutas\" (que otorgan recompensas positivas) y evitando \"venenos\" (que imponen castigos negativos). La meta final es desarrollar una política de comportamiento óptima que permita al agente limpiar el tablero de todas las frutas, garantizando su supervivencia al esquivar todos los venenos presentes.\n",
    "\n",
    "Para alcanzar este objetivo, el proyecto transitó por un riguroso proceso de experimentación, explorando múltiples paradigmas de la Inteligencia Artificial. Se inició con un enfoque en el Aprendizaje por Refuerzo (Reinforcement Learning), implementando y depurando algoritmos de vanguardia como Deep Q-Networks (DQN) y su variante mejorada, Double DQN (DDQN).\n",
    "\n",
    "Frente a los desafíos clásicos de convergencia y estabilidad inherentes a RL, la investigación se expandió para incluir otras estrategias. Se exploraron los Algoritmos Genéticos, un enfoque basado en principios de evolución, y el Aprendizaje por Imitación, una potente técnica de aprendizaje supervisado que requirió el desarrollo de un \"oráculo\" experto basado en el algoritmo de búsqueda A*.\n",
    "\n",
    "A continuación, se presenta el código documentado de la implementación final, reflejando la culminación de este profundo y multifacético proceso de desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200de7cd",
   "metadata": {},
   "source": [
    "### 2. Desarrollo y Experimentación\n",
    "El proyecto se desarrolló de forma iterativa, comenzando con un algoritmo fundamental y avanzando hacia técnicas más complejas para superar los desafíos encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9755b",
   "metadata": {},
   "source": [
    "#### 2.1. Punto de Partida: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6cbc7",
   "metadata": {},
   "source": [
    "El desarrollo del agente autónomo inició con la implementación del algoritmo clásico de Aprendizaje por Refuerzo: Q-Learning. Este enfoque permitió construir una base conceptual sólida para comprender cómo un agente puede aprender a actuar en un entorno dinámico sin conocimiento previo del mismo. A través de ensayo y error, el agente aprende una política óptima que maximiza su recompensa acumulada al recolectar frutas y evitar venenos.\n",
    "\n",
    "**Entorno y Representación de Estados** \n",
    "\n",
    "El entorno fue modelado como una cuadrícula de 5×5, donde cada celda puede contener una fruta (recompensa positiva), un veneno (castigo fuerte) o estar vacía. El estado del agente se representa únicamente por su posición (x,y) en la rejilla, lo que resulta en un espacio de estados de tamaño 25 (5 filas × 5 columnas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f858c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.q_table = np.zeros((num_estados[0], num_estados[1], num_acciones))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f1282",
   "metadata": {},
   "source": [
    "Cada entrada de la tabla Q[s][a] almacena el valor esperado de realizar la acción a en el estado s. El agente puede realizar cuatro acciones posibles: arriba, abajo, izquierda y derecha.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164bd6b",
   "metadata": {},
   "source": [
    "**Parámetros del Aprendizaje**\n",
    "\n",
    "Se definieron los siguientes hiperparámetros para controlar el proceso de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARÁMETROS DEL APRENDIZAJE POR REFUERZO (Q-LEARNING) ---\n",
    "# Estos son los \"hiperparámetros\" que controlan cómo aprende el agente.\n",
    "RECOMPENSA_FRUTA = 100         # Puntuación alta por encontrar una fruta.\n",
    "CASTIGO_VENENO = -100           # Castigo fuerte por tocar un veneno.\n",
    "RECOMPENSA_MOVIMIENTO = -0.1    # Castigo por movimiento para la eficiencia.\n",
    "ALPHA = 0.1                     # Tasa de aprendizaje.\n",
    "GAMMA = 0.9                     # Factor de descuento.\n",
    "EPSILON = 1.0                   # Tasa de exploración inicial.\n",
    "EPSILON_DECAY = 0.9995          # Factor de decaimiento de epsilon.\n",
    "MIN_EPSILON = 0.01              # Mínima tasa de exploración.\n",
    "NUM_EPISODIOS_ENTRENAMIENTO = 20000 #Número de partidas que jugará el agente para aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb7348",
   "metadata": {},
   "source": [
    "- ALPHA define cuánto se actualiza la tabla Q con la nueva información.\n",
    "\n",
    "- GAMMA determina la importancia del valor futuro frente a la recompensa inmediata.\n",
    "\n",
    "- EPSILON regula el equilibrio entre exploración y explotación, y su decaimiento permite al agente ser curioso al inicio y luego explotar lo aprendido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfa4ed",
   "metadata": {},
   "source": [
    "**Política de Acción: Epsilon-Greedy**\n",
    "\n",
    "El agente selecciona sus movimientos mediante una política epsilon-greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cae4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_accion(self, estado):\n",
    "    if random.uniform(0, 1) < self.epsilon:\n",
    "        return random.randint(0, self.num_acciones - 1)  # Exploración\n",
    "    else:\n",
    "        return np.argmax(self.q_table[estado])          # Explotación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db18b1c",
   "metadata": {},
   "source": [
    "Esta estrategia permite al agente explorar nuevas acciones al inicio del entrenamiento, pero gradualmente se convierte en un experto que toma decisiones óptimas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadef1ad",
   "metadata": {},
   "source": [
    "**Aprendizaje con la Ecuación de Bellman**\n",
    "\n",
    "La función de actualización aplica la ecuación de Bellman, actualizando el valor de la acción ejecutada según la recompensa recibida y el mejor valor futuro estimado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3da2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_q_table(self, estado, accion, recompensa, nuevo_estado):\n",
    "    valor_antiguo = self.q_table[estado][accion]\n",
    "    valor_futuro_maximo = np.max(self.q_table[nuevo_estado])\n",
    "    nuevo_q = valor_antiguo + ALPHA * (\n",
    "        recompensa + GAMMA * valor_futuro_maximo - valor_antiguo\n",
    "    )\n",
    "    self.q_table[estado][accion] = nuevo_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a32c3",
   "metadata": {},
   "source": [
    "Este mecanismo es el núcleo del aprendizaje en Q-Learning: el agente se adapta en función de la retroalimentación recibida del entorno.\n",
    "\n",
    "**Entrenamiento del Agente**\n",
    "\n",
    "El agente se entrena a lo largo de 20,000 episodios, en los que explora y aprende sobre el entorno. Cada episodio se reinicia desde la posición inicial y termina cuando el agente recolecta todas las frutas o toca un veneno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e945fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(NUM_EPISODIOS_ENTRENAMIENTO):\n",
    "    entorno.frutas = frutas_iniciales.copy()\n",
    "    entorno.venenos = venenos_iniciales.copy()\n",
    "    estado = entorno.reset_a_configuracion_inicial()\n",
    "    terminado = False\n",
    "    while not terminado:\n",
    "        accion = agente.elegir_accion(estado)\n",
    "        nuevo_estado, recompensa, terminado = entorno.step(accion, \"TRAINING\")\n",
    "        agente.actualizar_q_table(estado, accion, recompensa, nuevo_estado)\n",
    "        estado = nuevo_estado\n",
    "    agente.decaimiento_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c34be2",
   "metadata": {},
   "source": [
    "**Visualización y Modo Juego**\n",
    "\n",
    "Una vez entrenado, el agente puede ser probado en el modo PLAYING, donde actúa usando únicamente explotación (ϵ=0). El entorno fue diseñado con pygame, incluyendo sprites para representar frutas, venenos y paredes, lo cual facilita visualizar las decisiones del agente en tiempo real.\n",
    "\n",
    "**Limitaciones Observadas**\n",
    "\n",
    "Aunque este enfoque demostró ser funcional en escenarios simples, presentó problemas de convergencia y eficiencia cuando el número de frutas y venenos aumentaba. La tabla Q escala mal en entornos con más dimensiones, lo que motivó la transición hacia métodos más avanzados como Deep Q-Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb0971",
   "metadata": {},
   "source": [
    "#### 2.2 DQN con CNN (Deep Q-Network con Red Convolucional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a19768",
   "metadata": {},
   "source": [
    "**Motivación y Limitaciones del Q-Learning**\n",
    "\n",
    "Tras observar las limitaciones del Q-Learning clásico —en particular su incapacidad para escalar a entornos complejos o generalizar eficientemente— se adoptó el algoritmo Deep Q-Network (DQN) como siguiente paso evolutivo. Este enfoque reemplaza la tabla Q discreta por una red neuronal convolucional (CNN), capaz de aproximar los valores Q mediante aprendizaje profundo a partir de representaciones visuales del entorno.\n",
    "\n",
    "**Representación del Estado como Imagen Multicanal**\n",
    "\n",
    "Cada estado del entorno fue transformado en un tensor tridimensional de forma (3, 5, 5), donde:\n",
    "\n",
    "- Canal 0 indica la posición del agente.\n",
    "\n",
    "- Canal 1 marca las frutas.\n",
    "\n",
    "- Canal 2 representa los venenos.\n",
    "\n",
    "Esta representación permite que la CNN procese el entorno como una \"imagen\", reconociendo patrones espaciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment.py\n",
    "state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "state[0, agent_pos[0], agent_pos[1]] = 1.0  # Canal del agente\n",
    "state[1, fruta[0], fruta[1]] = 1.0         # Canal de frutas\n",
    "state[2, veneno[0], veneno[1]] = 1.0       # Canal de venenos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebeb96d",
   "metadata": {},
   "source": [
    "**Arquitectura de la Red Neuronal**\n",
    "\n",
    "La red CNN_DQN, definida en agent.py, sigue una arquitectura sencilla pero eficaz:\n",
    "\n",
    "- 2 capas convolucionales para extraer características espaciales.\n",
    "\n",
    "- 2 capas densas para calcular el valor Q de cada acción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura de la red neuronal que actúa como el \"cerebro\" visual del agente.\n",
    "class CNN_DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs): [cite: 1279]\n",
    "        super(CNN_DQN, self).__init__()\n",
    "        # Capas convolucionales para \"ver\" el tabler\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Cálculo del tamaño para la capa lineal\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        # Capas lineales para \"decidir\" el valor de cada acción\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff9073",
   "metadata": {},
   "source": [
    "Esta red recibe como entrada el estado en forma de tensor y devuelve un vector de valores Q (uno por acción)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7d6da",
   "metadata": {},
   "source": [
    "**Componentes del Agente DQN**\n",
    "\n",
    "El agente implementa técnicas avanzadas del algoritmo DQN según la arquitectura de Mnih et al. (2015):\n",
    "\n",
    "a) Red principal y red objetivo\n",
    "- La red principal es la que se entrena activamente.\n",
    "\n",
    "- La red objetivo se actualiza cada cierto número de pasos (update_target_every) para estabilizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273666d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595056bc",
   "metadata": {},
   "source": [
    "b) Memoria de Replay\n",
    "\n",
    "El agente almacena transiciones pasadas en una cola circular (deque), desde donde extrae mini-batches para entrenamiento aleatorio. Esto rompe la correlación temporal entre muestras y mejora la estabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.memory = deque(maxlen=20000)\n",
    "self.memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bedf73",
   "metadata": {},
   "source": [
    "c) Política Epsilon-Greedy\n",
    "\n",
    "Durante el entrenamiento, el agente decide sus acciones con una política que balancea exploración y explotación. Con probabilidad ϵ, elige una acción aleatoria; de lo contrario, toma la acción con mayor valor Q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ccf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if explore and np.random.rand() <= self.epsilon:\n",
    "    return random.randrange(self.action_size)\n",
    "...\n",
    "action_values = self.model(state_tensor)\n",
    "return np.argmax(action_values.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4613b0",
   "metadata": {},
   "source": [
    "El valor de ϵ decae progresivamente para favorecer la explotación del conocimiento adquirido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a768f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.epsilon > self.epsilon_min:\n",
    "    self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d172a9c",
   "metadata": {},
   "source": [
    "**Proceso de Entrenamiento**\n",
    "\n",
    "El agente entrena su red principal mediante retropropagación en lotes aleatorios (batch_size) tomados de la memoria de replay. Cada lote actualiza la red usando la pérdida entre los valores actuales y los objetivos (targets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo del valor objetivo (target Q)\n",
    "target_q_values = rewards + (gamma * next_q_values * (~dones))\n",
    "loss = criterion(current_q_values, target_q_values)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c60742",
   "metadata": {},
   "source": [
    "El uso de torch.no_grad() para calcular los targets evita que se acumulen gradientes en la red objetivo.\n",
    "\n",
    "**Interfaz Visual e Interacción**\n",
    "\n",
    "El entorno visual fue implementado en main.py usando Pygame. Incluye dos modos:\n",
    "\n",
    "- SETUP: permite al usuario colocar frutas y venenos manualmente en el tablero.\n",
    "\n",
    "- RUN: el agente entrenado toma el control y ejecuta su política aprendida, visualizando su comportamiento en tiempo real.\n",
    "\n",
    "Esto permitió una validación cualitativa del rendimiento del agente en diferentes configuraciones.\n",
    "\n",
    "**Resultados y Observaciones**\n",
    "\n",
    "- El uso de CNN permitió al agente generalizar mejor en diferentes escenarios.\n",
    "\n",
    "- El sistema de reward shaping (recompensas por acercarse a frutas) mejoró la eficiencia del entrenamiento.\n",
    "\n",
    "- La representación como imagen multicanal ayudó a la red a reconocer patrones espaciales útiles para la navegación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe144e4d",
   "metadata": {},
   "source": [
    "#### 2.3 DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac89c59",
   "metadata": {},
   "source": [
    "**Problemas con DQN clásico**\n",
    "\n",
    "Aunque Deep Q-Networks (DQN) mejoró la generalización del agente frente al Q-Learning tradicional, aún sufría de un problema conocido como overestimation bias: la tendencia a sobrevalorar las recompensas futuras. Esto ocurre porque DQN utiliza la misma red para seleccionar y evaluar las acciones futuras, lo que puede introducir errores acumulativos.\n",
    "\n",
    "Para mitigar este problema, se implementó el algoritmo Double DQN (DDQN), una mejora propuesta por Van Hasselt et al. (2016), que desacopla el proceso de selección de acción del de evaluación de valor. Esto reduce el sesgo y mejora la estabilidad del entrenamiento.\n",
    "\n",
    "**Diferencia Clave: Selección vs Evaluación**\n",
    "\n",
    "En DQN clásico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = reward + gamma * max(Q_target(next_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e858a8",
   "metadata": {},
   "source": [
    "En Double DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4587da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección con red principal\n",
    "next_actions = model(next_state).argmax(dim=1)\n",
    "\n",
    "# Evaluación con red objetivo\n",
    "next_q = target_model(next_state).gather(1, next_actions.unsqueeze(1))\n",
    "target = reward + gamma * next_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343fa13",
   "metadata": {},
   "source": [
    "Este cambio sutil mejora notablemente la precisión de la estimación de los valores Q futuros.\n",
    "\n",
    "**Implementación en el Proyecto**\n",
    "\n",
    "La clase Agent en agent.py fue modificada para aplicar el criterio Double DQN en el método replay(). La arquitectura de la red se mantuvo similar al agente DQN, pero se ajustó la forma en que se calcula el valor objetivo durante el entrenamiento.\n",
    "\n",
    "Fragmento clave del método replay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb594ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Selección con la red principal\n",
    "next_actions = self.model(next_states).detach().argmax(dim=1).unsqueeze(1)\n",
    "\n",
    "# 2. Evaluación con la red objetivo\n",
    "next_q_values = self.target_model(next_states).gather(1, next_actions)\n",
    "\n",
    "# 3. Cálculo de target\n",
    "targets = rewards + gamma * next_q_values * (1 - dones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9413b2",
   "metadata": {},
   "source": [
    "**Otras Mejoras y Técnicas Usadas**\n",
    "\n",
    "Además del cambio en el cálculo de los valores objetivos, el agente DDQN mantiene:\n",
    "\n",
    "- Una política epsilon-greedy con decaimiento gradual.\n",
    "\n",
    "- Replay Buffer circular para almacenar transiciones de entrenamiento.\n",
    "\n",
    "- Actualización periódica de la red objetivo para estabilizar el aprendizaje.\n",
    "\n",
    "- Entrenamiento mini-batch con retropropagación en train.py.\n",
    "\n",
    "**Interfaz Interactiva**\n",
    "\n",
    "La demostración del agente DDQN se integró a través de main.py, mientras que el entrenamiento puede ser gestionado desde interfaztrain.py. La representación visual de frutas, venenos y el agente permite observar cómo la política aprendida se adapta al entorno.\n",
    "\n",
    "**Resultados Obtenidos**\n",
    "\n",
    "Durante el entrenamiento de DDQN, se observó una mejora significativa en:\n",
    "\n",
    "- Estabilidad del aprendizaje: menos oscilaciones durante el entrenamiento.\n",
    "\n",
    "- Precisión en la toma de decisiones: el agente evitó más venenos sin necesidad de aumentar la exploración.\n",
    "\n",
    "- Generalización: el agente entrenado pudo actuar correctamente en escenarios no vistos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4021b",
   "metadata": {},
   "source": [
    "#### 2.4. Paradigma Alternativo: Algoritmos Genéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a90e4b",
   "metadata": {},
   "source": [
    "**Motivación**\n",
    "\n",
    "A diferencia de los algoritmos anteriores que dependen de gradientes y retropropagación (como Q-Learning o DQN), el enfoque evolutivo busca optimizar directamente las políticas de los agentes mediante la simulación de un proceso de selección natural. Esta técnica resulta especialmente útil cuando se trabaja con entornos ruidosos o no diferenciables, donde las funciones de pérdida tradicionales pueden fallar o llevar a soluciones subóptimas.\n",
    "\n",
    "**Fundamentos del Algoritmo Genético**\n",
    "\n",
    "El algoritmo genético implementado se basa en los principios fundamentales de la evolución biológica:\n",
    "\n",
    "1. Inicialización: Se genera una población aleatoria de agentes, cada uno con una red neuronal que actúa como su \"ADN\".\n",
    "\n",
    "2. Evaluación (Fitness): Cada agente se enfrenta a un entorno dinámico y su desempeño se mide como la suma de recompensas obtenidas.\n",
    "\n",
    "3. Selección: Se eligen los agentes con mayor fitness para reproducirse.\n",
    "\n",
    "4. Cruzamiento (Crossover): Los genes de dos padres se combinan para formar un nuevo agente hijo.\n",
    "\n",
    "5. Mutación: Se introduce variación aleatoria en los genes del hijo.\n",
    "\n",
    "6. Elitismo y Reemplazo: Se conservan los mejores agentes y se forma una nueva generación con los hijos generados.\n",
    "\n",
    "Este ciclo se repite durante múltiples generaciones hasta que el agente evoluciona una política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70729105",
   "metadata": {},
   "source": [
    "**Implementación del Agente Genético**\n",
    "\n",
    "La arquitectura general del entrenamiento se encuentra en train_ga.py, y el comportamiento del agente está definido en agent_ga.py.\n",
    "\n",
    "**a) Estructura de la Población**\n",
    "\n",
    "Cada agente está representado por una red neuronal densa que recibe el estado del entorno y genera una acción discreta. Los pesos de esta red constituyen su genotipo. Inicialmente, los pesos son asignados aleatoriamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6158e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_population():\n",
    "    return [Agent() for _ in range(POPULATION_SIZE)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc1513",
   "metadata": {},
   "source": [
    "**b) Evaluación de Fitness**\n",
    "\n",
    "Cada agente es evaluado en un escenario aleatorio donde debe recolectar frutas y evitar venenos. Su desempeño se mide por la suma total de recompensas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    action = agent.choose_action(state)\n",
    "    state, reward, done = env.step(action)\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276b19d",
   "metadata": {},
   "source": [
    "Este total es almacenado como su fitness, y determina su posibilidad de ser seleccionado como padre.\n",
    "\n",
    "**c) Selección de Padres**\n",
    "\n",
    "Se selecciona el 20% de los mejores agentes de la población como candidatos para reproducción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446552b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "parents = population[:int(POPULATION_SIZE * 0.2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ca8dc",
   "metadata": {},
   "source": [
    "**d) Cruzamiento Uniforme**\n",
    "\n",
    "La recombinación de genes se realiza de manera uniforme: cada peso tiene una probabilidad del 50% de heredarse de cada padre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in child_genes.keys():\n",
    "    if random.random() < 0.5:\n",
    "        child_genes[key] = p1_genes[key].clone()\n",
    "    else:\n",
    "        child_genes[key] = p2_genes[key].clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf78bf2",
   "metadata": {},
   "source": [
    "**e) Mutación Gaussiana**\n",
    "\n",
    "Cada parámetro tiene una probabilidad de ser alterado añadiendo ruido gaussiano con desviación estándar 0.1, lo que permite explorar nuevas soluciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn_like(child_genes[key]) * 0.1\n",
    "child_genes[key] += noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3e3bc",
   "metadata": {},
   "source": [
    "#### 2.5 Solución Definitiva: Aprendizaje por Imitación y Currículo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596289e",
   "metadata": {},
   "source": [
    "Tras los desafíos encontrados, la estrategia final y más exitosa fue el Aprendizaje por Imitación. Este enfoque cambia el paradigma: en lugar de que el agente descubra una estrategia por sí mismo, se le enseña directamente imitando a un experto perfecto. Para ello, se implementó el algoritmo de búsqueda A* como un \"oráculo\" capaz de calcular siempre la ruta óptima en cualquier tablero.\n",
    "\n",
    "El primer paso fue generar un \"libro de texto\" para el agente, creando miles de ejemplos de jugadas perfectas. Se crearon datasets separados para escenarios de 1, 2, 3 y 4 frutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función clave para generar los datos de entrenamiento para el currículo.\n",
    "def generate_expert_data_for_n_fruits(num_fruits, num_samples, output_file):\n",
    "    \"\"\"Genera un dataset experto para un número específico de frutas.\"\"\"\n",
    "    env = GridEnvironment()\n",
    "    expert_data = []\n",
    "    print(f\"Generando {num_samples} muestras para {num_fruits} fruta(s)...\")\n",
    "    \n",
    "    while len(expert_data) < num_samples:\n",
    "        # Generar un escenario aleatorio con `num_fruits` frutas.\n",
    "        # ... (código de generación de escenario) ...\n",
    "        env.reset(agent_pos=agent_p, fruit_pos=fruit_p, poison_pos=poison_p)\n",
    "        \n",
    "        # Simular el episodio usando el experto A* paso a paso.\n",
    "        for _ in range(50):\n",
    "            if not env.fruit_pos: break\n",
    "            \n",
    "            # El experto A* recalcula la mejor ruta desde la posición actual.\n",
    "            path = a_star_search(GRID_SIZE, env.agent_pos, goal_fruit, env.poison_pos)\n",
    "\n",
    "            if path and len(path) > 0:\n",
    "                # Se toma solo el primer paso de la ruta óptima.\n",
    "                action = get_action(env.agent_pos, path[0])\n",
    "                state = env.get_state()\n",
    "                # Se guarda el par (estado_actual, accion_correcta).\n",
    "                expert_data.append((state, action))\n",
    "                env.step(action)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(expert_data, f)\n",
    "    print(f\"Dataset '{output_file}' creado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670e8e0",
   "metadata": {},
   "source": [
    "La innovación crucial fue entrenar al agente con un Aprendizaje por Currículo. En lugar de mostrarle todos los datos a la vez, el agente fue entrenado por etapas, comenzando con la tarea más fácil (1 fruta) y aumentando gradualmente la dificultad. Esto le permitió dominar primero los conceptos básicos de \"ganar\" antes de enfrentarse a escenarios más complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d28ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El script de entrenamiento que implementa la estrategia de Curriculum Learning.\n",
    "if __name__ == \"__main__\":\n",
    "    model = AgentNetwork()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # El Currículo: una lista de \"lecciones\" de dificultad creciente.\n",
    "    CURRICULUM = [\n",
    "        (\"expert_data_1_fruit.pkl\", 25),   # Lección 1: Nivel básico\n",
    "        (\"expert_data_2_fruits.pkl\", 30),  # Lección 2: Nivel intermedio\n",
    "        (\"expert_data_3_fruits.pkl\", 40),  # Lección 3: Nivel avanzado\n",
    "        (\"expert_data_4_fruits.pkl\", 50)   # Lección 4: Nivel experto\n",
    "    ]\n",
    "\n",
    "    # Bucle principal que itera a través de las lecciones del currículo.\n",
    "    for i, (dataset_file, num_epochs) in enumerate(CURRICULUM):\n",
    "        print(f\"\\n--- Iniciando Lección {i+1}/{len(CURRICULUM)}: {dataset_file} ---\")\n",
    "        \n",
    "        # Cargar el dataset de la lección actual.\n",
    "        with open(dataset_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Preparar los datos para el entrenamiento.\n",
    "        # ... (código de creación del DataLoader) ...\n",
    "\n",
    "        # Entrenar el modelo (sin reiniciarlo) por el número de épocas de la lección.\n",
    "        for epoch in range(num_epochs):\n",
    "            # ... (bucle de entrenamiento estándar de aprendizaje supervisado) ...\n",
    "            print(f\"  Época {epoch+1}/{num_epochs}, Pérdida: {avg_loss:.4f}\")\n",
    "\n",
    "    # Guardar el modelo final, que ha aprendido de todo el currículo.\n",
    "    torch.save(model.state_dict(), \"imitacion_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a9c28",
   "metadata": {},
   "source": [
    "### Jugador humano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee67918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modo de juego humano con controles aleatorios - Come Frutas.\n",
    "\n",
    "Este módulo implementa una versión jugable del entorno donde un humano puede\n",
    "controlar el agente directamente. La característica única es que los controles\n",
    "de movimiento se asignan aleatoriamente cada vez que se inicia una partida,\n",
    "añadiendo un elemento de desafío y adaptabilidad.\n",
    "\n",
    "Características principales:\n",
    "- Modo Setup: Configuración manual del escenario\n",
    "- Modo Humano: Control directo del agente por el jugador\n",
    "- Controles aleatorios: Mapeo aleatorio de teclas a movimientos\n",
    "- Interfaz intuitiva: Gráficos y feedback visual\n",
    "- Desafío adaptativo: Cada partida requiere aprender nuevos controles\n",
    "\n",
    "Propósito educativo:\n",
    "- Comparar rendimiento humano vs. IA\n",
    "- Experimentar la dificultad de adaptación a controles cambiantes\n",
    "- Entender la importancia de la consistencia en interfaces\n",
    "- Apreciar la flexibilidad del aprendizaje humano\n",
    "\n",
    "Autor: [Tu nombre]\n",
    "Fecha: Agosto 2025\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "# CONFIGURACIÓN DEL ENTORNO VISUAL\n",
    "\"\"\"\n",
    "Parámetros visuales y dimensiones de la interfaz de juego.\n",
    "Utiliza celdas más grandes (120px) para mejor visibilidad durante el juego manual.\n",
    "\"\"\"\n",
    "GRID_WIDTH = 5              # Ancho de la cuadrícula en celdas\n",
    "GRID_HEIGHT = 5             # Alto de la cuadrícula en celdas\n",
    "CELL_SIZE = 120             # Tamaño de cada celda en píxeles (mayor para juego manual)\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la ventana (600px)\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto del área de juego (600px)\n",
    "\n",
    "# PALETA DE COLORES CONSISTENTE\n",
    "\"\"\"\n",
    "Esquema de colores oscuro profesional, consistente con otros módulos del proyecto.\n",
    "\"\"\"\n",
    "COLOR_FONDO = (25, 25, 25)      # Gris muy oscuro para el fondo\n",
    "COLOR_LINEAS = (40, 40, 40)     # Gris oscuro para líneas de cuadrícula\n",
    "COLOR_CURSOR = (255, 255, 0)    # Amarillo brillante para cursor de selección\n",
    "COLOR_TEXTO = (230, 230, 230)   # Gris claro para texto legible\n",
    "\n",
    "# SISTEMA DE CONTROLES ALEATORIOS\n",
    "\"\"\"\n",
    "Genera un conjunto de teclas válidas para asignación aleatoria de controles.\n",
    "Se evitan teclas especiales para prevenir conflictos con funciones del sistema.\n",
    "\"\"\"\n",
    "TECLAS_VALIDAS = [getattr(pygame, f\"K_{c}\") for c in string.ascii_lowercase + string.digits]\n",
    "\n",
    "class EntornoHumano:\n",
    "    \"\"\"\n",
    "    Entorno de juego optimizado para control humano directo.\n",
    "    \n",
    "    Esta clase maneja la lógica del juego cuando un humano controla el agente,\n",
    "    incluyendo movimiento, colisiones, recolección de objetos y condiciones\n",
    "    de victoria/derrota. Se enfoca en proporcionar feedback inmediato y\n",
    "    una experiencia de juego fluida.\n",
    "    \n",
    "    Diferencias con entornos de IA:\n",
    "    - Feedback inmediato con mensajes en consola\n",
    "    - Lógica de juego simplificada (sin recompensas numéricas)\n",
    "    - Terminación inmediata en victoria/derrota\n",
    "    - Controles responsivos para jugabilidad humana\n",
    "    \n",
    "    Attributes:\n",
    "        agente_pos (tuple): Posición actual del agente (x, y)\n",
    "        frutas (set): Conjunto de posiciones con frutas\n",
    "        venenos (set): Conjunto de posiciones con venenos\n",
    "        paredes (set): Conjunto de posiciones con paredes/obstáculos\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuración vacía.\n",
    "        \n",
    "        El agente comienza en la esquina superior izquierda (0,0) y todos\n",
    "        los conjuntos de elementos están vacíos, permitiendo configuración manual.\n",
    "        \"\"\"\n",
    "        self.agente_pos = (0, 0)    # Posición inicial del agente\n",
    "        self.frutas = set()         # Conjunto de posiciones de frutas\n",
    "        self.venenos = set()        # Conjunto de posiciones de venenos\n",
    "        self.paredes = set()        # Conjunto de posiciones de paredes\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resetea la posición del agente al inicio del juego.\n",
    "        \n",
    "        Coloca al agente en la posición inicial (0,0) sin modificar\n",
    "        la configuración del escenario. Utilizado al comenzar una nueva partida.\n",
    "        \"\"\"\n",
    "        self.agente_pos = (0, 0)\n",
    "\n",
    "    def limpiar(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno.\n",
    "        \n",
    "        Limpia frutas, venenos y paredes del escenario, dejando una\n",
    "        cuadrícula vacía para configuración desde cero.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acción del jugador humano en el entorno.\n",
    "        \n",
    "        Procesa el movimiento del agente, verifica colisiones y maneja\n",
    "        las interacciones con elementos del entorno. Proporciona feedback\n",
    "        inmediato al jugador mediante mensajes en consola.\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Dirección de movimiento:\n",
    "                         0 = Arriba (decrementar y)\n",
    "                         1 = Abajo (incrementar y)\n",
    "                         2 = Izquierda (decrementar x)\n",
    "                         3 = Derecha (incrementar x)\n",
    "        \n",
    "        Returns:\n",
    "            bool: True si el juego terminó (victoria o derrota), False si continúa\n",
    "        \"\"\"\n",
    "        # Calcular nueva posición basada en la acción\n",
    "        x, y = self.agente_pos\n",
    "        if accion == 0:     # Arriba\n",
    "            y -= 1\n",
    "        elif accion == 1:   # Abajo\n",
    "            y += 1\n",
    "        elif accion == 2:   # Izquierda\n",
    "            x -= 1\n",
    "        elif accion == 3:   # Derecha\n",
    "            x += 1\n",
    "\n",
    "        # Verificar colisiones: límites del tablero o paredes\n",
    "        if x < 0 or x >= GRID_WIDTH or y < 0 or y >= GRID_HEIGHT or (x, y) in self.paredes:\n",
    "            # Movimiento inválido: no actualizar posición\n",
    "            return False\n",
    "\n",
    "        # Movimiento válido: actualizar posición del agente\n",
    "        self.agente_pos = (x, y)\n",
    "        \n",
    "        # Verificar interacciones con elementos del entorno\n",
    "        if self.agente_pos in self.frutas:\n",
    "            # Fruta recogida: eliminar del conjunto\n",
    "            self.frutas.remove(self.agente_pos)\n",
    "            \n",
    "            # Verificar condición de victoria\n",
    "            if not self.frutas:\n",
    "                print(\"\\n✨ ¡Ganaste! Recolectaste todas las frutas.\\n\")\n",
    "                return True  # Juego terminado con éxito\n",
    "                \n",
    "        elif self.agente_pos in self.venenos:\n",
    "            # Veneno tocado: derrota inmediata\n",
    "            print(\"\\n☠️ ¡Oh no! Tocaste un veneno.\\n\")\n",
    "            return True  # Juego terminado con fallo\n",
    "            \n",
    "        # Continuar juego\n",
    "        return False\n",
    "\n",
    "    def dibujar(self, pantalla, modo, cursor_pos, img_fruta, img_veneno, img_pared, img_agente, _):\n",
    "        \"\"\"\n",
    "        Renderiza el estado completo del entorno con interfaz interactiva.\n",
    "        \n",
    "        Dibuja todos los elementos visuales del juego incluyendo grid, objetos\n",
    "        del entorno y cursor de selección. Proporciona feedback visual para\n",
    "        la interacción del jugador en diferentes modos (colocación/juego).\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde renderizar\n",
    "            modo (str): Modo actual de la interfaz ('frutas', 'venenos', 'paredes', 'jugar')\n",
    "            cursor_pos (tuple): Posición (x,y) del cursor en coordenadas de grid\n",
    "            img_fruta (pygame.Surface): Sprite de las frutas\n",
    "            img_veneno (pygame.Surface): Sprite de los venenos\n",
    "            img_pared (pygame.Surface): Sprite de las paredes\n",
    "            img_agente (pygame.Surface): Sprite del agente\n",
    "            _ : Parámetro no utilizado (compatibilidad de interfaz)\n",
    "        \n",
    "        Note:\n",
    "            Renderiza en orden específico: fondo, grid, objetos, agente, cursor.\n",
    "            El cursor cambia de color según el modo de colocación activo.\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "\n",
    "        # Dibujar líneas del grid para guía visual\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibujar elementos del entorno\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0]*CELL_SIZE, fruta[1]*CELL_SIZE))\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0]*CELL_SIZE, veneno[1]*CELL_SIZE))\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0]*CELL_SIZE, pared[1]*CELL_SIZE))\n",
    "\n",
    "        # Dibujar agente (jugador) - siempre visible en primer plano\n",
    "        pantalla.blit(img_agente, (self.agente_pos[0]*CELL_SIZE, self.agente_pos[1]*CELL_SIZE))\n",
    "\n",
    "        # Dibujar cursor de selección en modo configuración\n",
    "        if modo == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(cursor_pos[0]*CELL_SIZE, cursor_pos[1]*CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar información de interfaz\n",
    "        font = pygame.font.Font(None, 30)\n",
    "        pantalla.blit(font.render(f\"Modo: {modo}\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(font.render(\"F: Fruta, V: Veneno, W: Pared, C: Limpiar, H: Jugar\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(font.render(\"Descubre los controles ocultos usando letras/números\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "def cargar_imagen(nombre, fallback_color):\n",
    "    \"\"\"\n",
    "    Carga una imagen desde archivo con sistema de respaldo.\n",
    "    \n",
    "    Intenta cargar una imagen sprite desde el directorio actual.\n",
    "    Si la carga falla, crea una superficie de color sólido como respaldo.\n",
    "    Escala automáticamente al tamaño de celda definido.\n",
    "    \n",
    "    Args:\n",
    "        nombre (str): Nombre del archivo de imagen a cargar\n",
    "        fallback_color (tuple): Color RGB (r,g,b) para superficie de respaldo\n",
    "    \n",
    "    Returns:\n",
    "        pygame.Surface: Superficie cargada y escalada, o superficie de color\n",
    "                       si la carga falló\n",
    "    \n",
    "    Note:\n",
    "        Todas las imágenes se escalan a CELL_SIZE x CELL_SIZE píxeles.\n",
    "        Utiliza convert_alpha() para optimizar el renderizado con transparencia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir ruta completa al archivo de imagen\n",
    "        ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "        # Cargar imagen con soporte de transparencia\n",
    "        img = pygame.image.load(ruta).convert_alpha()\n",
    "        # Escalar a tamaño de celda estándar\n",
    "        return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "    except:\n",
    "        # Crear superficie de respaldo con color sólido si falla la carga\n",
    "        s = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "        s.fill(fallback_color)\n",
    "        return s\n",
    "\n",
    "def generar_controles_aleatorios():\n",
    "    \"\"\"\n",
    "    Genera un mapeo aleatorio de teclas para controles de movimiento.\n",
    "    \n",
    "    Crea una asignación aleatoria entre teclas del teclado y direcciones\n",
    "    de movimiento para añadir un elemento de desafío y descubrimiento\n",
    "    al juego. Los jugadores deben encontrar qué teclas controlan cada dirección.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapeo de códigos de tecla pygame a acciones de movimiento:\n",
    "              {tecla_pygame: accion_int}\n",
    "              donde accion_int es 0=Arriba, 1=Abajo, 2=Izquierda, 3=Derecha\n",
    "    \n",
    "    Note:\n",
    "        Utiliza teclas alfanuméricas (A-Z, 0-9) para máxima compatibilidad.\n",
    "        Garantiza que cada dirección tenga exactamente una tecla asignada.\n",
    "    \"\"\"\n",
    "    # Seleccionar 4 teclas aleatorias del conjunto disponible\n",
    "    teclas = random.sample(TECLAS_VALIDAS, 4)\n",
    "    # Crear lista de acciones de movimiento\n",
    "    acciones = [0, 1, 2, 3]  # Arriba, abajo, izquierda, derecha\n",
    "    # Mezclar aleatoriamente las acciones\n",
    "    random.shuffle(acciones)\n",
    "    # Crear diccionario de mapeo tecla->acción\n",
    "    return dict(zip(teclas, acciones))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal del juego en modo humano.\n",
    "    \n",
    "    Inicializa Pygame, configura la ventana de juego y ejecuta el bucle\n",
    "    principal que maneja dos modos: configuración del entorno y juego\n",
    "    con controles aleatorios. Proporciona una experiencia interactiva\n",
    "    donde el jugador puede diseñar niveles y luego jugarlos.\n",
    "    \n",
    "    Flujo del juego:\n",
    "        1. Modo SETUP: Colocar frutas, venenos y paredes con el mouse\n",
    "        2. Modo JUGAR: Controlar agente con teclas aleatorias descubiertas\n",
    "        3. Victoria: Recolectar todas las frutas\n",
    "        4. Derrota: Tocar veneno\n",
    "    \n",
    "    Controles SETUP:\n",
    "        - Mouse: Mover cursor\n",
    "        - F: Colocar fruta\n",
    "        - V: Colocar veneno  \n",
    "        - W: Colocar pared\n",
    "        - C: Limpiar todo\n",
    "        - H: Iniciar juego\n",
    "    \n",
    "    Controles JUGAR:\n",
    "        - Teclas aleatorias para movimiento (descubrir experimentando)\n",
    "        - ESC: Volver a configuración\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y configurar ventana\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 100))\n",
    "    pygame.display.set_caption(\"Modo Humano Aleatorio - Come Frutas\")\n",
    "\n",
    "    # Inicializar entorno y variables de estado\n",
    "    entorno = EntornoHumano()\n",
    "    cursor_pos = [0, 0]\n",
    "    modo = \"SETUP\"  # Modo inicial: configuración del entorno\n",
    "    mapeo_controles = {}  # Mapeo de teclas aleatorias (generado al jugar)\n",
    "\n",
    "    # Cargar sprites con colores de respaldo\n",
    "    img_fruta = cargar_imagen(\"fruta.png\", (40, 200, 40))\n",
    "    img_veneno = cargar_imagen(\"veneno.png\", (255, 50, 50))\n",
    "    img_pared = cargar_imagen(\"pared.jpg\", (80, 80, 80))\n",
    "    img_agente = cargar_imagen(\"agente.png\", (60, 100, 255))\n",
    "\n",
    "    # Variables de control del juego\n",
    "    reloj = pygame.time.Clock()\n",
    "    corriendo = True\n",
    "\n",
    "    # Bucle principal del juego\n",
    "    while corriendo:\n",
    "        # Procesar eventos de entrada\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "            elif evento.type == pygame.KEYDOWN:\n",
    "                if evento.key == pygame.K_s:\n",
    "                    modo = \"SETUP\"\n",
    "                elif evento.key == pygame.K_h:\n",
    "                    modo = \"HUMANO\"\n",
    "                    entorno.reset()\n",
    "                    mapeo_controles = generar_controles_aleatorios()\n",
    "\n",
    "                if modo == \"SETUP\":\n",
    "                    if evento.key == pygame.K_UP: cursor_pos[1] = max(0, cursor_pos[1]-1)\n",
    "                    elif evento.key == pygame.K_DOWN: cursor_pos[1] = min(GRID_HEIGHT-1, cursor_pos[1]+1)\n",
    "                    elif evento.key == pygame.K_LEFT: cursor_pos[0] = max(0, cursor_pos[0]-1)\n",
    "                    elif evento.key == pygame.K_RIGHT: cursor_pos[0] = min(GRID_WIDTH-1, cursor_pos[0]+1)\n",
    "                    # Colocación de elementos con teclas específicas\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    if evento.key == pygame.K_f: \n",
    "                        # F: Colocar/quitar fruta (toggle)\n",
    "                        entorno.frutas.symmetric_difference_update({pos})\n",
    "                        entorno.venenos.discard(pos)\n",
    "                        entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_v: \n",
    "                        # V: Colocar/quitar veneno (toggle)\n",
    "                        entorno.venenos.symmetric_difference_update({pos})\n",
    "                        entorno.frutas.discard(pos)\n",
    "                        entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_w: \n",
    "                        # W: Colocar/quitar pared (toggle)\n",
    "                        entorno.paredes.symmetric_difference_update({pos})\n",
    "                        entorno.frutas.discard(pos)\n",
    "                        entorno.venenos.discard(pos)\n",
    "                    elif evento.key == pygame.K_c: \n",
    "                        # C: Limpiar todo el entorno\n",
    "                        entorno.limpiar()\n",
    "\n",
    "                # Controles específicos del modo HUMANO\n",
    "                elif modo == \"HUMANO\":\n",
    "                    if evento.key in mapeo_controles:\n",
    "                        # Ejecutar acción de movimiento con tecla aleatoria\n",
    "                        accion = mapeo_controles[evento.key]\n",
    "                        terminado = entorno.step(accion)\n",
    "                        if terminado:\n",
    "                            # Volver a configuración al terminar el juego\n",
    "                            modo = \"SETUP\"\n",
    "\n",
    "        # Renderizar estado actual del juego\n",
    "        entorno.dibujar(pantalla, modo, cursor_pos, img_fruta, img_veneno, img_pared, img_agente, mapeo_controles)\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(30)\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a5bfd",
   "metadata": {},
   "source": [
    "### 3. Resultados y Conclusión\n",
    "El viaje a través de estos diferentes paradigmas de IA fue revelador. Mientras que los enfoques de Aprendizaje por Refuerzo y Algoritmos Genéticos mostraron potencial, también exhibieron dificultades para converger de manera consistente en un entorno tan \"frágil\", donde un solo error puede llevar al fracaso.\n",
    "\n",
    "La estrategia de Aprendizaje por Imitación combinada con el Aprendizaje por Currículo demostró ser la más efectiva y robusta. Al aprender de un experto perfecto y hacerlo de manera gradual, el agente final fue capaz de generalizar su conocimiento y resolver de manera confiable tanto escenarios simples como complejos. Para la demostración visual, se desarrolló una interfaz en Pygame que permite a los usuarios configurar sus propios niveles y observar al agente entrenado resolverlos en tiempo real.\n",
    "\n",
    "Este proyecto subraya una lección fundamental en el desarrollo de IA: a menudo, la estrategia de entrenamiento y la calidad de los datos son tan o más importantes que la elección del algoritmo en sí. El resultado es un agente competente y una profunda comprensión práctica de los desafíos y soluciones en el campo del Machine Learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
