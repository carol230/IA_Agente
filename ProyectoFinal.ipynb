{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803cfcb5",
   "metadata": {},
   "source": [
    "# Taller sobre clustering aglomerativo y DBSCAN\n",
    "# Agente Come Frutas\n",
    "\n",
    "## Integrantes: Ayala Ivonne, Cumbal Mateo, Garc茅s Boris, Morales David, Pereira Alicia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512b684",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### Introducci贸n\n",
    "El presente informe detalla el proceso de dise帽o, implementaci贸n y experimentaci贸n para el desarrollo de un agente aut贸nomo de Inteligencia Artificial en el marco del proyecto \"Agente Come-Frutas\". El desaf铆o se centra en un problema cl谩sico de toma de decisiones secuenciales en un entorno din谩mico y con riesgos, sirviendo como un caso de estudio pr谩ctico para la aplicaci贸n de t茅cnicas avanzadas de Machine Learning.\n",
    "\n",
    "El problema consiste en un entorno de rejilla de 5x5 en el que un agente debe aprender a navegar de manera eficiente. El objetivo principal es maximizar una puntuaci贸n recolectando \"frutas\" (que otorgan recompensas positivas) y evitando \"venenos\" (que imponen castigos negativos). La meta final es desarrollar una pol铆tica de comportamiento 贸ptima que permita al agente limpiar el tablero de todas las frutas, garantizando su supervivencia al esquivar todos los venenos presentes.\n",
    "\n",
    "Para alcanzar este objetivo, el proyecto transit贸 por un riguroso proceso de experimentaci贸n, explorando m煤ltiples paradigmas de la Inteligencia Artificial. Se inici贸 con un enfoque en el Aprendizaje por Refuerzo (Reinforcement Learning), implementando y depurando algoritmos de vanguardia como Deep Q-Networks (DQN) y su variante mejorada, Double DQN (DDQN).\n",
    "\n",
    "Frente a los desaf铆os cl谩sicos de convergencia y estabilidad inherentes a RL, la investigaci贸n se expandi贸 para incluir otras estrategias. Se exploraron los Algoritmos Gen茅ticos, un enfoque basado en principios de evoluci贸n, y el Aprendizaje por Imitaci贸n, una potente t茅cnica de aprendizaje supervisado que requiri贸 el desarrollo de un \"or谩culo\" experto basado en el algoritmo de b煤squeda A*.\n",
    "\n",
    "A continuaci贸n, se presenta el c贸digo documentado de la implementaci贸n final, reflejando la culminaci贸n de este profundo y multifac茅tico proceso de desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9755b",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90092b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONSTANTES DE CONFIGURACIN DEL JUEGO Y LA PANTALLA ---\n",
    "# Estas constantes definen el tama帽o del tablero, de cada celda y de la ventana del juego.\n",
    "GRID_WIDTH = 5\n",
    "GRID_HEIGHT = 5\n",
    "CELL_SIZE = 120\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE\n",
    "\n",
    "# Colores (formato RGB) para los elementos de la interfaz.\n",
    "COLOR_FONDO = (25, 25, 25)\n",
    "COLOR_LINEAS = (40, 40, 40)\n",
    "COLOR_AGENTE = (60, 100, 255)\n",
    "COLOR_PARED = (80, 80, 80)\n",
    "COLOR_TEXTO = (230, 230, 230)\n",
    "COLOR_CURSOR = (255, 255, 0)\n",
    "\n",
    "# --- PARMETROS DEL APRENDIZAJE POR REFUERZO (Q-LEARNING) ---\n",
    "# Estos son los \"hiperpar谩metros\" que controlan c贸mo aprende el agente.\n",
    "RECOMPENSA_FRUTA = 100         # Puntuaci贸n alta por encontrar una fruta.\n",
    "CASTIGO_VENENO = -100           # Castigo fuerte por tocar un veneno.\n",
    "RECOMPENSA_MOVIMIENTO = -0.1    # Peque帽o castigo por cada movimiento para incentivar la eficiencia.\n",
    "ALPHA = 0.1                     # Tasa de aprendizaje.\n",
    "GAMMA = 0.9                     # Factor de descuento.\n",
    "EPSILON = 1.0                   # Tasa de exploraci贸n inicial.\n",
    "EPSILON_DECAY = 0.9995          # Factor de decaimiento de epsilon.\n",
    "MIN_EPSILON = 0.01              # M铆nima tasa de exploraci贸n.\n",
    "NUM_EPISODIOS_ENTRENAMIENTO = 20000 # N煤mero de partidas que jugar谩 el agente para aprender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cae4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLASE DEL AGENTE ---\n",
    "# Define el \"cerebro\" del agente. Contiene la Tabla Q y la l贸gica para aprender y decidir.\n",
    "class AgenteQLearning:\n",
    "    def __init__(self, num_estados, num_acciones):\n",
    "        self.num_acciones = num_acciones\n",
    "        # La Tabla Q es una matriz que almacena el \"valor\" de cada acci贸n en cada estado posible.\n",
    "        # Aqu铆, el estado est谩 definido por la posici贸n (x, y) del agente en el tablero.\n",
    "        self.q_table = np.zeros((num_estados[0], num_estados[1], num_acciones))\n",
    "        self.epsilon = EPSILON  # Tasa de exploraci贸n (curiosidad).\n",
    "\n",
    "    def elegir_accion(self, estado):\n",
    "        \"\"\"Decide qu茅 acci贸n tomar usando la estrategia epsilon-greedy.\"\"\"\n",
    "        # Con probabilidad epsilon, toma una acci贸n aleatoria (exploraci贸n).\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randint(0, self.num_acciones - 1)\n",
    "        # De lo contrario, elige la mejor acci贸n conocida seg煤n la Tabla Q (explotaci贸n).\n",
    "        else:\n",
    "            return np.argmax(self.q_table[estado])\n",
    "\n",
    "    def actualizar_q_table(self, estado, accion, recompensa, nuevo_estado):\n",
    "        \"\"\"Actualiza el valor en la Tabla Q usando la f贸rmula de Bellman.\"\"\"\n",
    "        valor_antiguo = self.q_table[estado][accion]\n",
    "        # El valor futuro es el m谩ximo valor Q que se puede obtener desde el nuevo estado.\n",
    "        valor_futuro_maximo = np.max(self.q_table[nuevo_estado])\n",
    "        \n",
    "        # F贸rmula de Q-Learning: se actualiza el valor antiguo basado en la recompensa\n",
    "        # obtenida y el valor futuro esperado.\n",
    "        nuevo_q = valor_antiguo + ALPHA * (\n",
    "            recompensa + GAMMA * valor_futuro_maximo - valor_antiguo\n",
    "        )\n",
    "        self.q_table[estado][accion] = nuevo_q\n",
    "\n",
    "    def decaimiento_epsilon(self):\n",
    "        \"\"\"Reduce gradualmente el valor de epsilon para pasar de explorar a explotar.\"\"\"\n",
    "        if self.epsilon > MIN_EPSILON:\n",
    "            self.epsilon *= EPSILON_DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLASE DEL ENTORNO ---\n",
    "# Define las reglas del juego, el tablero y c贸mo interact煤a el agente con 茅l.\n",
    "class EntornoGrid:\n",
    "    def __init__(self):\n",
    "        self.agente_pos = (0, 0)\n",
    "        # Usamos 'sets' para un manejo eficiente de las posiciones de los objetos.\n",
    "        self.frutas = set()\n",
    "        self.venenos = set()\n",
    "        self.paredes = set()\n",
    "        self.reset_a_configuracion_inicial()\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"Resetea la posici贸n del agente al inicio (esquina superior izquierda).\"\"\"\n",
    "        self.agente_pos = (0, 0)\n",
    "        return self.agente_pos\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"Elimina todos los objetos del tablero.\"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion, modo_juego):\n",
    "        \"\"\"\n",
    "        Ejecuta un paso en el juego.\n",
    "        El agente toma una 'accion' y el entorno devuelve el 'nuevo_estado',\n",
    "        la 'recompensa' y si el juego ha 'terminado'.\n",
    "        \"\"\"\n",
    "        x, y = self.agente_pos\n",
    "        # Acciones: 0=arriba, 1=abajo, 2=izquierda, 3=derecha\n",
    "        if accion == 0: y -= 1\n",
    "        elif accion == 1: y += 1\n",
    "        elif accion == 2: x -= 1\n",
    "        elif accion == 3: x += 1\n",
    "\n",
    "        # Comprueba si el movimiento es v谩lido (dentro de los l铆mites y no choca con una pared).\n",
    "        if (x < 0 or x >= GRID_WIDTH or y < 0 or y >= GRID_HEIGHT or (x, y) in self.paredes):\n",
    "            # Si el movimiento es inv谩lido, el agente no se mueve y recibe una peque帽a penalizaci贸n.\n",
    "            return self.agente_pos, RECOMPENSA_MOVIMIENTO, False\n",
    "\n",
    "        # Actualiza la posici贸n del agente si el movimiento es v谩lido.\n",
    "        self.agente_pos = (x, y)\n",
    "        nuevo_estado = self.agente_pos\n",
    "        terminado = False\n",
    "\n",
    "        if nuevo_estado in self.frutas:\n",
    "            recompensa = RECOMPENSA_FRUTA\n",
    "            self.frutas.remove(nuevo_estado)\n",
    "            # El episodio solo termina con 茅xito si ya no quedan m谩s frutas.\n",
    "            if not self.frutas:\n",
    "                terminado = True\n",
    "        elif nuevo_estado in self.venenos:\n",
    "            recompensa = CASTIGO_VENENO\n",
    "            terminado = True  # Tocar un veneno siempre termina el juego.\n",
    "        else:\n",
    "            recompensa = RECOMPENSA_MOVIMIENTO\n",
    "\n",
    "        return nuevo_estado, recompensa, terminado\n",
    "\n",
    "    def dibujar(self, pantalla, modo_juego, cursor_pos, img_fruta, img_veneno, img_pared, img_agente):\n",
    "        \"\"\"Dibuja todos los elementos del juego en la pantalla.\"\"\"\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "\n",
    "        # Dibuja la cuadr铆cula.\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibuja las paredes, frutas, venenos y el agente usando sus im谩genes.\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "        pantalla.blit(img_agente, (self.agente_pos[0] * CELL_SIZE, self.agente_pos[1] * CELL_SIZE))\n",
    "\n",
    "        # En modo SETUP, dibuja un cursor para indicar d贸nde se colocar谩n los objetos.\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE, cursor_pos[1] * CELL_SIZE, CELL_SIZE, CELL_SIZE\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Dibuja la informaci贸n de ayuda y el modo de juego actual en la parte inferior.\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        controles_setup = font.render(\n",
    "            \"SETUP: Mover con flechas. F=Fruta, V=Veneno, W=Pared. 'C' para limpiar.\", True, COLOR_TEXTO\n",
    "        )\n",
    "        controles_run = font.render(\n",
    "            \"'T' para Entrenar, 'P' para Jugar, 'S' para Setup.\", True, COLOR_TEXTO\n",
    "        )\n",
    "\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles_setup, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles_run, (10, SCREEN_HEIGHT + 55))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIN PRINCIPAL DEL JUEGO ---\n",
    "# Orquesta todo el juego: inicializaci贸n, bucle principal, manejo de eventos y modos.\n",
    "def main():\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente Come-Frutas  vs 锔 (Q-Learning)\")\n",
    "\n",
    "    # --- Carga de im谩genes ---\n",
    "    # Intenta cargar los archivos de imagen. Si no los encuentra, usa cuadrados de colores como respaldo.\n",
    "    try:\n",
    "        ruta_fruta = os.path.join(os.path.dirname(__file__), \"fruta.png\")\n",
    "        img_fruta_original = pygame.image.load(ruta_fruta).convert_alpha()\n",
    "        img_fruta = pygame.transform.scale(img_fruta_original, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        print(\"Advertencia: No se encontr贸 'fruta.png'. Se usar谩 un cuadrado verde.\")\n",
    "        img_fruta = pygame.Surface((CELL_SIZE, CELL_SIZE)); img_fruta.fill((40, 200, 40))\n",
    "\n",
    "    try:\n",
    "        ruta_veneno = os.path.join(os.path.dirname(__file__), \"veneno.png\")\n",
    "        img_veneno_original = pygame.image.load(ruta_veneno).convert_alpha()\n",
    "        img_veneno = pygame.transform.scale(img_veneno_original, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        print(\"Advertencia: No se encontr贸 'veneno.png'. Se usar谩 un cuadrado rojo.\")\n",
    "        img_veneno = pygame.Surface((CELL_SIZE, CELL_SIZE)); img_veneno.fill((255, 50, 50))\n",
    "\n",
    "    try:\n",
    "        ruta_pared = os.path.join(os.path.dirname(__file__), \"pared.png\")\n",
    "        img_pared_original = pygame.image.load(ruta_pared).convert_alpha()\n",
    "        img_pared = pygame.transform.scale(img_pared_original, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        print(\"Advertencia: No se encontr贸 'pared.png'. Se usar谩 un cuadrado gris.\")\n",
    "        img_pared = pygame.Surface((CELL_SIZE, CELL_SIZE)); img_pared.fill(COLOR_PARED)\n",
    "\n",
    "    try:\n",
    "        ruta_agente = os.path.join(os.path.dirname(__file__), \"agente.png\")\n",
    "        img_agente_original = pygame.image.load(ruta_agente).convert_alpha()\n",
    "        img_agente = pygame.transform.scale(img_agente_original, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        print(\"Advertencia: No se encontr贸 'agente.png'. Se usar谩 un cuadrado azul.\")\n",
    "        img_agente = pygame.Surface((CELL_SIZE, CELL_SIZE)); img_agente.fill(COLOR_AGENTE)\n",
    "\n",
    "    # Inicializaci贸n del entorno y el agente.\n",
    "    entorno = EntornoGrid()\n",
    "    agente = AgenteQLearning(num_estados=(GRID_HEIGHT, GRID_WIDTH), num_acciones=4)\n",
    "\n",
    "    reloj = pygame.time.Clock()\n",
    "    corriendo = True\n",
    "    modo_juego = \"SETUP\"  # El juego comienza en modo de configuraci贸n.\n",
    "    cursor_pos = [0, 0]\n",
    "\n",
    "    # Guarda la configuraci贸n inicial del tablero para poder resetearlo.\n",
    "    frutas_iniciales = entorno.frutas.copy()\n",
    "    venenos_iniciales = entorno.venenos.copy()\n",
    "\n",
    "    # --- BUCLE PRINCIPAL DEL JUEGO ---\n",
    "    while corriendo:\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            # Manejo de eventos de teclado para cambiar de modo y configurar el tablero.\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # --- MODO ENTRENAMIENTO (T) ---\n",
    "                if evento.key == pygame.K_t:\n",
    "                    if modo_juego != \"TRAINING\":\n",
    "                        print(\"--- INICIANDO ENTRENAMIENTO ---\")\n",
    "                        modo_juego = \"TRAINING\"\n",
    "                        # Crea un nuevo agente con una Tabla Q vac铆a.\n",
    "                        agente = AgenteQLearning(num_estados=(GRID_HEIGHT, GRID_WIDTH), num_acciones=4)\n",
    "                        # Muestra un mensaje de \"Entrenando...\" en pantalla.\n",
    "                        pantalla.fill(COLOR_FONDO)\n",
    "                        font = pygame.font.Font(None, 50)\n",
    "                        texto_entrenando = font.render(\"Entrenando...\", True, COLOR_TEXTO)\n",
    "                        rect = texto_entrenando.get_rect(center=(SCREEN_WIDTH / 2, SCREEN_HEIGHT / 2))\n",
    "                        pantalla.blit(texto_entrenando, rect)\n",
    "                        pygame.display.flip()\n",
    "\n",
    "                        # Bucle de entrenamiento principal.\n",
    "                        for episodio in range(NUM_EPISODIOS_ENTRENAMIENTO):\n",
    "                            # Resetea el entorno a la configuraci贸n definida en modo SETUP.\n",
    "                            entorno.frutas = frutas_iniciales.copy()\n",
    "                            entorno.venenos = venenos_iniciales.copy()\n",
    "                            estado = entorno.reset_a_configuracion_inicial()\n",
    "                            terminado = False\n",
    "                            \n",
    "                            # Bucle de una partida (episodio).\n",
    "                            while not terminado:\n",
    "                                accion = agente.elegir_accion(estado)\n",
    "                                nuevo_estado, recompensa, terminado = entorno.step(accion, \"TRAINING\")\n",
    "                                agente.actualizar_q_table(estado, accion, recompensa, nuevo_estado)\n",
    "                                estado = nuevo_estado\n",
    "                            \n",
    "                            # Reduce epsilon al final de cada episodio.\n",
    "                            agente.decaimiento_epsilon()\n",
    "                            \n",
    "                            # Imprime el progreso cada 1000 episodios.\n",
    "                            if (episodio + 1) % 1000 == 0:\n",
    "                                print(f\"Episodio: {episodio + 1}/{NUM_EPISODIOS_ENTRENAMIENTO}, Epsilon: {agente.epsilon:.4f}\")\n",
    "\n",
    "                        print(\"--- ENTRENAMIENTO COMPLETADO ---\")\n",
    "                        # Prepara el tablero para la demostraci贸n del agente ya entrenado.\n",
    "                        entorno.frutas = frutas_iniciales.copy()\n",
    "                        entorno.venenos = venenos_iniciales.copy()\n",
    "                        entorno.reset_a_configuracion_inicial()\n",
    "                        agente.epsilon = 0  # Modo experto: solo explotaci贸n, sin acciones aleatorias.\n",
    "                        modo_juego = \"PLAYING\"\n",
    "\n",
    "                # --- MODO JUEGO (P) ---\n",
    "                elif evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO JUEGO (AGENTE ENTRENADO) ---\")\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    # Resetea el tablero a la configuraci贸n inicial.\n",
    "                    entorno.frutas = frutas_iniciales.copy()\n",
    "                    entorno.venenos = venenos_iniciales.copy()\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    agente.epsilon = 0 # El agente usar谩 su conocimiento sin explorar.\n",
    "\n",
    "                # --- MODO SETUP (S) ---\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # L贸gica para configurar el tablero en modo SETUP.\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    if evento.key == pygame.K_UP: cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN: cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT: cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT: cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "                    \n",
    "                    pos_celda = tuple(cursor_pos)\n",
    "                    # Tecla F: A帽ade o quita una fruta.\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        if pos_celda in entorno.frutas: entorno.frutas.remove(pos_celda)\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos_celda)\n",
    "                            entorno.venenos.discard(pos_celda); entorno.paredes.discard(pos_celda)\n",
    "                    # Tecla V: A帽ade o quita un veneno.\n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        if pos_celda in entorno.venenos: entorno.venenos.remove(pos_celda)\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos_celda)\n",
    "                            entorno.frutas.discard(pos_celda); entorno.paredes.discard(pos_celda)\n",
    "                    # Tecla W: A帽ade o quita una pared.\n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        if pos_celda in entorno.paredes: entorno.paredes.remove(pos_celda)\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos_celda)\n",
    "                            entorno.frutas.discard(pos_celda); entorno.venenos.discard(pos_celda)\n",
    "                    # Tecla C: Limpia el tablero.\n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        print(\"--- TABLERO LIMPIO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "                    \n",
    "                    # Actualiza la configuraci贸n inicial guardada cada vez que se hace un cambio.\n",
    "                    frutas_iniciales = entorno.frutas.copy()\n",
    "                    venenos_iniciales = entorno.venenos.copy()\n",
    "\n",
    "        # --- L贸gica de juego que se ejecuta en cada frame ---\n",
    "        # L贸gica del juego en modo PLAYING: el agente toma decisiones.\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            if entorno.frutas:\n",
    "                estado = entorno.agente_pos\n",
    "                accion = agente.elegir_accion(estado)\n",
    "                _, _, terminado = entorno.step(accion, \"PLAYING\")\n",
    "                if terminado:\n",
    "                    if not entorno.frutas:\n",
    "                        print(\"隆Todas las frutas recolectadas! Volviendo a modo SETUP.\")\n",
    "                    else:\n",
    "                        print(\"Juego terminado (veneno). Volviendo a modo SETUP.\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "                time.sleep(0.1) # Peque帽a pausa para ver el movimiento del agente.\n",
    "            else:\n",
    "                modo_juego = \"SETUP\"\n",
    "                print(\"No hay frutas en el tablero. Volviendo a modo SETUP.\")\n",
    "        \n",
    "        # Dibuja la pantalla en cada frame, excepto durante el entrenamiento.\n",
    "        if modo_juego != \"TRAINING\":\n",
    "            entorno.dibujar(pantalla, modo_juego, tuple(cursor_pos), img_fruta, img_veneno, img_pared, img_agente)\n",
    "            pygame.display.flip()\n",
    "\n",
    "        reloj.tick(60) # Limita el juego a 60 frames por segundo.\n",
    "\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto de entrada del programa.\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb0971",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff248b",
   "metadata": {},
   "source": [
    "#### agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c03c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\"\"\"\n",
    "Implementaci贸n del agente DQN (Deep Q-Network) con arquitectura CNN.\n",
    "\n",
    "Este m贸dulo contiene la implementaci贸n completa del algoritmo DQN, incluyendo:\n",
    "- Red neuronal convolucional para procesamiento de estados espaciales.\n",
    "- Sistema de memoria de replay para entrenamiento estable.\n",
    "- Estrategia epsilon-greedy para balancear exploraci贸n/explotaci贸n.\n",
    "- Red objetivo para estabilizar el c谩lculo de los valores Q.\n",
    "- Optimizaci贸n con el optimizador Adam.\n",
    "\n",
    "El agente est谩 dise帽ado espec铆ficamente para problemas de navegaci贸n en grillas\n",
    "donde el estado se representa como im谩genes multi-canal, aprovechando las\n",
    "capacidades de las CNNs para reconocer patrones espaciales.\n",
    "\n",
    "Caracter铆sticas principales:\n",
    "- Arquitectura CNN optimizada para grillas peque帽as.\n",
    "- Memoria de replay para descorrelacionar experiencias.\n",
    "- Actualizaci贸n peri贸dica de la red objetivo.\n",
    "- T茅cnicas de estabilizaci贸n (gradient clipping, target network).\n",
    "- Sistema de guardado/carga de modelos entrenados.\n",
    "\n",
    "Referencias:\n",
    "- DQN: Mnih et al. (2015) \"Human-level control through deep reinforcement learning\"\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. RED NEURONAL CONVOLUCIONAL PARA DQN ---\n",
    "class CNN_DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal convolucional optimizada para Q-learning en entornos de grilla.\n",
    "    \n",
    "    Esta arquitectura est谩 dise帽ada para procesar estados representados como\n",
    "    tensores 3D (canales x altura x anchura).\n",
    "    \n",
    "    Arquitectura:\n",
    "    1. **Capas Convolucionales**: Para extraer caracter铆sticas espaciales.\n",
    "       - Conv1: 3->16 canales.\n",
    "       - Conv2: 16->32 canales.\n",
    "    \n",
    "    2. **Capas Completamente Conectadas**: Para tomar decisiones basadas en las caracter铆sticas.\n",
    "       - FC1: 256 neuronas.\n",
    "       - FC2: Salida de valores Q para cada acci贸n.\n",
    "    \n",
    "    Args:\n",
    "        h (int): Altura de la grilla de entrada.\n",
    "        w (int): Anchura de la grilla de entrada.\n",
    "        outputs (int): N煤mero de acciones posibles.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(CNN_DQN, self).__init__()\n",
    "        \n",
    "        # --- CAPAS CONVOLUCIONALES ---\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # --- CLCULO DINMICO DEL TAMAO DE CARACTERSTICAS ---\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        # --- CAPAS COMPLETAMENTE CONECTADAS ---\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagaci贸n hacia adelante de la red.\n",
    "        \n",
    "        Procesa el estado de entrada a trav茅s de las capas para generar\n",
    "        valores Q para cada acci贸n posible.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Estado de entrada con forma (batch, 3, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Valores Q para cada acci贸n con forma (batch, num_actions).\n",
    "        \"\"\"\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    \n",
    "# --- 2. AGENTE DQN CON MEMORIA DE REPLAY Y RED OBJETIVO ---\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agente de aprendizaje por refuerzo que implementa el algoritmo DQN.\n",
    "    \n",
    "    Este agente combina varias t茅cnicas clave de deep reinforcement learning:\n",
    "    \n",
    "    **Componentes principales:**\n",
    "    1. **Red Principal**: Se entrena activamente y decide las acciones.\n",
    "    2. **Red Objetivo**: Una copia de la red principal que se actualiza lentamente,\n",
    "       proporcionando targets estables para el entrenamiento y reduciendo oscilaciones.\n",
    "    3. **Memoria de Replay**: Almacena experiencias para un aprendizaje m谩s estable.\n",
    "    4. **Estrategia Epsilon-Greedy**: Balancea entre explorar el entorno y explotar el conocimiento.\n",
    "    \n",
    "    Args:\n",
    "        state_shape (tuple): Forma del estado (canales, altura, anchura).\n",
    "        action_size (int): N煤mero de acciones posibles en el entorno.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_shape, action_size):\n",
    "        # --- CONFIGURACIN BSICA ---\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # --- MEMORIA DE REPLAY ---\n",
    "        # Almacena tuplas de (estado, acci贸n, recompensa, siguiente_estado, terminado).\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        \n",
    "        # --- HIPERPARMETROS DE APRENDIZAJE ---\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.learning_rate = 0.0001\n",
    "        self.update_target_every = 5\n",
    "        \n",
    "        # --- INICIALIZACIN DE REDES NEURONALES ---\n",
    "        h, w = state_shape[1], state_shape[2]\n",
    "        self.model = CNN_DQN(h, w, action_size)\n",
    "        self.target_model = CNN_DQN(h, w, action_size)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # --- CONFIGURACIN DE OPTIMIZACIN ---\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Actualiza la red objetivo copiando los pesos de la red principal.\n",
    "        \n",
    "        Esta operaci贸n es fundamental en DQN para mantener los targets estables\n",
    "        durante el entrenamiento, evitando que el objetivo cambie en cada paso.\n",
    "        \"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Almacena una experiencia en la memoria de replay.\n",
    "        \n",
    "        Esto permite al agente aprender de un conjunto de experiencias pasadas y\n",
    "        no correlacionadas, lo que estabiliza el entrenamiento.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado actual.\n",
    "            action (int): Acci贸n tomada.\n",
    "            reward (float): Recompensa recibida.\n",
    "            next_state (np.array): Estado resultante.\n",
    "            done (bool): True si el episodio termin贸.\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, explore=True):\n",
    "        \"\"\"\n",
    "        Selecciona una acci贸n usando la estrategia epsilon-greedy.\n",
    "        \n",
    "        - **Exploraci贸n**: Con probabilidad epsilon, elige una acci贸n al azar.\n",
    "        - **Explotaci贸n**: Con probabilidad 1-epsilon, elige la mejor acci贸n seg煤n la red.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado actual del entorno.\n",
    "            explore (bool): Permite la exploraci贸n. Poner en False para la demostraci贸n.\n",
    "        \n",
    "        Returns:\n",
    "            int: La acci贸n seleccionada.\n",
    "        \"\"\"\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state_tensor)\n",
    "        \n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Entrena la red neuronal usando un lote de experiencias de la memoria.\n",
    "        \n",
    "        Este es el n煤cleo del algoritmo de aprendizaje DQN.\n",
    "        \n",
    "        Proceso:\n",
    "        1. Muestrear un lote (batch) aleatorio de experiencias.\n",
    "        2. Calcular los valores Q actuales (predicciones) con la red principal.\n",
    "        3. Calcular los valores Q objetivo (targets) usando la red objetivo.\n",
    "        4. Optimizar la red principal para minimizar la diferencia entre predicciones y targets.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): N煤mero de experiencias a usar para el entrenamiento.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array([e[0] for e in minibatch]))\n",
    "        actions = torch.LongTensor([e[1] for e in minibatch]).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor([e[2] for e in minibatch]).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(np.array([e[3] for e in minibatch]))\n",
    "        dones = torch.BoolTensor([e[4] for e in minibatch]).unsqueeze(1)\n",
    "\n",
    "        current_q_values = self.model(states).gather(1, actions)\n",
    "        \n",
    "        # --- CLCULO DEL TARGET SEGN DQN ---\n",
    "        # La red objetivo calcula el valor m谩ximo del siguiente estado.\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # Ecuaci贸n de Bellman para el target: R + gamma * max_Q(s', a')\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "        \n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping para prevenir gradientes explosivos y estabilizar.\n",
    "        torch.nn.utils.clip_grad_value_(self.model.parameters(), 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decaimiento de epsilon para reducir la exploraci贸n.\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def load(self, name):\n",
    "        \"\"\"\n",
    "        Carga los pesos de un modelo entrenado desde un archivo.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Ruta al archivo de pesos del modelo (.pth).\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "        self.update_target_network()\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Guarda los pesos del modelo actual en un archivo.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Ruta donde se guardar谩 el archivo de pesos (.pth).\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c56236",
   "metadata": {},
   "source": [
    "#### dqn_agente_comefrutas.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn_agente_comefrutas.py\n",
    "\"\"\"\n",
    "Interfaz gr谩fica de demostraci贸n para agente DQN (Deep Q-Network).\n",
    "\n",
    "Este m贸dulo proporciona una interfaz visual interactiva para demostrar el\n",
    "comportamiento de un agente DQN entrenado en el problema de recolecci贸n de frutas.\n",
    "A diferencia del DDQN, esta implementaci贸n utiliza DQN cl谩sico con una sola red.\n",
    "\n",
    "Caracter铆sticas principales:\n",
    "- Interfaz de configuraci贸n interactiva para crear escenarios personalizados\n",
    "- Visualizaci贸n en tiempo real del comportamiento del agente entrenado\n",
    "- Sistema de dos modos: configuraci贸n (SETUP) y ejecuci贸n (PLAYING)\n",
    "- Compatibilidad con modelos DQN preentrenados\n",
    "- Interfaz de usuario intuitiva con controles de teclado y mouse\n",
    "\n",
    "El sistema est谩 dise帽ado para:\n",
    "- Demostraciones educativas del comportamiento de IA\n",
    "- Validaci贸n visual del rendimiento del agente\n",
    "- Experimentaci贸n r谩pida con diferentes configuraciones de entorno\n",
    "- Evaluaci贸n cualitativa de estrategias aprendidas\n",
    "\n",
    "Diferencias con DDQN:\n",
    "- Utiliza una sola red neuronal (no red objetivo separada)\n",
    "- Implementaci贸n m谩s simple del algoritmo Q-learning\n",
    "- Compatible con modelos entrenados usando DQN cl谩sico\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from agent import Agent\n",
    "\n",
    "# --- CONFIGURACIN DE LA INTERFAZ VISUAL ---\n",
    "\"\"\"Par谩metros de configuraci贸n para la ventana y visualizaci贸n.\"\"\"\n",
    "GRID_WIDTH = 5          # Ancho de la grilla en n煤mero de celdas\n",
    "GRID_HEIGHT = 5         # Alto de la grilla en n煤mero de celdas\n",
    "CELL_SIZE = 120         # Tama帽o de cada celda en p铆xeles (120x120)\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la ventana (600px)\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto total de la ventana (600px)\n",
    "\n",
    "# --- ESQUEMA DE COLORES ---\n",
    "\"\"\"Paleta de colores para una interfaz moderna y legible.\"\"\"\n",
    "COLOR_FONDO = (25, 25, 25)        # Fondo oscuro para reducir fatiga visual\n",
    "COLOR_LINEAS = (40, 40, 40)       # L铆neas de grilla sutiles\n",
    "COLOR_CURSOR = (255, 255, 0)      # Cursor amarillo brillante para visibilidad\n",
    "COLOR_TEXTO = (230, 230, 230)     # Texto claro sobre fondo oscuro\n",
    "\n",
    "\n",
    "class EntornoGrid:\n",
    "    \"\"\"\n",
    "    Entorno de grilla especializado para demostraci贸n de agentes DQN.\n",
    "    \n",
    "    Esta clase maneja tanto la l贸gica del entorno como su representaci贸n visual,\n",
    "    proporcionando una plataforma completa para demostrar el comportamiento de\n",
    "    agentes DQN entrenados en problemas de navegaci贸n y recolecci贸n.\n",
    "    \n",
    "    Caracter铆sticas del entorno:\n",
    "    - Grilla bidimensional con elementos configurables\n",
    "    - Gesti贸n de colisiones y l铆mites\n",
    "    - Sistema de recompensas integrado\n",
    "    - Representaci贸n visual con Pygame\n",
    "    - Compatibilidad con formato de estado DQN (tensor 3D)\n",
    "    \n",
    "    Elementos del entorno:\n",
    "    - Agente: Entidad controlada por IA que debe recolectar frutas\n",
    "    - Frutas: Objetivos que otorgan recompensas positivas\n",
    "    - Venenos: Obst谩culos que causan penalizaciones y reseteo\n",
    "    - Paredes: Barreras f铆sicas que bloquean el movimiento\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tama帽o de la grilla (siempre cuadrada)\n",
    "        agent_pos (tuple): Posici贸n actual del agente (fila, columna)\n",
    "        frutas (set): Conjunto de posiciones que contienen frutas\n",
    "        venenos (set): Conjunto de posiciones que contienen venenos\n",
    "        paredes (set): Conjunto de posiciones que contienen paredes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuraci贸n por defecto.\n",
    "        \n",
    "        El entorno comienza con:\n",
    "        - Agente en posici贸n (0,0) - esquina superior izquierda\n",
    "        - Todos los conjuntos de elementos vac铆os\n",
    "        - Tama帽o de grilla determinado por GRID_WIDTH\n",
    "        \"\"\"\n",
    "        self.size = GRID_WIDTH\n",
    "        self.agent_pos = (0, 0)  # Posici贸n inicial est谩ndar\n",
    "        self.frutas = set()      # Conjunto vac铆o inicialmente\n",
    "        self.venenos = set()     # Conjunto vac铆o inicialmente\n",
    "        self.paredes = set()     # Conjunto vac铆o inicialmente\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"\n",
    "        Reinicia solo la posici贸n del agente sin modificar el entorno.\n",
    "        \n",
    "        Esta funci贸n es 煤til para comenzar nuevos episodios manteniendo\n",
    "        la misma configuraci贸n de elementos (frutas, venenos, paredes)\n",
    "        establecida durante el modo setup.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno despu茅s del reset\n",
    "        \"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno excepto el agente.\n",
    "        \n",
    "        Funci贸n de utilidad para limpiar completamente el entorno\n",
    "        y comenzar una nueva configuraci贸n desde cero. til en\n",
    "        modo setup para crear nuevos escenarios r谩pidamente.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acci贸n del agente y actualiza el estado del entorno.\n",
    "        \n",
    "        Este m茅todo implementa la l贸gica principal del entorno, incluyendo:\n",
    "        - Procesamiento de movimientos del agente\n",
    "        - Detecci贸n de colisiones con paredes y l铆mites\n",
    "        - C谩lculo de recompensas seg煤n las interacciones\n",
    "        - Gesti贸n de condiciones de terminaci贸n\n",
    "        - Manejo especial de venenos (penalizaci贸n + reset)\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Acci贸n a ejecutar por el agente\n",
    "                0: Mover hacia arriba (fila-1)\n",
    "                1: Mover hacia abajo (fila+1)\n",
    "                2: Mover hacia la izquierda (columna-1)\n",
    "                3: Mover hacia la derecha (columna+1)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, episodio_terminado)\n",
    "                - nuevo_estado (np.array): Estado resultante\n",
    "                - recompensa (float): Recompensa obtenida\n",
    "                - episodio_terminado (bool): True si complet贸 o fall贸\n",
    "        \n",
    "        Sistema de recompensas:\n",
    "            - Colisi贸n con pared/l铆mite: -0.1 (movimiento inv谩lido)\n",
    "            - Movimiento v谩lido: -0.05 (costo de vida)\n",
    "            - Tocar veneno: -10.0 (penalizaci贸n severa + reset a origen)\n",
    "            - Recolectar fruta: +1.0 (recompensa por objetivo)\n",
    "            - Completar nivel: +10.0 adicional (todas las frutas recolectadas)\n",
    "        \"\"\"\n",
    "        # Obtener posici贸n actual del agente\n",
    "        fila, col = self.agent_pos\n",
    "        \n",
    "        # Calcular nueva posici贸n seg煤n la acci贸n\n",
    "        if accion == 0:      # Arriba\n",
    "            fila -= 1\n",
    "        elif accion == 1:    # Abajo\n",
    "            fila += 1\n",
    "        elif accion == 2:    # Izquierda\n",
    "            col -= 1\n",
    "        elif accion == 3:    # Derecha\n",
    "            col += 1\n",
    "\n",
    "        # Verificar colisiones con l铆mites de grilla o paredes\n",
    "        if (\n",
    "            fila < 0                        # L铆mite superior\n",
    "            or fila >= GRID_HEIGHT          # L铆mite inferior\n",
    "            or col < 0                      # L铆mite izquierdo\n",
    "            or col >= GRID_WIDTH            # L铆mite derecho\n",
    "            or (fila, col) in self.paredes  # Colisi贸n con pared\n",
    "        ):\n",
    "            # Movimiento inv谩lido: penalizaci贸n menor, mantener posici贸n\n",
    "            return self.get_state(), -0.1, False\n",
    "        \n",
    "        # Movimiento v谩lido: actualizar posici贸n\n",
    "        x, y = fila, col\n",
    "        self.agent_pos = (x, y)\n",
    "        recompensa = -0.05  # Costo base por movimiento (fomenta eficiencia)\n",
    "        terminado = False\n",
    "\n",
    "        # Procesar interacciones con elementos del entorno\n",
    "        if self.agent_pos in self.venenos:\n",
    "            # Penalizaci贸n por tocar veneno y reset a posici贸n inicial\n",
    "            recompensa = -10.0\n",
    "            self.agent_pos = (0, 0)  # Reset autom谩tico a origen\n",
    "            \n",
    "        elif self.agent_pos in self.frutas:\n",
    "            # Recompensa por recolectar fruta\n",
    "            recompensa = 1.0\n",
    "            self.frutas.remove(self.agent_pos)  # Eliminar fruta recolectada\n",
    "            \n",
    "            # Verificar si se complet贸 el nivel\n",
    "            if not self.frutas:  # No quedan frutas\n",
    "                recompensa += 10.0   # Bonus por completar\n",
    "                terminado = True     # Episodio exitoso\n",
    "                self.agent_pos = (0, 0)  # Reset para pr贸ximo episodio\n",
    "\n",
    "        return self.get_state(), recompensa, terminado\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Convierte el estado actual del entorno a formato tensor para DQN.\n",
    "        \n",
    "        Esta funci贸n es crucial para la compatibilidad con redes neuronales\n",
    "        convolucionales, transformando la representaci贸n discreta del entorno\n",
    "        en un tensor 3D que puede ser procesado eficientemente por la CNN.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Tensor 3D con forma (3, size, size) donde:\n",
    "                - Canal 0: Posici贸n del agente (1.0 donde est谩, 0.0 resto)\n",
    "                - Canal 1: Posiciones de frutas (1.0 donde hay frutas)\n",
    "                - Canal 2: Posiciones de venenos (1.0 donde hay venenos)\n",
    "        \n",
    "        Caracter铆sticas del formato:\n",
    "        - Tipo float32 para compatibilidad con PyTorch\n",
    "        - Representaci贸n binaria (0.0 o 1.0) para claridad\n",
    "        - Canales separados permiten que la CNN detecte patrones espec铆ficos\n",
    "        - Dimensiones compatibles con arquitectura Conv2D\n",
    "        \n",
    "        Nota: Las paredes no se incluyen en el estado ya que son est谩ticas\n",
    "              y el agente las aprende a trav茅s de las restricciones de movimiento.\n",
    "        \"\"\"\n",
    "        # Inicializar tensor de estado con ceros\n",
    "        estado = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posici贸n del agente\n",
    "        estado[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruta in self.frutas:\n",
    "            estado[1, fruta[0], fruta[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de venenos\n",
    "        for veneno in self.venenos:\n",
    "            estado[2, veneno[0], veneno[1]] = 1.0\n",
    "            \n",
    "        return estado\n",
    "\n",
    "    def dibujar(\n",
    "        self,\n",
    "        pantalla,\n",
    "        modo_juego,\n",
    "        cursor_pos,\n",
    "        img_fruta,\n",
    "        img_veneno,\n",
    "        img_pared,\n",
    "        img_agente,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Renderiza el entorno completo en la pantalla usando Pygame.\n",
    "        \n",
    "        Esta funci贸n maneja toda la visualizaci贸n del entorno, incluyendo\n",
    "        elementos del juego, interfaz de usuario y informaci贸n contextual.\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde renderizar\n",
    "            modo_juego (str): Modo actual (\"SETUP\" o \"PLAYING\")\n",
    "            cursor_pos (tuple): Posici贸n del cursor en modo setup\n",
    "            img_fruta (pygame.Surface): Imagen para representar frutas\n",
    "            img_veneno (pygame.Surface): Imagen para representar venenos\n",
    "            img_pared (pygame.Surface): Imagen para representar paredes\n",
    "            img_agente (pygame.Surface): Imagen para representar al agente\n",
    "        \n",
    "        Proceso de renderizado:\n",
    "        1. Limpiar pantalla con color de fondo\n",
    "        2. Dibujar grilla de referencia\n",
    "        3. Renderizar elementos por capas (paredes  frutas  venenos  agente)\n",
    "        4. Mostrar cursor en modo setup\n",
    "        5. Renderizar informaci贸n de controles y estado\n",
    "        \n",
    "        El orden de renderizado es importante para la superposici贸n correcta\n",
    "        de elementos visuales y la legibilidad de la interfaz.\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar grilla de referencia\n",
    "        # L铆neas verticales\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        # L铆neas horizontales\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Renderizar elementos del entorno (orden de capas importante)\n",
    "        # 1. Paredes (fondo) - obst谩culos est谩ticos\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "            \n",
    "        # 2. Frutas (objetivos) - elementos a recolectar\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "            \n",
    "        # 3. Venenos (peligros) - elementos a evitar\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "\n",
    "        # 4. Agente (primer plano) - jugador controlado por IA\n",
    "        pantalla.blit(\n",
    "            img_agente, (self.agent_pos[0] * CELL_SIZE, self.agent_pos[1] * CELL_SIZE)\n",
    "        )\n",
    "\n",
    "        # 5. Cursor de selecci贸n (solo en modo setup)\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar informaci贸n textual de la interfaz\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        \n",
    "        # Mostrar modo actual\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Instrucciones para modo setup\n",
    "        controles1 = font.render(\n",
    "            \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\", True, COLOR_TEXTO\n",
    "        )\n",
    "        \n",
    "        # Controles generales\n",
    "        controles2 = font.render(\"P=Jugar, S=Setup\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Posicionar textos en la parte inferior de la pantalla\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles1, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles2, (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal que ejecuta la interfaz de demostraci贸n DQN.\n",
    "    \n",
    "    Esta funci贸n implementa un sistema completo de demostraci贸n interactiva\n",
    "    que permite a los usuarios configurar entornos personalizados y observar\n",
    "    el comportamiento de un agente DQN entrenado.\n",
    "    \n",
    "    Flujo de la aplicaci贸n:\n",
    "    1. Inicializaci贸n de Pygame y carga de recursos visuales\n",
    "    2. Carga del agente DQN preentrenado\n",
    "    3. Bucle principal con dos modos de operaci贸n:\n",
    "       - SETUP: Configuraci贸n interactiva del entorno\n",
    "       - PLAYING: Demostraci贸n del agente entrenado\n",
    "    4. Renderizado continuo y gesti贸n de eventos\n",
    "    \n",
    "    Modos de operaci贸n:\n",
    "    \n",
    "    **MODO SETUP (Configuraci贸n):**\n",
    "    - Navegaci贸n con flechas del teclado\n",
    "    - F: A帽adir/quitar frutas en posici贸n del cursor\n",
    "    - V: A帽adir/quitar venenos en posici贸n del cursor\n",
    "    - W: A帽adir/quitar paredes en posici贸n del cursor\n",
    "    - C: Limpiar completamente el entorno\n",
    "    \n",
    "    **MODO PLAYING (Demostraci贸n):**\n",
    "    - El agente DQN toma control autom谩tico\n",
    "    - Visualizaci贸n en tiempo real de decisiones\n",
    "    - Finalizaci贸n autom谩tica y retorno a setup\n",
    "    \n",
    "    **Controles Globales:**\n",
    "    - P: Cambiar a modo PLAYING\n",
    "    - S: Cambiar a modo SETUP\n",
    "    - ESC/X: Salir de la aplicaci贸n\n",
    "    \"\"\"\n",
    "    # Inicializar sistema gr谩fico Pygame\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente DQN - Come Frutas \")\n",
    "\n",
    "    def cargar_img(nombre, color_fallback):\n",
    "        \"\"\"\n",
    "        Funci贸n auxiliar para cargar im谩genes con respaldo de color.\n",
    "        \n",
    "        Intenta cargar una imagen desde archivo y, si falla, crea una\n",
    "        superficie de color s贸lido como alternativa. Esto garantiza que\n",
    "        la aplicaci贸n funcione incluso sin los archivos de imagen.\n",
    "        \n",
    "        Args:\n",
    "            nombre (str): Nombre/ruta del archivo de imagen\n",
    "            color_fallback (tuple): Color RGB de respaldo (r, g, b)\n",
    "            \n",
    "        Returns:\n",
    "            pygame.Surface: Superficie escalada al tama帽o de celda\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "            img = pygame.image.load(ruta).convert_alpha()\n",
    "            return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "        except:\n",
    "            # Crear superficie de color s贸lido como respaldo\n",
    "            surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "            surf.fill(color_fallback)\n",
    "            return surf\n",
    "\n",
    "    # Cargar recursos visuales con colores de respaldo\n",
    "    img_fruta = cargar_img(\"../fruta.png\", (0, 255, 0))      # Verde si falla\n",
    "    img_veneno = cargar_img(\"../veneno.png\", (255, 0, 0))     # Rojo si falla\n",
    "    img_pared = cargar_img(\"../pared.png\", (100, 100, 100))   # Gris si falla\n",
    "    img_agente = cargar_img(\"../agente.png\", (0, 0, 255))     # Azul si falla\n",
    "\n",
    "    # Inicializar componentes principales\n",
    "    entorno = EntornoGrid()                                    # Entorno de simulaci贸n\n",
    "    agente = Agent(state_shape=(3, GRID_HEIGHT, GRID_WIDTH), action_size=4)  # Agente DQN\n",
    "    agente.load(\"DQN/dqn_model.pth\")                          # Cargar modelo preentrenado\n",
    "\n",
    "    # Variables de control de la interfaz\n",
    "    cursor_pos = [0, 0]        # Posici贸n del cursor en modo setup\n",
    "    modo_juego = \"SETUP\"       # Modo inicial\n",
    "    reloj = pygame.time.Clock()  # Control de framerate\n",
    "    corriendo = True           # Flag principal del bucle\n",
    "\n",
    "    # Bucle principal de la aplicaci贸n\n",
    "    while corriendo:\n",
    "        # Procesar eventos del usuario\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # --- CONTROLES GLOBALES ---\n",
    "                if evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO JUEGO ---\")\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    time.sleep(0.5)  # Pausa para evitar acciones inmediatas\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # --- CONTROLES ESPECFICOS DEL MODO SETUP ---\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Navegaci贸n del cursor con flechas del teclado\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Obtener posici贸n actual del cursor\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    \n",
    "                    # Gesti贸n de elementos en la posici贸n del cursor\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        # Alternar fruta en posici贸n actual\n",
    "                        if pos in entorno.frutas:\n",
    "                            entorno.frutas.remove(pos)\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posici贸n\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        # Alternar veneno en posici贸n actual\n",
    "                        if pos in entorno.venenos:\n",
    "                            entorno.venenos.remove(pos)\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posici贸n\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        # Alternar pared en posici贸n actual\n",
    "                        if pos in entorno.paredes:\n",
    "                            entorno.paredes.remove(pos)\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posici贸n\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        # Limpiar completamente el entorno\n",
    "                        print(\"--- LIMPIANDO ENTORNO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "\n",
    "        # --- LGICA DEL MODO PLAYING ---\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # Obtener estado actual del entorno\n",
    "            estado = entorno.get_state()\n",
    "            \n",
    "            # El agente DQN elige la mejor acci贸n (sin exploraci贸n)\n",
    "            # explore=False garantiza que use solo la pol铆tica aprendida\n",
    "            accion = agente.choose_action(estado, explore=False)\n",
    "            \n",
    "            # Ejecutar la acci贸n en el entorno\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            \n",
    "            # Verificar si el episodio termin贸\n",
    "            if terminado:\n",
    "                print(\"Juego terminado. Volviendo a SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "                \n",
    "            # Control de velocidad para observaci贸n humana\n",
    "            time.sleep(0.1)  # 10 FPS para visualizaci贸n clara\n",
    "\n",
    "        # --- SISTEMA DE RENDERIZADO ---\n",
    "        # Crear superficie temporal para composici贸n\n",
    "        pantalla_con_info = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "        pantalla_con_info.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Renderizar el entorno completo en la superficie temporal\n",
    "        entorno.dibujar(\n",
    "            pantalla_con_info,\n",
    "            modo_juego,\n",
    "            tuple(cursor_pos),  # Convertir lista a tupla\n",
    "            img_fruta,\n",
    "            img_veneno,\n",
    "            img_pared,\n",
    "            img_agente,\n",
    "        )\n",
    "        \n",
    "        # Transferir superficie temporal a pantalla principal\n",
    "        pantalla.blit(pantalla_con_info, (0, 0))\n",
    "        \n",
    "        # Actualizar pantalla y controlar framerate\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(60)  # Limitar a 60 FPS para suavidad\n",
    "\n",
    "    # Limpiar recursos al salir de la aplicaci贸n\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa de demostraci贸n DQN.\n",
    "    \n",
    "    Ejecuta la funci贸n main() cuando el archivo se ejecuta directamente.\n",
    "    Este patr贸n permite importar clases y funciones de este m贸dulo sin\n",
    "    ejecutar autom谩ticamente la interfaz de demostraci贸n.\n",
    "    \n",
    "    Uso t铆pico:\n",
    "        python dqn_agente_comefrutas.py  # Ejecuta la demostraci贸n\n",
    "        \n",
    "    La aplicaci贸n est谩 dise帽ada para:\n",
    "    - Demostraciones educativas de algoritmos DQN\n",
    "    - Validaci贸n visual del comportamiento del agente\n",
    "    - Experimentaci贸n r谩pida con configuraciones de entorno\n",
    "    - Presentaciones de proyectos de IA/ML\n",
    "    \n",
    "    Diferencias con versi贸n DDQN:\n",
    "    - Utiliza algoritmo DQN cl谩sico (una sola red)\n",
    "    - Compatible con modelos entrenados con DQN simple\n",
    "    - Interfaz id茅ntica pero agente subyacente diferente\n",
    "    \"\"\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70047265",
   "metadata": {},
   "source": [
    "#### enviroment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645cbfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment.py\n",
    "\"\"\"\n",
    "Entorno de cuadr铆cula para el entrenamiento de un agente DQN.\n",
    "\n",
    "Este m贸dulo implementa un entorno de juego donde un agente debe navegar\n",
    "por una cuadr铆cula para recoger frutas mientras evita venenos. El entorno\n",
    "utiliza reward shaping para guiar al agente hacia las frutas.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GridEnvironment:\n",
    "    \"\"\"\n",
    "    Entorno de cuadr铆cula para un agente que debe recoger frutas y evitar venenos.\n",
    "    \n",
    "    El entorno consiste en una cuadr铆cula de tama帽o configurable donde:\n",
    "    - El agente se mueve en 4 direcciones (arriba, abajo, izquierda, derecha)\n",
    "    - Las frutas proporcionan recompensas positivas\n",
    "    - Los venenos proporcionan recompensas negativas y terminan el juego\n",
    "    - El objetivo es recoger todas las frutas sin tocar venenos\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tama帽o de la cuadr铆cula (size x size)\n",
    "        agent_pos (np.array): Posici贸n actual del agente [fila, columna]\n",
    "        fruit_pos (list): Lista de posiciones de frutas\n",
    "        poison_pos (list): Lista de posiciones de venenos\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de cuadr铆cula.\n",
    "        \n",
    "        Args:\n",
    "            size (int, optional): Tama帽o de la cuadr铆cula. Por defecto es 5x5.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, agent_pos=(0, 0), fruit_pos=[], poison_pos=[]):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno con una configuraci贸n espec铆fica.\n",
    "        \n",
    "        Establece las posiciones iniciales del agente, frutas y venenos.\n",
    "        Si no se proporcionan posiciones, se usan listas vac铆as para frutas y venenos.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (tuple, optional): Posici贸n inicial del agente (fila, columna). \n",
    "                                       Por defecto (0, 0).\n",
    "            fruit_pos (list, optional): Lista de tuplas con posiciones de frutas.\n",
    "                                       Por defecto lista vac铆a.\n",
    "            poison_pos (list, optional): Lista de tuplas con posiciones de venenos.\n",
    "                                        Por defecto lista vac铆a.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno como array 3D (3, size, size).\n",
    "        \"\"\"\n",
    "        self.agent_pos = np.array(agent_pos)\n",
    "        self.fruit_pos = [np.array(p) for p in fruit_pos]\n",
    "        self.poison_pos = [np.array(p) for p in poison_pos]\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera la representaci贸n del estado actual del entorno.\n",
    "        \n",
    "        El estado se representa como una \"imagen\" de 3 canales que puede ser\n",
    "        procesada por una CNN. Cada canal representa un tipo de elemento:\n",
    "        \n",
    "        - Canal 0: Posici贸n del agente (1.0 donde est谩 el agente, 0.0 en el resto)\n",
    "        - Canal 1: Posiciones de frutas (1.0 donde hay frutas, 0.0 en el resto)\n",
    "        - Canal 2: Posiciones de venenos (1.0 donde hay venenos, 0.0 en el resto)\n",
    "        \n",
    "        Esta representaci贸n permite que el agente \"vea\" todo el entorno de una vez\n",
    "        y facilita el procesamiento por redes neuronales convolucionales.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado del entorno como array 3D de forma (3, size, size)\n",
    "                     con valores float32.\n",
    "        \"\"\"\n",
    "        state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posici贸n del agente\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de las frutas\n",
    "        for fruit in self.fruit_pos:\n",
    "            state[1, fruit[0], fruit[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de los venenos\n",
    "        for poison in self.poison_pos:\n",
    "            state[2, poison[0], poison[1]] = 1.0\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Ejecuta una acci贸n en el entorno y retorna el resultado.\n",
    "        \n",
    "        Esta funci贸n implementa la l贸gica principal del juego, incluyendo:\n",
    "        1. Movimiento del agente\n",
    "        2. C谩lculo de recompensas con reward shaping\n",
    "        3. Detecci贸n de colisiones con frutas y venenos\n",
    "        4. Determinaci贸n de condiciones de terminaci贸n\n",
    "        \n",
    "        El sistema de recompensas incluye:\n",
    "        - Recompensa por acercarse a frutas (+0.1)\n",
    "        - Castigo por alejarse de frutas (-0.15)\n",
    "        - Recompensa por recoger frutas (+1.0)\n",
    "        - Castigo por tocar veneno (-1.0, termina el juego)\n",
    "        - Recompensa por completar el nivel (+5.0)\n",
    "        - Castigo base por movimiento (-0.05, fomenta eficiencia)\n",
    "        \n",
    "        Args:\n",
    "            action (int): Acci贸n a realizar:\n",
    "                         0 = Arriba (decrementar fila)\n",
    "                         1 = Abajo (incrementar fila)\n",
    "                         2 = Izquierda (decrementar columna)\n",
    "                         3 = Derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, terminado)\n",
    "                - nuevo_estado (np.array): Estado del entorno despu茅s de la acci贸n\n",
    "                - recompensa (float): Recompensa obtenida por la acci贸n\n",
    "                - terminado (bool): True si el episodio ha terminado\n",
    "        \"\"\"\n",
    "        \n",
    "        # FASE 1: REWARD SHAPING - Calcular distancia a fruta m谩s cercana ANTES del movimiento\n",
    "        # Esto permite dar recompensas por acercarse/alejarse de las frutas\n",
    "        old_dist_to_fruit = float('inf')\n",
    "        if self.fruit_pos:\n",
    "            distances = [np.linalg.norm(self.agent_pos - fruit) for fruit in self.fruit_pos]\n",
    "            old_dist_to_fruit = min(distances)\n",
    "\n",
    "        \n",
    "        # FASE 2: MOVIMIENTO DEL AGENTE\n",
    "        # Actualizar la posici贸n del agente basada en la acci贸n seleccionada\n",
    "        if action == 0:      # Arriba\n",
    "            self.agent_pos[0] -= 1\n",
    "        elif action == 1:    # Abajo\n",
    "            self.agent_pos[0] += 1\n",
    "        elif action == 2:    # Izquierda\n",
    "            self.agent_pos[1] -= 1\n",
    "        elif action == 3:    # Derecha\n",
    "            self.agent_pos[1] += 1\n",
    "\n",
    "        # Limitar la posici贸n del agente a los l铆mites del tablero\n",
    "        # np.clip asegura que las coordenadas est茅n entre 0 y (size-1)\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.size - 1)\n",
    "\n",
    "        \n",
    "        # FASE 3: CLCULO DE RECOMPENSAS\n",
    "        \n",
    "        # Recompensa base: peque帽o castigo por cada movimiento para fomentar eficiencia\n",
    "        reward = -0.05  \n",
    "        done = False\n",
    "\n",
    "        # REWARD SHAPING: Calcular nueva distancia y recompensar acercamiento a frutas\n",
    "        # Esto ayuda al agente a aprender a navegar hacia las frutas incluso antes de alcanzarlas\n",
    "        new_dist_to_fruit = float('inf')\n",
    "        if self.fruit_pos:\n",
    "            distances = [np.linalg.norm(self.agent_pos - fruit) for fruit in self.fruit_pos]\n",
    "            new_dist_to_fruit = min(distances)\n",
    "\n",
    "            # Recompensar por acercarse, castigar por alejarse\n",
    "            if new_dist_to_fruit < old_dist_to_fruit:\n",
    "                reward += 0.1   # Recompensa por acercarse a una fruta\n",
    "            else:\n",
    "                reward -= 0.15  # Castigo por alejarse (ligeramente mayor para evitar indecisi贸n)\n",
    "\n",
    "        \n",
    "        # FASE 4: DETECCIN DE EVENTOS\n",
    "        \n",
    "        # Verificar si el agente recogi贸 una fruta\n",
    "        for i, fruit in enumerate(self.fruit_pos):\n",
    "            if np.array_equal(self.agent_pos, fruit):\n",
    "                reward += 1.0  # Gran recompensa por recoger fruta\n",
    "                self.fruit_pos.pop(i)  # Remover la fruta del entorno\n",
    "                break  # Solo puede recoger una fruta por paso\n",
    "        \n",
    "        # Verificar si el agente toc贸 veneno (termina el juego)\n",
    "        if any(np.array_equal(self.agent_pos, poison) for poison in self.poison_pos):\n",
    "            reward = -1.0  # Castigo severo y absoluto por tocar veneno\n",
    "            done = True    # Terminar el episodio inmediatamente\n",
    "\n",
    "        # Verificar condici贸n de victoria: no quedan frutas\n",
    "        if not self.fruit_pos:\n",
    "            done = True\n",
    "            reward += 5.0  # Gran recompensa bonus por completar el objetivo\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d7e91",
   "metadata": {},
   "source": [
    "#### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628abc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\"\"\"\n",
    "Interfaz gr谩fica interactiva para visualizar un agente DQN entrenado.\n",
    "\n",
    "Este m贸dulo implementa una aplicaci贸n Pygame que permite:\n",
    "1. Configurar un escenario colocando frutas y venenos manualmente\n",
    "2. Observar c贸mo el agente DQN entrenado resuelve el escenario\n",
    "3. Reiniciar para probar diferentes configuraciones\n",
    "\n",
    "La aplicaci贸n tiene dos modos:\n",
    "- Modo Setup: El usuario coloca elementos en la cuadr铆cula\n",
    "- Modo Run: El agente toma control y ejecuta su pol铆tica aprendida\n",
    "\n",
    "Controles:\n",
    "- Click izquierdo: Colocar fruta\n",
    "- Click derecho: Colocar veneno  \n",
    "- Espacio: Iniciar simulaci贸n del agente\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "from environment import GridEnvironment\n",
    "from agent import Agent\n",
    "\n",
    "# CONFIGURACIN DE PYGAME Y CONSTANTES DEL JUEGO\n",
    "\"\"\"\n",
    "Configuraci贸n visual y dimensiones de la aplicaci贸n.\n",
    "\"\"\"\n",
    "GRID_SIZE = 5        # Tama帽o de la cuadr铆cula (5x5)\n",
    "CELL_SIZE = 100      # Tama帽o de cada celda en p铆xeles\n",
    "WIDTH, HEIGHT = GRID_SIZE * CELL_SIZE, GRID_SIZE * CELL_SIZE  # Ventana de 500x500 p铆xeles\n",
    "WIN = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Agente Come-Frutas\")\n",
    "pygame.font.init()   # Inicializar fuentes para texto si se necesita\n",
    "\n",
    "# INICIALIZACIN DEL AGENTE DQN ENTRENADO\n",
    "\"\"\"\n",
    "Carga el agente DQN previamente entrenado desde archivo.\n",
    "El agente utilizar谩 su pol铆tica aprendida para navegar por el entorno.\n",
    "\"\"\"\n",
    "env = GridEnvironment(size=GRID_SIZE)\n",
    "action_size = 4  # 4 acciones posibles: arriba, abajo, izquierda, derecha\n",
    "state_shape = (3, GRID_SIZE, GRID_SIZE)  # Forma del estado: 3 canales x 5x5 grid\n",
    "agent = Agent(state_shape, action_size)  # Crear instancia del agente\n",
    "agent.load(\"dqn_model.pth\")              # Cargar pesos del modelo entrenado\n",
    "\n",
    "# DEFINICIN DE COLORES\n",
    "\"\"\"\n",
    "Paleta de colores para los elementos visuales del juego.\n",
    "Utiliza sistema RGB (Red, Green, Blue) con valores 0-255.\n",
    "\"\"\"\n",
    "COLOR_GRID = (200, 200, 200)   # Gris claro para las l铆neas de la cuadr铆cula\n",
    "COLOR_AGENT = (0, 0, 255)      # Azul para el agente\n",
    "COLOR_FRUIT = (0, 255, 0)      # Verde para las frutas\n",
    "COLOR_POISON = (255, 0, 0)     # Rojo para los venenos\n",
    "\n",
    "def draw_grid():\n",
    "    \"\"\"\n",
    "    Dibuja las l铆neas de la cuadr铆cula en la ventana.\n",
    "    \n",
    "    Crea una cuadr铆cula visual de 5x5 dibujando l铆neas verticales y horizontales\n",
    "    separadas por CELL_SIZE p铆xeles. Esto ayuda a visualizar las celdas donde\n",
    "    se pueden colocar elementos y donde se mueve el agente.\n",
    "    \"\"\"\n",
    "    # L铆neas verticales\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        pygame.draw.line(WIN, COLOR_GRID, (x, 0), (x, HEIGHT))\n",
    "    # L铆neas horizontales  \n",
    "    for y in range(0, HEIGHT, CELL_SIZE):\n",
    "        pygame.draw.line(WIN, COLOR_GRID, (0, y), (WIDTH, y))\n",
    "\n",
    "def draw_elements(agent_pos, fruits, poisons):\n",
    "    \"\"\"\n",
    "    Dibuja todos los elementos del juego en sus posiciones actuales.\n",
    "    \n",
    "    Renderiza visualmente:\n",
    "    - Agente: Como un cuadrado azul que ocupa toda la celda\n",
    "    - Frutas: Como c铆rculos verdes centrados en sus celdas\n",
    "    - Venenos: Como cuadrados rojos m谩s peque帽os centrados en sus celdas\n",
    "    \n",
    "    Args:\n",
    "        agent_pos (np.array): Posici贸n del agente [fila, columna]\n",
    "        fruits (list): Lista de posiciones de frutas [(fila, col), ...]\n",
    "        poisons (list): Lista de posiciones de venenos [(fila, col), ...]\n",
    "    \n",
    "    Note:\n",
    "        Las coordenadas se invierten para Pygame: agent_pos[1] es X, agent_pos[0] es Y\n",
    "    \"\"\"\n",
    "    # Dibujar agente como cuadrado azul completo\n",
    "    if agent_pos[0] >= 0:  # Solo dibujar si el agente est谩 en el tablero\n",
    "        pygame.draw.rect(WIN, COLOR_AGENT, \n",
    "                        (agent_pos[1] * CELL_SIZE, agent_pos[0] * CELL_SIZE, \n",
    "                         CELL_SIZE, CELL_SIZE))\n",
    "    \n",
    "    # Dibujar frutas como c铆rculos verdes\n",
    "    for f in fruits:\n",
    "        center_x = f[1] * CELL_SIZE + CELL_SIZE // 2\n",
    "        center_y = f[0] * CELL_SIZE + CELL_SIZE // 2\n",
    "        radius = CELL_SIZE // 3\n",
    "        pygame.draw.circle(WIN, COLOR_FRUIT, (center_x, center_y), radius)\n",
    "    \n",
    "    # Dibujar venenos como cuadrados rojos m谩s peque帽os\n",
    "    for p in poisons:\n",
    "        margin = 20  # Margen para hacer el cuadrado m谩s peque帽o\n",
    "        pygame.draw.rect(WIN, COLOR_POISON, \n",
    "                        (p[1] * CELL_SIZE + margin, p[0] * CELL_SIZE + margin, \n",
    "                         CELL_SIZE - 2*margin, CELL_SIZE - 2*margin))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal que maneja el bucle de la aplicaci贸n.\n",
    "    \n",
    "    Implementa una m谩quina de estados con dos modos:\n",
    "    \n",
    "    MODO SETUP:\n",
    "    - Permite al usuario colocar frutas y venenos con clics del mouse\n",
    "    - Click izquierdo: Colocar fruta\n",
    "    - Click derecho: Colocar veneno\n",
    "    - Presionar ESPACIO: Iniciar simulaci贸n\n",
    "    \n",
    "    MODO RUN:\n",
    "    - El agente DQN toma control del juego\n",
    "    - Ejecuta acciones basadas en su pol铆tica aprendida\n",
    "    - Visualiza el comportamiento del agente en tiempo real\n",
    "    - Se reinicia autom谩ticamente al terminar\n",
    "    \n",
    "    La aplicaci贸n se ejecuta hasta que el usuario cierre la ventana.\n",
    "    \"\"\"\n",
    "    # Variables de estado del juego\n",
    "    fruits = []      # Lista de posiciones de frutas colocadas por el usuario\n",
    "    poisons = []     # Lista de posiciones de venenos colocadas por el usuario  \n",
    "    mode = \"setup\"   # Modo actual: \"setup\" (configuraci贸n) o \"run\" (simulaci贸n)\n",
    "\n",
    "    # Configuraci贸n del bucle principal\n",
    "    clock = pygame.time.Clock()  # Para controlar FPS\n",
    "    run = True                   # Flag de control del bucle principal\n",
    "    # BUCLE PRINCIPAL DE LA APLICACIN\n",
    "    while run:\n",
    "        # Limpiar pantalla con fondo negro\n",
    "        WIN.fill((0, 0, 0))\n",
    "        # Dibujar cuadr铆cula base\n",
    "        draw_grid()\n",
    "\n",
    "        # MANEJO DE EVENTOS DE USUARIO\n",
    "        for event in pygame.event.get():\n",
    "            # Evento de cierre de ventana\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "\n",
    "            # EVENTOS EN MODO SETUP (Configuraci贸n manual)\n",
    "            if mode == \"setup\":\n",
    "                # Manejo de clics del mouse para colocar elementos\n",
    "                if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                    pos = pygame.mouse.get_pos()\n",
    "                    # Convertir coordenadas de p铆xeles a coordenadas de cuadr铆cula\n",
    "                    col = pos[0] // CELL_SIZE\n",
    "                    row = pos[1] // CELL_SIZE\n",
    "                    \n",
    "                    # Click izquierdo (bot贸n 1): Colocar fruta\n",
    "                    if event.button == 1 and (row, col) not in fruits:\n",
    "                        fruits.append((row, col))\n",
    "                        print(f\"Fruta colocada en ({row}, {col})\")\n",
    "                    \n",
    "                    # Click derecho (bot贸n 3): Colocar veneno  \n",
    "                    elif event.button == 3 and (row, col) not in poisons:\n",
    "                        poisons.append((row, col))\n",
    "                        print(f\"Veneno colocado en ({row}, {col})\")\n",
    "\n",
    "                # Manejo de teclas para cambiar de modo\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_SPACE:\n",
    "                        mode = \"run\"\n",
    "                        # Inicializar el entorno con la configuraci贸n del usuario\n",
    "                        state = env.reset(agent_pos=(0, 0), fruit_pos=fruits, poison_pos=poisons)\n",
    "                        print(\"=== INICIANDO SIMULACIN DEL AGENTE ===\")\n",
    "                        print(f\"Frutas: {len(fruits)}, Venenos: {len(poisons)}\")\n",
    "\n",
    "        # RENDERIZADO SEGN EL MODO ACTUAL\n",
    "        \n",
    "        if mode == \"setup\":\n",
    "            # MODO CONFIGURACIN: Mostrar elementos colocados por el usuario\n",
    "            # Usar posici贸n (-1,-1) para que el agente no aparezca en pantalla\n",
    "            draw_elements(np.array([-1, -1]), fruits, poisons)\n",
    "        \n",
    "        elif mode == \"run\":\n",
    "            # MODO SIMULACIN: El agente DQN ejecuta su pol铆tica\n",
    "            \n",
    "            # Obtener estado actual del entorno\n",
    "            state = env.get_state()\n",
    "            \n",
    "            # El agente decide la acci贸n usando su pol铆tica entrenada\n",
    "            # explore=False significa que usa solo explotaci贸n, no exploraci贸n\n",
    "            action = agent.choose_action(state, explore=False)\n",
    "            \n",
    "            # Ejecutar la acci贸n en el entorno\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Renderizar estado actual del juego\n",
    "            draw_elements(env.agent_pos, env.fruit_pos, env.poison_pos)\n",
    "\n",
    "            # Verificar si el episodio termin贸\n",
    "            if done:\n",
    "                if not env.fruit_pos:  # Victoria: todas las frutas recogidas\n",
    "                    print(\" 隆XITO! El agente recogi贸 todas las frutas\")\n",
    "                else:  # Derrota: toc贸 veneno\n",
    "                    print(\" DERROTA: El agente toc贸 veneno\")\n",
    "                \n",
    "                print(\"=== SIMULACIN TERMINADA ===\")\n",
    "                \n",
    "                # Reiniciar para permitir nueva configuraci贸n\n",
    "                fruits = []\n",
    "                poisons = []\n",
    "                mode = \"setup\"\n",
    "                \n",
    "                # Pausa dram谩tica antes de reiniciar\n",
    "                pygame.time.delay(2000)\n",
    "\n",
    "            # Pausa entre movimientos para visualizaci贸n clara\n",
    "            pygame.time.delay(300)\n",
    "\n",
    "        # Actualizar pantalla con todos los cambios\n",
    "        pygame.display.update()\n",
    "\n",
    "    # Limpieza al cerrar la aplicaci贸n\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa.\n",
    "    \n",
    "    Ejecuta la funci贸n main() solo si este archivo se ejecuta directamente\n",
    "    (no si se importa como m贸dulo).\n",
    "    \"\"\"\n",
    "    print(\"=== AGENTE DQN COME-FRUTAS ===\")\n",
    "    print(\"CONTROLES:\")\n",
    "    print(\" Click izquierdo: Colocar fruta\")\n",
    "    print(\" Click derecho: Colocar veneno\")\n",
    "    print(\" ESPACIO: Iniciar simulaci贸n\")\n",
    "    print(\" Cerrar ventana: Salir\")\n",
    "    print(\"\\n隆Configura un escenario y observa al agente!\")\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a901f",
   "metadata": {},
   "source": [
    "### DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f3b84",
   "metadata": {},
   "source": [
    "#### agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\"\"\"\n",
    "Implementaci贸n completa del agente DDQN (Double Deep Q-Network).\n",
    "\n",
    "Este m贸dulo contiene la implementaci贸n del algoritmo DDQN, una mejora del DQN cl谩sico\n",
    "que aborda el problema de sobreestimaci贸n de valores Q mediante el uso de dos redes\n",
    "neuronales: una para selecci贸n de acciones y otra para evaluaci贸n de valores.\n",
    "\n",
    "Caracter铆sticas principales:\n",
    "- Red neuronal convolucional optimizada para entornos de grilla\n",
    "- Algoritmo DDQN con separaci贸n de selecci贸n y evaluaci贸n\n",
    "- Memoria de replay extendida (50,000 experiencias)\n",
    "- T茅cnicas de estabilizaci贸n avanzadas\n",
    "- Sistema robusto de guardado/carga de modelos\n",
    "\n",
    "Algoritmo DDQN:\n",
    "La innovaci贸n clave es el uso de dos redes para calcular targets:\n",
    "1. Red principal: Selecciona la mejor acci贸n del siguiente estado\n",
    "2. Red objetivo: Eval煤a el valor Q de esa acci贸n seleccionada\n",
    "\n",
    "Esto reduce significativamente la sobreestimaci贸n de valores Q que sufre DQN cl谩sico,\n",
    "resultando en un aprendizaje m谩s estable y pol铆ticas m谩s robustas.\n",
    "\n",
    "Referencias:\n",
    "- van Hasselt et al. (2016): \"Deep Reinforcement Learning with Double Q-learning\"\n",
    "- Mnih et al. (2015): \"Human-level control through deep reinforcement learning\"\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. RED NEURONAL CONVOLUCIONAL PARA DDQN ---\n",
    "class CNN_DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal convolucional especializada para DDQN en entornos de grilla.\n",
    "    \n",
    "    Esta arquitectura est谩 optimizada para procesar estados representados como\n",
    "    im谩genes multi-canal, t铆picos en problemas de navegaci贸n espacial donde\n",
    "    el estado se puede visualizar como una cuadr铆cula con diferentes tipos\n",
    "    de elementos (agente, objetivos, obst谩culos).\n",
    "    \n",
    "    Dise帽o arquitect贸nico:\n",
    "    \n",
    "    **Etapa Convolucional (Extracci贸n de caracter铆sticas):**\n",
    "    - Conv1: 316 canales, kernel 3x3  Detecta patrones b谩sicos locales\n",
    "    - Conv2: 1632 canales, kernel 3x3  Combina patrones en caracter铆sticas complejas\n",
    "    - ReLU en cada capa para introducir no-linealidad\n",
    "    - Padding=1 preserva dimensiones espaciales\n",
    "    \n",
    "    **Etapa Completamente Conectada (Toma de decisiones):**\n",
    "    - FC1: Procesa caracter铆sticas extra铆das (256 neuronas)\n",
    "    - FC2: Genera valores Q para cada acci贸n posible\n",
    "    \n",
    "    **Ventajas de esta arquitectura:**\n",
    "    - Invarianza a traslaciones locales (convoluciones)\n",
    "    - Reducci贸n progresiva de par谩metros vs redes totalmente conectadas\n",
    "    - Capacidad de detectar patrones espaciales complejos\n",
    "    - Escalabilidad a entornos de diferentes tama帽os\n",
    "    \n",
    "    Args:\n",
    "        h (int): Altura de la grilla de entrada\n",
    "        w (int): Anchura de la grilla de entrada\n",
    "        outputs (int): N煤mero de acciones posibles (valores Q de salida)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(CNN_DQN, self).__init__()\n",
    "        \n",
    "        # --- CAPAS CONVOLUCIONALES PARA EXTRACCIN DE CARACTERSTICAS ---\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        # Entrada: 3 canales (agente, frutas, venenos)\n",
    "        # Salida: 16 mapas de caracter铆sticas\n",
    "        # Kernel 3x3: Ventana de percepci贸n local 贸ptima para grillas peque帽as\n",
    "        # Padding=1: Preserva dimensiones espaciales de entrada\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Entrada: 16 mapas de caracter铆sticas de la capa anterior\n",
    "        # Salida: 32 mapas de caracter铆sticas m谩s abstractas\n",
    "        # Mayor profundidad permite detectar patrones m谩s complejos\n",
    "        \n",
    "        # --- CLCULO DINMICO DE DIMENSIONES ---\n",
    "        \"\"\"\n",
    "        Funci贸n auxiliar para calcular dimensiones despu茅s de convoluciones.\n",
    "        Esencial para conectar correctamente las capas convolucionales\n",
    "        con las capas completamente conectadas.\n",
    "        \n",
    "        F贸rmula: output_size = (input_size + 2*padding - kernel_size) // stride + 1\n",
    "        \"\"\"\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        # Aplicar la funci贸n de c谩lculo a ambas dimensiones espaciales\n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32  # 32 canales de la 煤ltima conv\n",
    "        \n",
    "        # --- CAPAS COMPLETAMENTE CONECTADAS PARA TOMA DE DECISIONES ---\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        # Capa oculta densa que procesa las caracter铆sticas extra铆das\n",
    "        # 256 neuronas: Balance entre capacidad expresiva y eficiencia computacional\n",
    "        # Suficiente para capturar relaciones complejas entre caracter铆sticas espaciales\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "        # Capa de salida que produce valores Q para cada acci贸n\n",
    "        # Sin funci贸n de activaci贸n (los valores Q pueden ser negativos)\n",
    "        # N煤mero de neuronas = n煤mero de acciones posibles\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagaci贸n hacia adelante de la red neuronal.\n",
    "        \n",
    "        Implementa el flujo completo de informaci贸n desde el estado de entrada\n",
    "        hasta los valores Q de salida, aplicando las transformaciones necesarias\n",
    "        para extraer caracter铆sticas espaciales y generar estimaciones de valor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Estado de entrada con forma (batch_size, 3, height, width)\n",
    "                             - batch_size: N煤mero de estados en el lote\n",
    "                             - 3: Canales (agente, frutas, venenos)\n",
    "                             - height, width: Dimensiones espaciales de la grilla\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Valores Q para cada acci贸n con forma (batch_size, num_actions)\n",
    "        \n",
    "        Flujo de procesamiento:\n",
    "        1. Convoluci贸n 1 + ReLU: Detecci贸n de patrones b谩sicos\n",
    "        2. Convoluci贸n 2 + ReLU: Extracci贸n de caracter铆sticas complejas\n",
    "        3. Aplanamiento: Conversi贸n de 2D a 1D para capas densas\n",
    "        4. FC1 + ReLU: Procesamiento de alto nivel de caracter铆sticas\n",
    "        5. FC2: Generaci贸n de valores Q finales (sin activaci贸n)\n",
    "        \"\"\"\n",
    "        # Primera capa convolucional con activaci贸n ReLU\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        \n",
    "        # Segunda capa convolucional con activaci贸n ReLU\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        \n",
    "        # Aplanar caracter铆sticas espaciales para capas densas\n",
    "        # Transforma tensor 4D (batch, canales, alto, ancho)  2D (batch, caracter铆sticas)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Primera capa completamente conectada con activaci贸n ReLU\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        \n",
    "        # Capa de salida sin activaci贸n (valores Q pueden ser negativos)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "# --- 2. AGENTE DDQN CON EXPERIENCE REPLAY Y TARGET NETWORK ---\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agente de Deep Q-Learning con arquitectura CNN para navegaci贸n en grilla.\n",
    "    \n",
    "    Implementa un agente de aprendizaje por refuerzo que utiliza una red neuronal\n",
    "    convolucional para procesar estados espaciales y aprender una pol铆tica 贸ptima\n",
    "    para navegar en un entorno de grilla, evitando venenos y recolectando frutas.\n",
    "    \n",
    "    Caracter铆sticas principales:\n",
    "    - CNN para procesamiento de estados espaciales (grilla 5x5)\n",
    "    - Experience replay con buffer de memoria para estabilidad\n",
    "    - Target network para c谩lculos de valores Q objetivo\n",
    "    - Estrategia epsilon-greedy con decaimiento para exploraci贸n\n",
    "    - Optimizaci贸n Adam para entrenamiento eficiente\n",
    "    \n",
    "    Arquitectura del agente:\n",
    "    1. Red principal: Entrenamiento y selecci贸n de acciones\n",
    "    2. Red objetivo: C谩lculos estables de valores Q futuro\n",
    "    3. Buffer de experiencias: Almacena transiciones para replay\n",
    "    4. Optimizador: Adam para actualizaci贸n de pesos\n",
    "    \n",
    "    El agente mejora mediante:\n",
    "    - Exploraci贸n inicial alta (epsilon=1.0) para descubrir el entorno\n",
    "    - Decaimiento gradual hacia explotaci贸n (epsilon_min=0.01)\n",
    "    - Entrenamiento con experiencias pasadas (experience replay)\n",
    "    - Actualizaci贸n peri贸dica de la red objetivo para estabilidad\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_shape, action_size):\n",
    "        \"\"\"\n",
    "        Inicializa el agente DDQN con configuraci贸n optimizada para el entorno.\n",
    "        \n",
    "        Args:\n",
    "            state_shape (tuple): Forma del estado (canales, altura, ancho)\n",
    "                                T铆picamente (3, 5, 5) para grilla con agente/frutas/venenos\n",
    "            action_size (int): N煤mero de acciones posibles (4: arriba, abajo, izq, der)\n",
    "        \n",
    "        Configuraci贸n de hiperpar谩metros:\n",
    "        - memory: 50,000 experiencias para diversidad y estabilidad\n",
    "        - gamma: 0.99 (alta importancia a recompensas futuras)\n",
    "        - epsilon: 1.00.01 (exploraci贸n total a m铆nima)\n",
    "        - epsilon_decay: 0.9995 (decaimiento gradual)\n",
    "        - learning_rate: 0.0001 (ajuste fino y estable)\n",
    "        - update_target_every: 5 (frecuencia de actualizaci贸n de red objetivo)\n",
    "        \"\"\"\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # --- CONFIGURACIN DE EXPERIENCE REPLAY ---\n",
    "        self.memory = deque(maxlen=50000)      \n",
    "        # Buffer circular que almacena hasta 50,000 experiencias\n",
    "        # Tama帽o grande permite mayor diversidad de experiencias\n",
    "        # Memoria circular: experiencias antiguas se eliminan autom谩ticamente\n",
    "        \n",
    "        # --- PARMETROS DE APRENDIZAJE ---\n",
    "        self.gamma = 0.99                     \n",
    "        # Factor de descuento alto para valorar recompensas futuras\n",
    "        # 0.99 significa que recompensas 100 pasos adelante valen ~37% del valor actual\n",
    "        \n",
    "        # --- ESTRATEGIA DE EXPLORACIN EPSILON-GREEDY ---\n",
    "        self.epsilon = 1.0                    \n",
    "        # Exploraci贸n inicial: 100% acciones aleatorias para descubrir entorno\n",
    "        \n",
    "        self.epsilon_min = 0.01               \n",
    "        # Exploraci贸n m铆nima: siempre mantener 1% de acciones aleatorias\n",
    "        # Evita quedar atrapado en m铆nimos locales\n",
    "        \n",
    "        self.epsilon_decay = 0.9995           \n",
    "        # Decaimiento gradual: epsilon *= 0.9995 cada episodio\n",
    "        # Transici贸n suave de exploraci贸n a explotaci贸n\n",
    "        \n",
    "        # --- OPTIMIZACIN ---\n",
    "        self.learning_rate = 0.0001           \n",
    "        # Tasa de aprendizaje baja para entrenamiento estable y convergencia suave\n",
    "        # Evita oscilaciones en la funci贸n de p茅rdida\n",
    "        \n",
    "        # --- ACTUALIZACIN DE RED OBJETIVO ---\n",
    "        self.update_target_every = 5          \n",
    "        # Frecuencia de actualizaci贸n de la red objetivo (cada 5 entrenamientos)\n",
    "        # Balance entre estabilidad y adaptaci贸n a nuevos pesos\n",
    "        \n",
    "        # --- INICIALIZACIN DE REDES NEURONALES ---\n",
    "        h, w = state_shape[1], state_shape[2]  # Dimensiones de la grilla\n",
    "        \n",
    "        # Red principal: Se entrena continuamente con nuevas experiencias\n",
    "        self.model = CNN_DQN(h, w, action_size)\n",
    "        \n",
    "        # Red objetivo: Proporciona valores Q estables para c谩lculos de objetivo\n",
    "        self.target_model = CNN_DQN(h, w, action_size)\n",
    "        \n",
    "        # Inicializar red objetivo con mismos pesos que red principal\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # --- CONFIGURACIN DE ENTRENAMIENTO ---\n",
    "        # Optimizador Adam: Adaptativo, eficiente para redes neuronales\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Funci贸n de p茅rdida: Error cuadr谩tico medio para regresi贸n de valores Q\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Contador de pasos para tracking de actualizaciones\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Actualiza la red objetivo copiando pesos de la red principal.\n",
    "        \n",
    "        La red objetivo es fundamental para la estabilidad del entrenamiento:\n",
    "        - Proporciona valores Q estables para calcular objetivos\n",
    "        - Se actualiza menos frecuentemente que la red principal\n",
    "        - Evita que los objetivos cambien constantemente durante entrenamiento\n",
    "        \n",
    "        Proceso:\n",
    "        1. Copia completa de todos los par谩metros de la red principal\n",
    "        2. La red objetivo permanece fija hasta la pr贸xima actualizaci贸n\n",
    "        3. Garantiza consistencia en los c谩lculos de valores Q objetivo\n",
    "        \"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Almacena una experiencia en el buffer de memory replay.\n",
    "        \n",
    "        Experience replay es una t茅cnica fundamental en Deep Q-Learning que:\n",
    "        - Rompe correlaciones temporales entre experiencias consecutivas\n",
    "        - Permite reutilizar experiencias valiosas m煤ltiples veces\n",
    "        - Mejora la eficiencia de uso de datos\n",
    "        - Estabiliza el entrenamiento de la red neuronal\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado actual del agente en la grilla\n",
    "                             Forma (3, 5, 5) con canales para agente/frutas/venenos\n",
    "            action (int): Acci贸n tomada (0=arriba, 1=abajo, 2=izq, 3=der)\n",
    "            reward (float): Recompensa recibida por la acci贸n\n",
    "                           +10 por fruta, -10 por veneno, -1 por movimiento\n",
    "            next_state (np.array): Estado resultante despu茅s de la acci贸n\n",
    "            done (bool): True si el episodio termin贸 (todas frutas recogidas)\n",
    "        \n",
    "        El buffer circular (deque) gestiona autom谩ticamente:\n",
    "        - Eliminaci贸n de experiencias antiguas cuando se alcanza el l铆mite\n",
    "        - Mantener diversidad de experiencias para entrenamiento robusto\n",
    "        - Acceso eficiente para muestreo aleatorio durante replay\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, explore=True):\n",
    "        \"\"\"\n",
    "        Selecciona una acci贸n usando estrategia epsilon-greedy.\n",
    "        \n",
    "        Implementa el balance cr铆tico entre exploraci贸n y explotaci贸n:\n",
    "        - Exploraci贸n: Necesaria para descubrir nuevas estrategias\n",
    "        - Explotaci贸n: Usar conocimiento actual para maximizar recompensas\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado actual del entorno (3, 5, 5)\n",
    "            explore (bool): Si False, siempre usa la mejor acci贸n conocida\n",
    "                           til para evaluaci贸n sin exploraci贸n aleatoria\n",
    "        \n",
    "        Returns:\n",
    "            int: ndice de acci贸n seleccionada (0-3)\n",
    "        \n",
    "        Estrategia epsilon-greedy:\n",
    "        - Probabilidad epsilon: Acci贸n aleatoria (exploraci贸n)\n",
    "        - Probabilidad (1-epsilon): Mejor acci贸n seg煤n red neuronal (explotaci贸n)\n",
    "        \n",
    "        Progresi贸n de epsilon:\n",
    "        - Inicio: 蔚=1.0  100% exploraci贸n para mapear el entorno\n",
    "        - Medio: 蔚~0.5  Balance exploraci贸n/explotaci贸n\n",
    "        - Final: 蔚=0.01  99% explotaci贸n, 1% exploraci贸n residual\n",
    "        \"\"\"\n",
    "        self.steps_done += 1  # Contador para tracking de progreso\n",
    "        \n",
    "        # Exploraci贸n: acci贸n aleatoria si epsilon lo determina y explore=True\n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Explotaci贸n: usar red neuronal para encontrar mejor acci贸n\n",
    "        # Convertir estado a tensor PyTorch y agregar dimensi贸n de batch\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Inferencia sin calcular gradientes (m谩s eficiente)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state_tensor)\n",
    "        \n",
    "        # Seleccionar acci贸n con mayor valor Q predicho\n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Entrena la red neuronal usando experiencias pasadas con Double DQN.\n",
    "        \n",
    "        Double DQN mejora el algoritmo DQN cl谩sico al separar la selecci贸n\n",
    "        y evaluaci贸n de acciones, reduciendo la sobrestimaci贸n sistem谩tica\n",
    "        de valores Q que puede llevar a pol铆ticas sub贸ptimas.\n",
    "        \n",
    "        Diferencias DQN vs Double DQN:\n",
    "        \n",
    "        DQN Cl谩sico:\n",
    "        target = reward + gamma * max(target_network(next_state))\n",
    "        Problema: La misma red selecciona y eval煤a  sobrestimaci贸n\n",
    "        \n",
    "        Double DQN:\n",
    "        best_action = argmax(main_network(next_state))     # Selecci贸n\n",
    "        target = reward + gamma * target_network(next_state)[best_action]  # Evaluaci贸n\n",
    "        Ventaja: Separaci贸n reduce sesgo de sobrestimaci贸n\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Tama帽o del lote de experiencias para entrenamiento\n",
    "                             T铆picamente 32-64 para balance eficiencia/estabilidad\n",
    "        \n",
    "        Proceso de entrenamiento:\n",
    "        1. Verificar que hay suficientes experiencias en memoria\n",
    "        2. Muestrear batch aleatorio de experiencias\n",
    "        3. Calcular valores Q actuales para estados del batch\n",
    "        4. Aplicar l贸gica Double DQN para calcular objetivos\n",
    "        5. Computar loss (MSE entre predicciones y objetivos)\n",
    "        6. Backpropagation y actualizaci贸n de pesos\n",
    "        7. Decrecer epsilon (menos exploraci贸n)\n",
    "        8. Aplicar clipping de gradientes para estabilidad\n",
    "        \"\"\"\n",
    "        # No entrenar si memoria insuficiente\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        # --- MUESTREO ALEATORIO DE EXPERIENCIAS ---\n",
    "        # Rompe correlaciones temporales y mejora generalizaci贸n\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Separar componentes de las experiencias en tensores\n",
    "        states = torch.FloatTensor(np.array([e[0] for e in minibatch]))\n",
    "        actions = torch.LongTensor([e[1] for e in minibatch]).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor([e[2] for e in minibatch]).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(np.array([e[3] for e in minibatch]))\n",
    "        dones = torch.BoolTensor([e[4] for e in minibatch]).unsqueeze(1)\n",
    "\n",
    "        # --- VALORES Q ACTUALES ---\n",
    "        # Calcular Q-values para estados actuales usando red principal\n",
    "        current_q_values = self.model(states).gather(1, actions)\n",
    "        \n",
    "        # --- LGICA DOUBLE DQN ---\n",
    "        with torch.no_grad():  # No calcular gradientes para eficiencia\n",
    "            # 1. Red principal SELECCIONA mejor acci贸n para siguiente estado\n",
    "            #    Usa conocimiento m谩s actualizado para selecci贸n\n",
    "            best_next_actions = self.model(next_states).max(1)[1].unsqueeze(1)\n",
    "            \n",
    "            # 2. Red objetivo EVALA el valor de la acci贸n seleccionada\n",
    "            #    Usa pesos m谩s estables para evaluaci贸n consistente\n",
    "            next_q_values_target = self.target_model(next_states).gather(1, best_next_actions)\n",
    "        \n",
    "        # --- CLCULO DE OBJETIVOS Q ---\n",
    "        # Si episodio termin贸 (done=True), no hay valor futuro\n",
    "        # target = reward + descuento * valor_futuro * (no_terminado)\n",
    "        target_q_values = rewards + (self.gamma * next_q_values_target * (~dones))\n",
    "        \n",
    "        # --- ENTRENAMIENTO DE LA RED ---\n",
    "        # Error cuadr谩tico medio entre predicciones y objetivos\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimizaci贸n con backpropagation\n",
    "        self.optimizer.zero_grad()  # Limpiar gradientes previos\n",
    "        loss.backward()             # Calcular gradientes\n",
    "        \n",
    "        # Clipping de gradientes para prevenir explosi贸n\n",
    "        # Limita gradientes a [-1, 1] para estabilidad num茅rica\n",
    "        torch.nn.utils.clip_grad_value_(self.model.parameters(), 1)\n",
    "        \n",
    "        self.optimizer.step()       # Actualizar pesos\n",
    "        \n",
    "        # --- DECAIMIENTO DE EXPLORACIN ---\n",
    "        # Reducir epsilon gradualmente para transici贸n exploraci贸nexplotaci贸n\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        \"\"\"\n",
    "        Carga un modelo pre-entrenado desde archivo.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Ruta al archivo .pth con los pesos del modelo\n",
    "        \n",
    "        Funcionalidad:\n",
    "        - Carga pesos de la red principal desde archivo\n",
    "        - Actualiza red objetivo para mantener consistencia\n",
    "        - Permite continuar entrenamiento o hacer inferencia\n",
    "        - Preserva arquitectura de red definida en __init__\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "        self.update_target_network()  # Sincronizar red objetivo\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Guarda el modelo entrenado en un archivo.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Ruta donde guardar el archivo .pth\n",
    "        \n",
    "        Funcionalidad:\n",
    "        - Guarda solo los pesos de la red principal (m谩s compacto)\n",
    "        - La red objetivo se puede reconstruir al cargar\n",
    "        - Formato PyTorch est谩ndar para compatibilidad\n",
    "        - Permite reutilizar modelos entrenados\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe144e4d",
   "metadata": {},
   "source": [
    "#### enviroment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment.py\n",
    "\"\"\"\n",
    "Entorno de grilla para el entrenamiento de agentes de aprendizaje por refuerzo.\n",
    "Este m贸dulo implementa un entorno de grilla donde un agente debe recolectar frutas \n",
    "mientras evita venenos, dise帽ado espec铆ficamente para algoritmos DDQN.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GridEnvironment:\n",
    "    \"\"\"\n",
    "    Entorno de grilla 2D para simulaci贸n de agentes que recolectan frutas y evitan venenos.\n",
    "    \n",
    "    El entorno consiste en una grilla cuadrada donde:\n",
    "    - El agente se mueve en 4 direcciones (arriba, abajo, izquierda, derecha)\n",
    "    - Las frutas otorgan recompensas positivas cuando son recolectadas\n",
    "    - Los venenos causan penalizaciones y resetean la posici贸n del agente\n",
    "    - El objetivo es recolectar todas las frutas minimizando las penalizaciones\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tama帽o de la grilla (size x size)\n",
    "        start_pos (tuple): Posici贸n inicial del agente en cada episodio\n",
    "        agent_pos (np.array): Posici贸n actual del agente\n",
    "        fruit_pos (list): Lista de posiciones de las frutas\n",
    "        poison_pos (list): Lista de posiciones de los venenos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de grilla.\n",
    "        \n",
    "        Args:\n",
    "            size (int, optional): Tama帽o de la grilla cuadrada. Por defecto es 5x5.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.start_pos = (0, 0)  # Posici贸n inicial por defecto\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, agent_pos=(0, 0), fruit_pos=[], poison_pos=[]):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno con una configuraci贸n espec铆fica.\n",
    "        \n",
    "        Este m茅todo prepara el entorno para un nuevo episodio, estableciendo las posiciones\n",
    "        iniciales del agente, frutas y venenos. Es crucial para el entrenamiento ya que\n",
    "        permite configurar diferentes escenarios de aprendizaje.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (tuple, optional): Posici贸n inicial del agente (x, y). Por defecto (0, 0).\n",
    "            fruit_pos (list, optional): Lista de tuplas con posiciones de frutas. Por defecto vac铆a.\n",
    "            poison_pos (list, optional): Lista de tuplas con posiciones de venenos. Por defecto vac铆a.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno como tensor 3D (canales, altura, anchura).\n",
    "        \"\"\"\n",
    "        self.start_pos = np.array(agent_pos)  # Guardamos la posici贸n inicial del episodio\n",
    "        self.agent_pos = np.array(agent_pos)\n",
    "        self.fruit_pos = [np.array(p) for p in fruit_pos]\n",
    "        self.poison_pos = [np.array(p) for p in poison_pos]\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Obtiene el estado actual del entorno como una representaci贸n tensorial.\n",
    "        \n",
    "        El estado se representa como un tensor 3D de forma (3, size, size) donde:\n",
    "        - Canal 0: Posici贸n del agente (1.0 en la posici贸n actual, 0.0 en el resto)\n",
    "        - Canal 1: Posiciones de las frutas (1.0 donde hay frutas, 0.0 en el resto)\n",
    "        - Canal 2: Posiciones de los venenos (1.0 donde hay venenos, 0.0 en el resto)\n",
    "        \n",
    "        Esta representaci贸n permite que las redes neuronales procesen eficientemente\n",
    "        la informaci贸n espacial del entorno usando convoluciones.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Tensor 3D de forma (3, size, size) representando el estado actual.\n",
    "        \"\"\"\n",
    "        state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posici贸n del agente\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de las frutas\n",
    "        for fruit in self.fruit_pos:\n",
    "            state[1, fruit[0], fruit[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de los venenos\n",
    "        for poison in self.poison_pos:\n",
    "            state[2, poison[0], poison[1]] = 1.0\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Ejecuta una acci贸n en el entorno y devuelve el resultado.\n",
    "        \n",
    "        Este m茅todo implementa la l贸gica principal del entorno, procesando las acciones\n",
    "        del agente y calculando las recompensas correspondientes. Incluye manejo especial\n",
    "        para venenos que resetean la posici贸n del agente sin terminar el episodio.\n",
    "        \n",
    "        Args:\n",
    "            action (int): Acci贸n a ejecutar\n",
    "                - 0: Mover hacia arriba (decrementar fila)\n",
    "                - 1: Mover hacia abajo (incrementar fila)\n",
    "                - 2: Mover hacia la izquierda (decrementar columna)\n",
    "                - 3: Mover hacia la derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, episodio_terminado)\n",
    "                - nuevo_estado (np.array): Estado resultante despu茅s de la acci贸n\n",
    "                - recompensa (float): Recompensa obtenida por la acci贸n\n",
    "                - episodio_terminado (bool): True si el episodio ha terminado\n",
    "        \n",
    "        L贸gica de recompensas:\n",
    "            - Movimiento b谩sico: -0.05 (costo de vida)\n",
    "            - Tocar veneno: -10.0 (penalizaci贸n fuerte + reset a posici贸n inicial)\n",
    "            - Recolectar fruta: +1.0 (recompensa por objetivo)\n",
    "            - Completar nivel: +10.0 (bonus por recolectar todas las frutas)\n",
    "        \"\"\"\n",
    "        # Ejecutar el movimiento seg煤n la acci贸n seleccionada\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] -= 1    # Mover hacia arriba\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] += 1    # Mover hacia abajo\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] -= 1    # Mover hacia la izquierda\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] += 1    # Mover hacia la derecha\n",
    "        \n",
    "        # Asegurar que el agente permanezca dentro de los l铆mites de la grilla\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.size - 1)\n",
    "\n",
    "        # Recompensa base por cada movimiento (costo de vida)\n",
    "        reward = -0.05\n",
    "        done = False\n",
    "\n",
    "        # --- LGICA DE MANEJO DE VENENOS ---\n",
    "        # Verificar si el agente toc贸 alg煤n veneno\n",
    "        if any(np.array_equal(self.agent_pos, p) for p in self.poison_pos):\n",
    "            reward = -10.0  # Penalizaci贸n severa por tocar veneno\n",
    "            self.agent_pos = np.copy(self.start_pos)  # Resetear a posici贸n inicial\n",
    "            # IMPORTANTE: done NO es True. El episodio contin煤a despu茅s del reset.\n",
    "        else:\n",
    "            # --- LGICA DE RECOLECCIN DE FRUTAS ---\n",
    "            # Esta l贸gica solo se ejecuta si NO se toc贸 un veneno\n",
    "            eaten_fruit_this_step = False\n",
    "            \n",
    "            # Verificar si el agente recolect贸 alguna fruta\n",
    "            for i, fruit in enumerate(self.fruit_pos):\n",
    "                if np.array_equal(self.agent_pos, fruit):\n",
    "                    reward += 1.0  # Recompensa por recolectar fruta\n",
    "                    self.fruit_pos.pop(i)  # Remover la fruta recolectada\n",
    "                    eaten_fruit_this_step = True\n",
    "                    break\n",
    "\n",
    "            # Opcional: Aqu铆 se puede agregar reward shaping basado en distancia\n",
    "            if not eaten_fruit_this_step and self.fruit_pos:\n",
    "                # Ejemplo: reward += -0.01 * distancia_a_fruta_m谩s_cercana\n",
    "                pass  # Actualmente no implementado\n",
    "\n",
    "            # --- CONDICIN DE VICTORIA ---\n",
    "            # Si no quedan frutas, el episodio termina exitosamente\n",
    "            if not self.fruit_pos:\n",
    "                done = True\n",
    "                reward += 10.0  # Bonus por completar el nivel\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94c721",
   "metadata": {},
   "source": [
    "#### interfaz.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420bcad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn_agente_comefrutas.py (versi贸n integrada con interfaz gr谩fica completa)\n",
    "\"\"\"\n",
    "Interfaz gr谩fica para el entrenamiento y visualizaci贸n de agentes DDQN.\n",
    "\n",
    "Este m贸dulo implementa una interfaz gr谩fica completa usando Pygame que permite:\n",
    "- Configurar entornos de manera interactiva\n",
    "- Visualizar el comportamiento del agente entrenado\n",
    "- Alternar entre modo setup y modo juego\n",
    "- Gestionar elementos del entorno (frutas, venenos, paredes)\n",
    "\n",
    "El sistema est谩 dise帽ado para facilitar la experimentaci贸n con diferentes\n",
    "configuraciones de entorno y la evaluaci贸n visual del rendimiento del agente.\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from agent import Agent\n",
    "\n",
    "# --- CONFIGURACIN GENERAL ---\n",
    "\"\"\"Constantes de configuraci贸n para la interfaz gr谩fica.\"\"\"\n",
    "GRID_WIDTH = 5          # Ancho de la grilla en celdas\n",
    "GRID_HEIGHT = 5         # Alto de la grilla en celdas\n",
    "CELL_SIZE = 120         # Tama帽o de cada celda en p铆xeles\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la pantalla\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto total de la pantalla\n",
    "\n",
    "# Paleta de colores para la interfaz\n",
    "COLOR_FONDO = (25, 25, 25)        # Fondo oscuro\n",
    "COLOR_LINEAS = (40, 40, 40)       # L铆neas de la grilla\n",
    "COLOR_CURSOR = (255, 255, 0)      # Cursor amarillo en modo setup\n",
    "COLOR_TEXTO = (230, 230, 230)     # Texto claro\n",
    "\n",
    "\n",
    "# --- ENTORNO PARA DDQN (misma estructura visual que Q-learning) ---\n",
    "class EntornoGrid:\n",
    "    \"\"\"\n",
    "    Entorno de grilla con interfaz gr谩fica para agentes DDQN.\n",
    "    \n",
    "    Esta clase maneja tanto la l贸gica del entorno como su representaci贸n visual,\n",
    "    proporcionando una interfaz interactiva para configurar y visualizar el\n",
    "    comportamiento del agente. Compatible con la arquitectura DDQN.\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tama帽o de la grilla\n",
    "        agent_pos (tuple): Posici贸n actual del agente (x, y)\n",
    "        frutas (set): Conjunto de posiciones con frutas\n",
    "        venenos (set): Conjunto de posiciones con venenos\n",
    "        paredes (set): Conjunto de posiciones con paredes (obst谩culos)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de grilla con configuraci贸n por defecto.\n",
    "        \n",
    "        El agente comienza en la posici贸n (0,0) y todos los conjuntos de\n",
    "        elementos est谩n vac铆os inicialmente.\n",
    "        \"\"\"\n",
    "        self.size = GRID_WIDTH\n",
    "        self.agent_pos = (0, 0)\n",
    "        self.frutas = set()\n",
    "        self.venenos = set()\n",
    "        self.paredes = set()\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"\n",
    "        Resetea el agente a su posici贸n inicial sin modificar el entorno.\n",
    "        \n",
    "        Esta funci贸n es 煤til para reiniciar episodios manteniendo la misma\n",
    "        configuraci贸n de frutas, venenos y paredes establecida en modo setup.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno despu茅s del reset.\n",
    "        \"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno excepto el agente.\n",
    "        \n",
    "        Esta funci贸n es 煤til para limpiar completamente el entorno y comenzar\n",
    "        una nueva configuraci贸n desde cero en modo setup.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acci贸n del agente y actualiza el estado del entorno.\n",
    "        \n",
    "        Este m茅todo implementa la l贸gica de movimiento y las reglas del juego,\n",
    "        incluyendo colisiones con paredes, recolecci贸n de frutas y penalizaciones\n",
    "        por venenos.\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Acci贸n a ejecutar\n",
    "                - 0: Mover hacia arriba (y-1)\n",
    "                - 1: Mover hacia abajo (y+1)\n",
    "                - 2: Mover hacia la izquierda (x-1)\n",
    "                - 3: Mover hacia la derecha (x+1)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (estado, recompensa, terminado)\n",
    "                - estado (np.array): Nuevo estado del entorno\n",
    "                - recompensa (float): Recompensa obtenida por la acci贸n\n",
    "                - terminado (bool): True si el episodio ha terminado\n",
    "        \n",
    "        L贸gica de recompensas:\n",
    "            - Colisi贸n con pared/l铆mite: -0.1 (sin movimiento)\n",
    "            - Movimiento v谩lido: -0.05 (costo de vida)\n",
    "            - Tocar veneno: -10.0 (penalizaci贸n + reset a origen)\n",
    "            - Recolectar fruta: +1.0\n",
    "            - Completar nivel: +10.0 adicional\n",
    "        \"\"\"\n",
    "        x, y = self.agent_pos\n",
    "        \n",
    "        # Calcular nueva posici贸n seg煤n la acci贸n\n",
    "        if accion == 0:\n",
    "            y -= 1    # Mover hacia arriba\n",
    "        elif accion == 1:\n",
    "            y += 1    # Mover hacia abajo\n",
    "        elif accion == 2:\n",
    "            x -= 1    # Mover hacia la izquierda\n",
    "        elif accion == 3:\n",
    "            x += 1    # Mover hacia la derecha\n",
    "\n",
    "        # Verificar colisiones con l铆mites de la grilla o paredes\n",
    "        if (\n",
    "            x < 0\n",
    "            or x >= GRID_WIDTH\n",
    "            or y < 0\n",
    "            or y >= GRID_HEIGHT\n",
    "            or (x, y) in self.paredes\n",
    "        ):\n",
    "            # Movimiento inv谩lido: penalizaci贸n menor y no se mueve\n",
    "            return self.get_state(), -0.1, False\n",
    "\n",
    "        # Movimiento v谩lido: actualizar posici贸n del agente\n",
    "        self.agent_pos = (x, y)\n",
    "        recompensa = -0.05  # Costo base por movimiento\n",
    "        terminado = False\n",
    "\n",
    "        # Verificar interacciones con elementos del entorno\n",
    "        if self.agent_pos in self.venenos:\n",
    "            # Penalizaci贸n por tocar veneno y reset a posici贸n inicial\n",
    "            recompensa = -10.0\n",
    "            self.agent_pos = (0, 0)\n",
    "        elif self.agent_pos in self.frutas:\n",
    "            # Recompensa por recolectar fruta\n",
    "            recompensa = 1.0\n",
    "            self.frutas.remove(self.agent_pos)\n",
    "            \n",
    "            # Verificar si se complet贸 el nivel (no quedan frutas)\n",
    "            if not self.frutas:\n",
    "                recompensa += 10.0  # Bonus por completar\n",
    "                terminado = True\n",
    "\n",
    "        return self.get_state(), recompensa, terminado\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Obtiene la representaci贸n del estado actual como tensor 3D.\n",
    "        \n",
    "        Convierte el estado del entorno en un formato compatible con redes\n",
    "        neuronales convolucionales, usando 3 canales para representar\n",
    "        diferentes tipos de elementos.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Tensor 3D de forma (3, size, size) donde:\n",
    "                - Canal 0: Posici贸n del agente (1.0 donde est谩 el agente)\n",
    "                - Canal 1: Posiciones de frutas (1.0 donde hay frutas)\n",
    "                - Canal 2: Posiciones de venenos (1.0 donde hay venenos)\n",
    "                \n",
    "        Nota: Las paredes no se incluyen en el estado ya que son est谩ticas\n",
    "              y se manejan a trav茅s de las restricciones de movimiento.\n",
    "        \"\"\"\n",
    "        estado = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posici贸n del agente\n",
    "        estado[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruta in self.frutas:\n",
    "            estado[1, fruta[0], fruta[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de venenos\n",
    "        for veneno in self.venenos:\n",
    "            estado[2, veneno[0], veneno[1]] = 1.0\n",
    "            \n",
    "        return estado\n",
    "\n",
    "    def dibujar(\n",
    "        self,\n",
    "        pantalla,\n",
    "        modo_juego,\n",
    "        cursor_pos,\n",
    "        img_fruta,\n",
    "        img_veneno,\n",
    "        img_pared,\n",
    "        img_agente,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Renderiza el entorno completo en la pantalla de Pygame.\n",
    "        \n",
    "        Este m茅todo se encarga de dibujar todos los elementos visuales del juego,\n",
    "        incluyendo la grilla, elementos del entorno, el agente, el cursor (en modo setup)\n",
    "        y la informaci贸n de controles.\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde dibujar\n",
    "            modo_juego (str): Modo actual (\"SETUP\" o \"PLAYING\")\n",
    "            cursor_pos (tuple): Posici贸n del cursor en modo setup\n",
    "            img_fruta (pygame.Surface): Imagen de la fruta\n",
    "            img_veneno (pygame.Surface): Imagen del veneno\n",
    "            img_pared (pygame.Surface): Imagen de la pared\n",
    "            img_agente (pygame.Surface): Imagen del agente\n",
    "        \n",
    "        Elementos visuales renderizados:\n",
    "            1. Fondo y grilla\n",
    "            2. Paredes (obst谩culos est谩ticos)\n",
    "            3. Frutas (objetivos a recolectar)\n",
    "            4. Venenos (elementos a evitar)\n",
    "            5. Agente (jugador controlado por IA)\n",
    "            6. Cursor (solo en modo setup)\n",
    "            7. Informaci贸n de controles y modo actual\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar l铆neas de la grilla (verticales)\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "            \n",
    "        # Dibujar l铆neas de la grilla (horizontales)\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibujar elementos del entorno en orden de capas\n",
    "        # 1. Paredes (fondo)\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "            \n",
    "        # 2. Frutas (objetivos)\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "            \n",
    "        # 3. Venenos (peligros)\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "\n",
    "        # 4. Agente (primer plano)\n",
    "        pantalla.blit(\n",
    "            img_agente, (self.agent_pos[0] * CELL_SIZE, self.agent_pos[1] * CELL_SIZE)\n",
    "        )\n",
    "\n",
    "        # 5. Cursor (solo en modo setup)\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar informaci贸n textual\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        \n",
    "        # Informaci贸n del modo actual\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Controles para modo setup\n",
    "        controles1 = font.render(\n",
    "            \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\", True, COLOR_TEXTO\n",
    "        )\n",
    "        \n",
    "        # Controles generales\n",
    "        controles2 = font.render(\"P=Jugar, S=Setup\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Posicionar textos en la parte inferior\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles1, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles2, (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "\n",
    "# --- MAIN CON INTERFAZ COMPLETA ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal que ejecuta la interfaz gr谩fica completa del sistema DDQN.\n",
    "    \n",
    "    Esta funci贸n implementa el bucle principal del programa, manejando:\n",
    "    - Inicializaci贸n de Pygame y carga de recursos\n",
    "    - Gesti贸n de eventos de teclado para ambos modos\n",
    "    - Alternancia entre modo setup y modo juego\n",
    "    - Renderizado continuo de la interfaz\n",
    "    - Ejecuci贸n autom谩tica del agente en modo juego\n",
    "    \n",
    "    Modos de operaci贸n:\n",
    "        SETUP: Permite configurar el entorno interactivamente\n",
    "            - Flechas: Mover cursor\n",
    "            - F: A帽adir/quitar fruta\n",
    "            - V: A帽adir/quitar veneno  \n",
    "            - W: A帽adir/quitar pared\n",
    "            - C: Limpiar entorno\n",
    "            \n",
    "        PLAYING: El agente entrenado juega autom谩ticamente\n",
    "            - Usa el modelo DDQN cargado para tomar decisiones\n",
    "            - Visualiza el comportamiento del agente en tiempo real\n",
    "            - Termina autom谩ticamente y vuelve a setup al completar\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y crear ventana\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente DDQN - Come Frutas 锔\")\n",
    "\n",
    "    def cargar_img(nombre, color_fallback):\n",
    "        \"\"\"\n",
    "        Carga una imagen desde archivo con fallback a color s贸lido.\n",
    "        \n",
    "        Esta funci贸n auxiliar intenta cargar una imagen desde el directorio\n",
    "        del script. Si falla, crea una superficie de color s贸lido como respaldo.\n",
    "        \n",
    "        Args:\n",
    "            nombre (str): Nombre del archivo de imagen\n",
    "            color_fallback (tuple): Color RGB de respaldo si falla la carga\n",
    "            \n",
    "        Returns:\n",
    "            pygame.Surface: Superficie escalada al tama帽o de celda\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "            img = pygame.image.load(ruta).convert_alpha()\n",
    "            return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "        except:\n",
    "            # Crear superficie de color s贸lido como fallback\n",
    "            surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "            surf.fill(color_fallback)\n",
    "            return surf\n",
    "\n",
    "    # Cargar im谩genes con colores de respaldo\n",
    "    img_fruta = cargar_img(\"fruta.png\", (0, 255, 0))      # Verde si falla\n",
    "    img_veneno = cargar_img(\"veneno.png\", (255, 0, 0))     # Rojo si falla\n",
    "    img_pared = cargar_img(\"pared.png\", (100, 100, 100))   # Gris si falla\n",
    "    img_agente = cargar_img(\"agente.png\", (0, 0, 255))     # Azul si falla\n",
    "\n",
    "    # Inicializar componentes principales\n",
    "    entorno = EntornoGrid()\n",
    "    agente = Agent(state_shape=(3, GRID_HEIGHT, GRID_WIDTH), action_size=4)\n",
    "    agente.load(\"dqn_model.pth\")  # Cargar modelo entrenado\n",
    "\n",
    "    # Variables de control de la interfaz\n",
    "    cursor_pos = [0, 0]      # Posici贸n del cursor en modo setup\n",
    "    modo_juego = \"SETUP\"     # Modo inicial\n",
    "    reloj = pygame.time.Clock()  # Control de FPS\n",
    "    corriendo = True\n",
    "\n",
    "    # Bucle principal del programa\n",
    "    while corriendo:\n",
    "        # Procesar eventos de Pygame\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # Cambios de modo (disponibles en cualquier momento)\n",
    "                if evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO JUEGO ---\")\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    time.sleep(0.5)  # Pausa para evitar acciones inmediatas\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # Controles espec铆ficos del modo SETUP\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Movimiento del cursor\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Gesti贸n de elementos en la posici贸n del cursor\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    \n",
    "                    if evento.key == pygame.K_f:\n",
    "                        # Alternar fruta en posici贸n actual\n",
    "                        if pos in entorno.frutas:\n",
    "                            entorno.frutas.remove(pos)\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos)\n",
    "                            # Remover otros elementos de la misma posici贸n\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        # Alternar veneno en posici贸n actual\n",
    "                        if pos in entorno.venenos:\n",
    "                            entorno.venenos.remove(pos)\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos)\n",
    "                            # Remover otros elementos de la misma posici贸n\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        # Alternar pared en posici贸n actual\n",
    "                        if pos in entorno.paredes:\n",
    "                            entorno.paredes.remove(pos)\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos)\n",
    "                            # Remover otros elementos de la misma posici贸n\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        # Limpiar todo el entorno\n",
    "                        print(\"--- LIMPIANDO ENTORNO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "\n",
    "        # L贸gica del modo PLAYING (agente autom谩tico)\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # Obtener estado actual del entorno\n",
    "            estado = entorno.get_state()\n",
    "            \n",
    "            # El agente elige una acci贸n usando el modelo entrenado\n",
    "            # explore=False significa que usa solo explotaci贸n (sin exploraci贸n)\n",
    "            accion = agente.choose_action(estado, explore=False)\n",
    "            \n",
    "            # Ejecutar la acci贸n en el entorno\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            \n",
    "            # Si el episodio termin贸, volver al modo setup\n",
    "            if terminado:\n",
    "                print(\"Juego terminado. Volviendo a SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "                \n",
    "            # Controlar velocidad de visualizaci贸n (10 FPS para el agente)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # Renderizado de la interfaz\n",
    "        # Crear superficie completa incluyendo espacio para texto\n",
    "        pantalla_con_info = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "        pantalla_con_info.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar el entorno en la superficie\n",
    "        entorno.dibujar(\n",
    "            pantalla_con_info,\n",
    "            modo_juego,\n",
    "            tuple(cursor_pos),\n",
    "            img_fruta,\n",
    "            img_veneno,\n",
    "            img_pared,\n",
    "            img_agente,\n",
    "        )\n",
    "        \n",
    "        # Copiar la superficie completa a la pantalla principal\n",
    "        pantalla.blit(pantalla_con_info, (0, 0))\n",
    "        \n",
    "        # Actualizar la pantalla y controlar FPS\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(60)  # Limitar a 60 FPS para suavidad visual\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa.\n",
    "    \n",
    "    Ejecuta la funci贸n main() solo cuando el archivo se ejecuta directamente,\n",
    "    no cuando se importa como m贸dulo. Esto permite reutilizar las clases\n",
    "    y funciones en otros scripts sin ejecutar autom谩ticamente la interfaz.\n",
    "    \"\"\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf36f8",
   "metadata": {},
   "source": [
    "#### interfaztrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn_agente_comefrutas.py\n",
    "\"\"\"\n",
    "Interfaz de entrenamiento interactiva para agentes DDQN.\n",
    "\n",
    "Este m贸dulo proporciona una interfaz gr谩fica completa que permite:\n",
    "- Configurar entornos de entrenamiento de manera interactiva\n",
    "- Entrenar agentes DDQN con visualizaci贸n en tiempo real\n",
    "- Evaluar el rendimiento del agente despu茅s del entrenamiento\n",
    "- Gestionar el ciclo completo de desarrollo de IA: setup  entrenamiento  evaluaci贸n\n",
    "\n",
    "La interfaz integra tres modos principales:\n",
    "1. SETUP: Configuraci贸n interactiva del entorno\n",
    "2. TRAINING: Entrenamiento autom谩tico del agente DDQN\n",
    "3. PLAYING: Evaluaci贸n visual del agente entrenado\n",
    "\n",
    "Dise帽ado para facilitar la experimentaci贸n y el desarrollo iterativo de agentes\n",
    "de aprendizaje por refuerzo en entornos de grilla.\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from agent import Agent\n",
    "from environment import GridEnvironment\n",
    "\n",
    "# --- CONFIGURACIN DEL ENTORNO Y VISUALIZACIN ---\n",
    "\"\"\"Par谩metros principales del sistema de entrenamiento.\"\"\"\n",
    "GRID_SIZE = 5           # Tama帽o de la grilla (5x5)\n",
    "CELL_SIZE = 120         # Tama帽o en p铆xeles de cada celda\n",
    "SCREEN_WIDTH = GRID_SIZE * CELL_SIZE    # Ancho total de la ventana\n",
    "SCREEN_HEIGHT = GRID_SIZE * CELL_SIZE   # Alto total de la ventana\n",
    "\n",
    "# Esquema de colores para la interfaz\n",
    "COLOR_FONDO = (25, 25, 25)        # Fondo oscuro para mejor contraste\n",
    "COLOR_LINEAS = (40, 40, 40)       # L铆neas sutiles de la grilla\n",
    "COLOR_CURSOR = (255, 255, 0)      # Cursor amarillo brillante\n",
    "COLOR_TEXTO = (230, 230, 230)     # Texto claro y legible\n",
    "\n",
    "# --- PARMETROS DE ENTRENAMIENTO ---\n",
    "\"\"\"Configuraci贸n del proceso de entrenamiento DDQN.\"\"\"\n",
    "NUM_EPISODIOS_ENTRENAMIENTO = 3000    # N煤mero total de episodios de entrenamiento\n",
    "BATCH_SIZE = 128                      # Tama帽o del lote para replay de experiencias\n",
    "\n",
    "\n",
    "def cargar_imagen(ruta, color_si_falla):\n",
    "    \"\"\"\n",
    "    Carga una imagen desde archivo con sistema de fallback robusto.\n",
    "    \n",
    "    Esta funci贸n implementa un mecanismo de carga de im谩genes que garantiza\n",
    "    que el programa funcione incluso si los archivos de imagen no est谩n\n",
    "    disponibles, creando superficies de color como respaldo.\n",
    "    \n",
    "    Args:\n",
    "        ruta (str): Ruta relativa o absoluta al archivo de imagen\n",
    "        color_si_falla (tuple): Color RGB (r, g, b) a usar si falla la carga\n",
    "        \n",
    "    Returns:\n",
    "        pygame.Surface: Superficie escalada al tama帽o de celda, ya sea la\n",
    "                       imagen cargada o una superficie de color s贸lido\n",
    "                       \n",
    "    Caracter铆sticas:\n",
    "        - Manejo autom谩tico de transparencia (convert_alpha)\n",
    "        - Escalado autom谩tico al tama帽o de celda\n",
    "        - Fallback graceful a color s贸lido\n",
    "        - Compatible con todos los formatos soportados por Pygame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Intentar cargar la imagen desde archivo\n",
    "        img = pygame.image.load(ruta).convert_alpha()\n",
    "        # Escalar al tama帽o exacto de celda para consistencia visual\n",
    "        return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "    except pygame.error:\n",
    "        # Si falla la carga, crear superficie de color s贸lido\n",
    "        surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "        surf.fill(color_si_falla)\n",
    "        return surf\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal que ejecuta la interfaz de entrenamiento DDQN.\n",
    "    \n",
    "    Esta funci贸n implementa un sistema completo de desarrollo de agentes IA que incluye:\n",
    "    \n",
    "    1. **Configuraci贸n Interactiva (Modo SETUP)**:\n",
    "       - Dise帽o visual del entorno usando cursor\n",
    "       - Colocaci贸n de frutas, venenos y paredes\n",
    "       - Validaci贸n de configuraciones\n",
    "    \n",
    "    2. **Entrenamiento Automatizado (Modo TRAINING)**:\n",
    "       - Ejecuci贸n de algoritmo DDQN completo\n",
    "       - Actualizaci贸n de redes objetivo\n",
    "       - Monitoreo de progreso en tiempo real\n",
    "       - Gesti贸n de memoria de experiencias\n",
    "    \n",
    "    3. **Evaluaci贸n Visual (Modo PLAYING)**:\n",
    "       - Visualizaci贸n del comportamiento aprendido\n",
    "       - Modo sin exploraci贸n (solo explotaci贸n)\n",
    "       - An谩lisis cualitativo del rendimiento\n",
    "    \n",
    "    Flujo de trabajo t铆pico:\n",
    "        SETUP  TRAINING  PLAYING  [iteraci贸n]\n",
    "    \n",
    "    La interfaz permite experimentaci贸n r谩pida con diferentes configuraciones\n",
    "    de entorno y hiperpar谩metros de entrenamiento.\n",
    "    \"\"\"\n",
    "    # Inicializaci贸n del sistema gr谩fico\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente Come-Frutas DDQN 锔\")\n",
    "\n",
    "    # Cargar recursos visuales con colores de fallback espec铆ficos\n",
    "    img_fruta = cargar_imagen(\"fruta.png\", (40, 200, 40))    # Verde si falla\n",
    "    img_veneno = cargar_imagen(\"veneno.png\", (200, 40, 40))   # Rojo si falla\n",
    "    img_pared = cargar_imagen(\"pared.png\", (100, 100, 100))   # Gris si falla\n",
    "    img_agente = cargar_imagen(\"agente.png\", (40, 200, 40))   # Verde si falla\n",
    "\n",
    "    # Inicializaci贸n de componentes principales del sistema\n",
    "    entorno = GridEnvironment(size=GRID_SIZE)  # Entorno de simulaci贸n\n",
    "    agente = Agent(state_shape=(3, GRID_SIZE, GRID_SIZE), action_size=4)  # Agente DDQN\n",
    "\n",
    "    # Variables de control de la interfaz\n",
    "    cursor_pos = [0, 0]        # Posici贸n del cursor en modo setup\n",
    "    modo_juego = \"SETUP\"       # Estado inicial del sistema\n",
    "    reloj = pygame.time.Clock()  # Control de framerate\n",
    "    corriendo = True           # Flag principal del bucle\n",
    "\n",
    "    # Conjuntos para gestionar elementos del entorno configurables\n",
    "    frutas = set()    # Posiciones de objetivos (recompensa positiva)\n",
    "    venenos = set()   # Posiciones de peligros (penalizaci贸n)\n",
    "    paredes = set()   # Posiciones de obst谩culos (bloqueo de movimiento)\n",
    "\n",
    "    # Bucle principal del sistema de entrenamiento\n",
    "    while corriendo:\n",
    "        # Procesamiento de eventos del usuario\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # --- CONTROL DE ENTRENAMIENTO (Tecla T) ---\n",
    "                if evento.key == pygame.K_t:\n",
    "                    if modo_juego != \"TRAINING\":\n",
    "                        print(\"--- ENTRENANDO DDQN ---\")\n",
    "                        modo_juego = \"TRAINING\"\n",
    "                        \n",
    "                        # Bucle principal de entrenamiento DDQN\n",
    "                        for episodio in range(NUM_EPISODIOS_ENTRENAMIENTO):\n",
    "                            # Reiniciar entorno con configuraci贸n actual\n",
    "                            estado = entorno.reset(\n",
    "                                agent_pos=(0, 0),\n",
    "                                fruit_pos=list(frutas),\n",
    "                                poison_pos=list(venenos),\n",
    "                            )\n",
    "                            \n",
    "                            # Variables de control del episodio\n",
    "                            terminado = False\n",
    "                            total_reward = 0\n",
    "                            \n",
    "                            # Bucle del episodio individual\n",
    "                            while not terminado:\n",
    "                                # El agente elige acci贸n con exploraci贸n activa\n",
    "                                accion = agente.choose_action(estado, explore=True)\n",
    "                                \n",
    "                                # Ejecutar acci贸n y observar resultado\n",
    "                                nuevo_estado, recompensa, terminado = entorno.step(accion)\n",
    "                                \n",
    "                                # Almacenar experiencia en memoria de replay\n",
    "                                agente.remember(\n",
    "                                    estado, accion, recompensa, nuevo_estado, terminado\n",
    "                                )\n",
    "                                \n",
    "                                # Entrenar la red con experiencias pasadas\n",
    "                                agente.replay(BATCH_SIZE)\n",
    "                                \n",
    "                                # Actualizaci贸n peri贸dica de la red objetivo\n",
    "                                if agente.steps_done % agente.update_target_every == 0:\n",
    "                                    agente.update_target_network()\n",
    "                                \n",
    "                                # Preparar para siguiente paso\n",
    "                                estado = nuevo_estado\n",
    "                                total_reward += recompensa\n",
    "                            \n",
    "                            # Reporte de progreso cada 100 episodios\n",
    "                            if (episodio + 1) % 100 == 0:\n",
    "                                print(\n",
    "                                    f\"Ep {episodio+1}, Reward: {total_reward:.2f}, Epsilon: {agente.epsilon:.3f}\"\n",
    "                                )\n",
    "                        \n",
    "                        print(\"--- ENTRENAMIENTO COMPLETO ---\")\n",
    "                        modo_juego = \"PLAYING\"  # Cambiar autom谩ticamente a evaluaci贸n\n",
    "\n",
    "                # --- CONTROL DE MODOS (Teclas P y S) ---\n",
    "                elif evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO PLAYING ---\")\n",
    "                    # Reiniciar entorno para evaluaci贸n\n",
    "                    entorno.reset(\n",
    "                        agent_pos=(0, 0),\n",
    "                        fruit_pos=list(frutas),\n",
    "                        poison_pos=list(venenos),\n",
    "                    )\n",
    "                    modo_juego = \"PLAYING\"\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # --- CONTROLES DEL MODO SETUP ---\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Control de navegaci贸n del cursor\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_SIZE - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_SIZE - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Conversi贸n de coordenadas (cursor usa x,y pero entorno usa y,x)\n",
    "                    pos = tuple(cursor_pos[::-1])\n",
    "                    \n",
    "                    # Gesti贸n de elementos en la posici贸n del cursor\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        # Alternar fruta en posici贸n actual\n",
    "                        if pos in frutas:\n",
    "                            frutas.remove(pos)\n",
    "                        else:\n",
    "                            frutas.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posici贸n\n",
    "                            venenos.discard(pos)\n",
    "                            paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        # Alternar veneno en posici贸n actual\n",
    "                        if pos in venenos:\n",
    "                            venenos.remove(pos)\n",
    "                        else:\n",
    "                            venenos.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posici贸n\n",
    "                            frutas.discard(pos)\n",
    "                            paredes.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        # Alternar pared en posici贸n actual\n",
    "                        if pos in paredes:\n",
    "                            paredes.remove(pos)\n",
    "                        else:\n",
    "                            paredes.add(pos)\n",
    "                            # Limpiar otros elementos de la misma posici贸n\n",
    "                            frutas.discard(pos)\n",
    "                            venenos.discard(pos)\n",
    "                            \n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        # Limpiar completamente el entorno\n",
    "                        frutas.clear()\n",
    "                        venenos.clear()\n",
    "                        paredes.clear()\n",
    "\n",
    "        # --- LGICA DEL MODO PLAYING ---\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # Obtener estado actual del entorno\n",
    "            estado = entorno.get_state()\n",
    "            \n",
    "            # El agente toma decisiones sin exploraci贸n (solo explotaci贸n)\n",
    "            accion = agente.choose_action(estado, explore=False)\n",
    "            \n",
    "            # Ejecutar acci贸n y verificar si termin贸 el episodio\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            \n",
    "            if terminado:\n",
    "                print(\"Juego terminado. Volviendo a SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "            \n",
    "            # Control de velocidad de visualizaci贸n\n",
    "            time.sleep(0.1)  # 10 FPS para observar mejor el comportamiento\n",
    "\n",
    "        # --- SISTEMA DE RENDERIZADO COMPLETO ---\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar grilla de referencia\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Renderizar elementos del entorno (orden de capas importante)\n",
    "        # Nota: Las coordenadas se invierten para coincidir con el sistema visual\n",
    "        \n",
    "        # 1. Paredes (capa de fondo)\n",
    "        for pared in paredes:\n",
    "            pantalla.blit(img_pared, (pared[1] * CELL_SIZE, pared[0] * CELL_SIZE))\n",
    "            \n",
    "        # 2. Frutas (objetivos)\n",
    "        for fruta in frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[1] * CELL_SIZE, fruta[0] * CELL_SIZE))\n",
    "            \n",
    "        # 3. Venenos (peligros)\n",
    "        for veneno in venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[1] * CELL_SIZE, veneno[0] * CELL_SIZE))\n",
    "\n",
    "        # 4. Agente (solo visible fuera del modo setup)\n",
    "        if modo_juego != \"SETUP\":\n",
    "            pos = entorno.agent_pos\n",
    "            pantalla.blit(img_agente, (pos[1] * CELL_SIZE, pos[0] * CELL_SIZE))\n",
    "\n",
    "        # 5. Cursor (solo visible en modo setup)\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # --- INTERFAZ DE INFORMACIN Y CONTROLES ---\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        \n",
    "        # Mostrar modo actual\n",
    "        pantalla.blit(\n",
    "            font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO),\n",
    "            (10, SCREEN_HEIGHT + 5),\n",
    "        )\n",
    "        \n",
    "        # Controles para modo setup\n",
    "        pantalla.blit(\n",
    "            font.render(\n",
    "                \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\",\n",
    "                True,\n",
    "                COLOR_TEXTO,\n",
    "            ),\n",
    "            (10, SCREEN_HEIGHT + 30),\n",
    "        )\n",
    "        \n",
    "        # Controles generales del sistema\n",
    "        pantalla.blit(\n",
    "            font.render(\"T=Entrenar, P=Jugar, S=Setup\", True, COLOR_TEXTO),\n",
    "            (10, SCREEN_HEIGHT + 55),\n",
    "        )\n",
    "\n",
    "        # Actualizar pantalla y controlar framerate\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(60)  # Limitar a 60 FPS para suavidad visual\n",
    "\n",
    "    # Limpiar recursos al finalizar\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa de entrenamiento.\n",
    "    \n",
    "    Ejecuta la funci贸n main() cuando el archivo se ejecuta directamente.\n",
    "    Este patr贸n permite importar las funciones y clases de este m贸dulo\n",
    "    en otros scripts sin ejecutar autom谩ticamente la interfaz de entrenamiento.\n",
    "    \n",
    "    Uso t铆pico:\n",
    "        python interfaztrain.py  # Ejecuta la interfaz completa\n",
    "        \n",
    "    O desde otro script:\n",
    "        from interfaztrain import cargar_imagen  # Solo importa funciones\n",
    "    \"\"\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e6105",
   "metadata": {},
   "source": [
    "#### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e278b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\"\"\"\n",
    "Script de entrenamiento principal para el agente DDQN (Double Deep Q-Network).\n",
    "\n",
    "Este m贸dulo implementa el proceso completo de entrenamiento del agente DDQN para\n",
    "el problema de recolecci贸n de frutas evitando venenos. El entrenamiento utiliza\n",
    "generaci贸n aleatoria de escenarios para garantizar la generalizaci贸n del agente.\n",
    "\n",
    "Caracter铆sticas principales del entrenamiento:\n",
    "- Generaci贸n aleatoria de entornos para cada episodio\n",
    "- Implementaci贸n completa del algoritmo DDQN\n",
    "- Actualizaci贸n peri贸dica de la red objetivo\n",
    "- Guardado autom谩tico del modelo durante el entrenamiento\n",
    "- Monitoreo del progreso con m茅tricas de rendimiento\n",
    "\n",
    "El sistema est谩 dise帽ado para entrenar un agente robusto capaz de manejar\n",
    "una amplia variedad de configuraciones de entorno, desde escenarios simples\n",
    "hasta configuraciones complejas con m煤ltiples obst谩culos y objetivos.\n",
    "\n",
    "Algoritmo implementado:\n",
    "- Double Deep Q-Network (DDQN) con replay buffer\n",
    "- Exploraci贸n epsilon-greedy con decaimiento\n",
    "- Actualizaci贸n peri贸dica de red objetivo\n",
    "- Entrenamiento continuo con experiencias almacenadas\n",
    "\"\"\"\n",
    "\n",
    "from environment import GridEnvironment\n",
    "from agent import Agent\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- CONFIGURACIN DE ENTRENAMIENTO ---\n",
    "\"\"\"Hiperpar谩metros principales del proceso de entrenamiento.\"\"\"\n",
    "EPISODES = 25000    # N煤mero total de episodios de entrenamiento (juegos completos)\n",
    "GRID_SIZE = 5       # Tama帽o de la grilla del entorno (5x5 celdas)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Funci贸n principal que ejecuta el proceso completo de entrenamiento DDQN.\n",
    "    \n",
    "    Este bloque implementa el algoritmo de entrenamiento completo, incluyendo:\n",
    "    - Inicializaci贸n del entorno y agente\n",
    "    - Generaci贸n aleatoria de escenarios de entrenamiento\n",
    "    - Bucle principal de entrenamiento con DDQN\n",
    "    - Gesti贸n de experiencias y actualizaci贸n de redes\n",
    "    - Monitoreo y guardado del progreso\n",
    "    \n",
    "    El entrenamiento utiliza curriculum learning impl铆cito a trav茅s de la\n",
    "    variabilidad aleatoria de escenarios, exponiendo al agente a una amplia\n",
    "    gama de situaciones para mejorar la generalizaci贸n.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- INICIALIZACIN DE COMPONENTES ---\n",
    "    env = GridEnvironment(size=GRID_SIZE)              # Entorno de simulaci贸n\n",
    "    state_shape = (3, GRID_SIZE, GRID_SIZE)           # Forma del estado: 3 canales x 5x5\n",
    "    action_size = 4                                    # N煤mero de acciones posibles (4 direcciones)\n",
    "    agent = Agent(state_shape, action_size)           # Agente DDQN con arquitectura CNN\n",
    "    \n",
    "    # --- CONFIGURACIN DE HIPERPARMETROS ---\n",
    "    batch_size = 128    # Tama帽o del lote para entrenamiento de la red neural\n",
    "                       # Un batch size mayor proporciona gradientes m谩s estables\n",
    "\n",
    "    # --- BUCLE PRINCIPAL DE ENTRENAMIENTO ---\n",
    "    for e in range(EPISODES):\n",
    "        # --- GENERACIN ALEATORIA DE ESCENARIOS ---\n",
    "        \"\"\"\n",
    "        Cada episodio utiliza una configuraci贸n completamente aleatoria del entorno.\n",
    "        Esta estrategia es FUNDAMENTAL para la generalizaci贸n del agente, ya que\n",
    "        evita el sobreajuste a configuraciones espec铆ficas y fuerza al agente\n",
    "        a aprender estrategias robustas que funcionen en cualquier escenario.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Determinar n煤mero aleatorio de elementos en el entorno\n",
    "        num_fruits = np.random.randint(1, 5)    # Entre 1 y 4 frutas\n",
    "        num_poisons = np.random.randint(1, 4)   # Entre 1 y 3 venenos\n",
    "        \n",
    "        # Generar posiciones 煤nicas para todos los elementos\n",
    "        # Esto previene superposiciones y garantiza configuraciones v谩lidas\n",
    "        all_pos = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)]\n",
    "        random.shuffle(all_pos)  # Mezclar aleatoriamente todas las posiciones\n",
    "        \n",
    "        # Asignar posiciones 煤nicas para cada elemento\n",
    "        agent_pos = all_pos.pop()                           # Posici贸n inicial del agente\n",
    "        fruit_pos = [all_pos.pop() for _ in range(num_fruits)]   # Posiciones de frutas\n",
    "        poison_pos = [all_pos.pop() for _ in range(num_poisons)] # Posiciones de venenos\n",
    "\n",
    "        # Reiniciar entorno con la configuraci贸n generada\n",
    "        state = env.reset(agent_pos=agent_pos, fruit_pos=fruit_pos, poison_pos=poison_pos)\n",
    "        \n",
    "        # --- EJECUCIN DEL EPISODIO ---\n",
    "        \"\"\"\n",
    "        Cada episodio simula un juego completo donde el agente debe recolectar\n",
    "        todas las frutas mientras evita los venenos. El l铆mite de 50 pasos\n",
    "        previene episodios infinitos y fuerza al agente a ser eficiente.\n",
    "        \"\"\"\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Bucle de pasos dentro del episodio (m谩ximo 50 pasos)\n",
    "        for time in range(50):\n",
    "            # El agente elige una acci贸n usando la pol铆tica epsilon-greedy\n",
    "            # Durante el entrenamiento, explora aleatoriamente con probabilidad epsilon\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Ejecutar la acci贸n en el entorno\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Almacenar la experiencia en el buffer de replay\n",
    "            # Esta experiencia se usar谩 m谩s tarde para entrenar la red\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Actualizar estado y acumular recompensa\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # --- ACTUALIZACIN DE LA RED OBJETIVO ---\n",
    "            \"\"\"\n",
    "            La red objetivo se actualiza peri贸dicamente para estabilizar el entrenamiento.\n",
    "            Esto es una caracter铆stica clave del algoritmo DQN que previene la\n",
    "            divergencia durante el entrenamiento.\n",
    "            \"\"\"\n",
    "            if agent.steps_done % agent.update_target_every == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "            # Terminar episodio si se complet贸 el objetivo\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # --- MONITOREO DEL PROGRESO ---\n",
    "        \"\"\"\n",
    "        Imprimir estad铆sticas del episodio para monitorear el progreso del entrenamiento.\n",
    "        - Puntuaci贸n total: Indica qu茅 tan bien est谩 aprendiendo el agente\n",
    "        - Epsilon: Muestra el balance actual entre exploraci贸n y explotaci贸n\n",
    "        \"\"\"\n",
    "        print(f\"Episodio: {e+1}/{EPISODES}, Puntuaci贸n: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "        # --- ENTRENAMIENTO DE LA RED NEURAL ---\n",
    "        \"\"\"\n",
    "        El entrenamiento se realiza despu茅s de cada episodio usando experiencias\n",
    "        almacenadas en el buffer de replay. Esto permite que el agente aprenda\n",
    "        de experiencias pasadas, mejorando la eficiencia del aprendizaje.\n",
    "        \"\"\"\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "        # --- GUARDADO PERIDICO DEL MODELO ---\n",
    "        \"\"\"\n",
    "        Guardar el modelo cada 50 episodios para:\n",
    "        - Prevenir p茅rdida de progreso en caso de interrupciones\n",
    "        - Permitir evaluaci贸n de versiones intermedias\n",
    "        - Facilitar la reanudaci贸n del entrenamiento si es necesario\n",
    "        \"\"\"\n",
    "        if e % 50 == 0:\n",
    "            agent.save(\"dqn_model.pth\")\n",
    "\n",
    "    # --- FINALIZACIN DEL ENTRENAMIENTO ---\n",
    "    print(\"Entrenamiento finalizado. Modelo guardado en 'dqn_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9294a4",
   "metadata": {},
   "source": [
    "#### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6238cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\"\"\"\n",
    "Demostraci贸n interactiva del agente DDQN entrenado.\n",
    "\n",
    "Este m贸dulo proporciona una interfaz de demostraci贸n simple donde los usuarios pueden:\n",
    "- Configurar entornos personalizados mediante clics del mouse\n",
    "- Observar el comportamiento del agente DDQN entrenado en tiempo real\n",
    "- Experimentar con diferentes configuraciones de frutas y venenos\n",
    "\n",
    "El sistema est谩 dise帽ado como una demostraci贸n p煤blica o para validaci贸n\n",
    "del rendimiento del agente en escenarios definidos por el usuario.\n",
    "\n",
    "Caracter铆sticas principales:\n",
    "- Interfaz minimalista y f谩cil de usar\n",
    "- Carga autom谩tica del modelo entrenado\n",
    "- Visualizaci贸n en tiempo real del agente\n",
    "- Reinicio autom谩tico para m煤ltiples demostraciones\n",
    "\n",
    "Flujo de trabajo:\n",
    "1. Modo SETUP: El usuario configura frutas y venenos con clics\n",
    "2. Modo RUN: El agente ejecuta la soluci贸n autom谩ticamente\n",
    "3. Reinicio autom谩tico al completar la demostraci贸n\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "from environment import GridEnvironment\n",
    "from agent import Agent\n",
    "\n",
    "# --- CONFIGURACIN DE LA INTERFAZ GRFICA ---\n",
    "\"\"\"Par谩metros de visualizaci贸n y configuraci贸n de la ventana.\"\"\"\n",
    "GRID_SIZE = 5           # Tama帽o de la grilla (5x5)\n",
    "CELL_SIZE = 100         # Tama帽o de cada celda en p铆xeles\n",
    "WIDTH, HEIGHT = GRID_SIZE * CELL_SIZE, GRID_SIZE * CELL_SIZE  # Dimensiones de ventana\n",
    "WIN = pygame.display.set_mode((WIDTH, HEIGHT))               # Ventana principal\n",
    "pygame.display.set_caption(\"Agente Come-Frutas\")            # T铆tulo de la ventana\n",
    "pygame.font.init()                                           # Inicializar sistema de fuentes\n",
    "\n",
    "# --- INICIALIZACIN DEL AGENTE ENTRENADO ---\n",
    "\"\"\"\n",
    "Configuraci贸n y carga del agente DDQN preentrenado.\n",
    "\n",
    "Este bloque inicializa los componentes necesarios para ejecutar\n",
    "el agente entrenado en modo demostraci贸n.\n",
    "\"\"\"\n",
    "env = GridEnvironment(size=GRID_SIZE)                    # Entorno de simulaci贸n\n",
    "action_size = 4                                          # N煤mero de acciones posibles (4 direcciones)\n",
    "state_shape = (3, GRID_SIZE, GRID_SIZE)                 # Forma del estado: 3 canales x 5x5\n",
    "agent = Agent(state_shape, action_size)                 # Crear instancia del agente\n",
    "agent.load(\"dqn_model.pth\")                            # Cargar modelo preentrenado\n",
    "\n",
    "# --- ESQUEMA DE COLORES PARA VISUALIZACIN ---\n",
    "\"\"\"Colores RGB para los diferentes elementos del juego.\"\"\"\n",
    "COLOR_GRID = (200, 200, 200)    # Gris claro para las l铆neas de la grilla\n",
    "COLOR_AGENT = (0, 0, 255)       # Azul para el agente\n",
    "COLOR_FRUIT = (0, 255, 0)       # Verde para las frutas (objetivos)\n",
    "COLOR_POISON = (255, 0, 0)      # Rojo para los venenos (peligros)\n",
    "\n",
    "\n",
    "def draw_grid():\n",
    "    \"\"\"\n",
    "    Dibuja las l铆neas de la grilla en la ventana.\n",
    "    \n",
    "    Crea una cuadr铆cula visual que ayuda a los usuarios a identificar\n",
    "    las posiciones disponibles para colocar elementos durante el modo setup.\n",
    "    \n",
    "    La grilla se dibuja con l铆neas verticales y horizontales espaciadas\n",
    "    uniformemente seg煤n el tama帽o de celda configurado.\n",
    "    \"\"\"\n",
    "    # Dibujar l铆neas verticales\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        pygame.draw.line(WIN, COLOR_GRID, (x, 0), (x, HEIGHT))\n",
    "    \n",
    "    # Dibujar l铆neas horizontales  \n",
    "    for y in range(0, HEIGHT, CELL_SIZE):\n",
    "        pygame.draw.line(WIN, COLOR_GRID, (0, y), (WIDTH, y))\n",
    "\n",
    "\n",
    "def draw_elements(agent_pos, fruits, poisons):\n",
    "    \"\"\"\n",
    "    Renderiza todos los elementos del juego en la pantalla.\n",
    "    \n",
    "    Esta funci贸n se encarga de dibujar visualmente todos los componentes\n",
    "    del entorno: el agente, las frutas y los venenos, usando formas\n",
    "    geom茅tricas distintivas para cada tipo de elemento.\n",
    "    \n",
    "    Args:\n",
    "        agent_pos (np.array): Posici贸n actual del agente [fila, columna]\n",
    "        fruits (list): Lista de posiciones de frutas [(fila, col), ...]\n",
    "        poisons (list): Lista de posiciones de venenos [(fila, col), ...]\n",
    "    \n",
    "    Representaciones visuales:\n",
    "        - Agente: Rect谩ngulo azul que ocupa toda la celda\n",
    "        - Frutas: C铆rculos verdes centrados en las celdas\n",
    "        - Venenos: Cuadrados rojos m谩s peque帽os dentro de las celdas\n",
    "    \"\"\"\n",
    "    # Dibujar agente como rect谩ngulo azul\n",
    "    # Nota: Se intercambian coordenadas (agent_pos[1], agent_pos[0]) para\n",
    "    # convertir de coordenadas de matriz (fila, columna) a pantalla (x, y)\n",
    "    pygame.draw.rect(\n",
    "        WIN,\n",
    "        COLOR_AGENT,\n",
    "        (agent_pos[1] * CELL_SIZE, agent_pos[0] * CELL_SIZE, CELL_SIZE, CELL_SIZE),\n",
    "    )\n",
    "    \n",
    "    # Dibujar frutas como c铆rculos verdes\n",
    "    for f in fruits:\n",
    "        center_x = f[1] * CELL_SIZE + CELL_SIZE // 2  # Centro horizontal\n",
    "        center_y = f[0] * CELL_SIZE + CELL_SIZE // 2  # Centro vertical\n",
    "        radius = CELL_SIZE // 3                       # Radio del c铆rculo\n",
    "        pygame.draw.circle(WIN, COLOR_FRUIT, (center_x, center_y), radius)\n",
    "    \n",
    "    # Dibujar venenos como cuadrados rojos m谩s peque帽os\n",
    "    for p in poisons:\n",
    "        # Crear un margen de 20 p铆xeles alrededor del cuadrado\n",
    "        x = p[1] * CELL_SIZE + 20\n",
    "        y = p[0] * CELL_SIZE + 20\n",
    "        size = CELL_SIZE - 40  # Tama帽o reducido del cuadrado\n",
    "        pygame.draw.rect(WIN, COLOR_POISON, (x, y, size, size))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal que ejecuta la demostraci贸n interactiva del agente DDQN.\n",
    "    \n",
    "    Esta funci贸n implementa un sistema de dos modos que permite a los usuarios\n",
    "    configurar entornos personalizados y observar el comportamiento del agente.\n",
    "    \n",
    "    Modos de operaci贸n:\n",
    "    \n",
    "    1. **MODO SETUP** (Configuraci贸n interactiva):\n",
    "       - Permite al usuario colocar elementos usando el mouse\n",
    "       - Clic izquierdo: A帽adir frutas (objetivos)\n",
    "       - Clic derecho: A帽adir venenos (obst谩culos peligrosos)\n",
    "       - Barra espaciadora: Iniciar simulaci贸n\n",
    "    \n",
    "    2. **MODO RUN** (Demostraci贸n del agente):\n",
    "       - El agente DDQN toma control total\n",
    "       - Ejecuta acciones basadas en el modelo entrenado\n",
    "       - Visualizaci贸n en tiempo real del comportamiento\n",
    "       - Finalizaci贸n autom谩tica y reinicio\n",
    "    \n",
    "    El sistema est谩 dise帽ado para demostraciones p煤blicas, permitiendo\n",
    "    m煤ltiples usuarios configurar y probar diferentes escenarios.\n",
    "    \"\"\"\n",
    "    # Variables de estado del sistema\n",
    "    fruits = []           # Lista de posiciones de frutas configuradas por el usuario\n",
    "    poisons = []          # Lista de posiciones de venenos configuradas por el usuario\n",
    "    mode = \"setup\"        # Modo inicial: \"setup\" para configuraci贸n, \"run\" para ejecuci贸n\n",
    "\n",
    "    # Configuraci贸n del bucle principal\n",
    "    clock = pygame.time.Clock()  # Control de framerate\n",
    "    run = True                   # Flag principal del bucle\n",
    "\n",
    "    # Bucle principal de la demostraci贸n\n",
    "    while run:\n",
    "        # Limpiar pantalla con fondo negro\n",
    "        WIN.fill((0, 0, 0))\n",
    "        \n",
    "        # Dibujar grilla de referencia\n",
    "        draw_grid()\n",
    "\n",
    "        # Procesar eventos del usuario\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "\n",
    "            # --- LGICA DEL MODO SETUP ---\n",
    "            if mode == \"setup\":\n",
    "                # Manejo de clics del mouse para colocar elementos\n",
    "                if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                    pos = pygame.mouse.get_pos()\n",
    "                    col = pos[0] // CELL_SIZE  # Convertir coordenada x a columna\n",
    "                    row = pos[1] // CELL_SIZE  # Convertir coordenada y a fila\n",
    "\n",
    "                    # Clic izquierdo: A帽adir fruta (si no existe ya)\n",
    "                    if event.button == 1 and (row, col) not in fruits:\n",
    "                        fruits.append((row, col))\n",
    "                        \n",
    "                    # Clic derecho: A帽adir veneno (si no existe ya)\n",
    "                    elif event.button == 3 and (row, col) not in poisons:\n",
    "                        poisons.append((row, col))\n",
    "\n",
    "                # Tecla espaciadora: Iniciar simulaci贸n\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_SPACE:\n",
    "                        mode = \"run\"\n",
    "                        # Configurar el entorno con los elementos del usuario\n",
    "                        state = env.reset(\n",
    "                            agent_pos=(0, 0),      # Agente siempre inicia en (0,0)\n",
    "                            fruit_pos=fruits,      # Frutas configuradas por el usuario\n",
    "                            poison_pos=poisons     # Venenos configurados por el usuario\n",
    "                        )\n",
    "                        print(\"Iniciando simulaci贸n...\")\n",
    "\n",
    "        # --- RENDERIZADO SEGN EL MODO ACTUAL ---\n",
    "        if mode == \"setup\":\n",
    "            # Modo configuraci贸n: Mostrar elementos colocados por el usuario\n",
    "            # El agente se dibuja fuera de pantalla (posici贸n inv谩lida) para ocultarlo\n",
    "            draw_elements(\n",
    "                np.array([-1, -1]),  # Posici贸n fuera de pantalla para el agente\n",
    "                fruits,              # Frutas configuradas por el usuario\n",
    "                poisons              # Venenos configurados por el usuario\n",
    "            )\n",
    "\n",
    "        elif mode == \"run\":\n",
    "            # --- LGICA DEL AGENTE AUTNOMO ---\n",
    "            # Obtener estado actual del entorno\n",
    "            state = env.get_state()\n",
    "            \n",
    "            # El agente elige la mejor acci贸n sin exploraci贸n\n",
    "            # explore=False asegura que use solo la pol铆tica aprendida\n",
    "            action = agent.choose_action(state, explore=False)\n",
    "            \n",
    "            # Ejecutar la acci贸n en el entorno\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Renderizar estado actual con posiciones reales del entorno\n",
    "            draw_elements(\n",
    "                env.agent_pos,    # Posici贸n actual del agente\n",
    "                env.fruit_pos,    # Frutas restantes en el entorno\n",
    "                env.poison_pos    # Venenos en el entorno\n",
    "            )\n",
    "\n",
    "            # Verificar si el episodio termin贸\n",
    "            if done:\n",
    "                print(\"隆Simulaci贸n terminada!\")\n",
    "                # Reiniciar sistema para nueva demostraci贸n\n",
    "                fruits = []        # Limpiar frutas configuradas\n",
    "                poisons = []       # Limpiar venenos configurados\n",
    "                mode = \"setup\"     # Volver al modo configuraci贸n\n",
    "                pygame.time.delay(2000)  # Pausa de 2 segundos antes del reinicio\n",
    "\n",
    "            # Control de velocidad de visualizaci贸n\n",
    "            pygame.time.delay(300)  # Pausa de 300ms para observar movimientos\n",
    "\n",
    "        # Actualizar pantalla para mostrar cambios\n",
    "        pygame.display.update()\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa de demostraci贸n.\n",
    "    \n",
    "    Ejecuta la funci贸n main() cuando el archivo se ejecuta directamente.\n",
    "    Este patr贸n permite importar funciones de este m贸dulo en otros scripts\n",
    "    sin ejecutar autom谩ticamente la demostraci贸n.\n",
    "    \n",
    "    Uso t铆pico:\n",
    "        python main.py  # Ejecuta la demostraci贸n interactiva\n",
    "        \n",
    "    La demostraci贸n est谩 dise帽ada para:\n",
    "    - Presentaciones p煤blicas del proyecto\n",
    "    - Validaci贸n r谩pida del comportamiento del agente\n",
    "    - Experimentaci贸n interactiva con diferentes configuraciones\n",
    "    - Evaluaci贸n cualitativa del rendimiento del modelo\n",
    "    \"\"\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4021b",
   "metadata": {},
   "source": [
    "### Agente Gen茅tico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a90e4b",
   "metadata": {},
   "source": [
    "#### agente_ga.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_ga.py\n",
    "\"\"\"\n",
    "Implementaci贸n de un agente basado en algoritmos gen茅ticos para el entorno de recolecci贸n de frutas.\n",
    "\n",
    "Este m贸dulo define la arquitectura de red neuronal y la clase agente utilizados en el enfoque\n",
    "de algoritmos gen茅ticos. A diferencia del DQN que aprende mediante gradientes, este agente\n",
    "evoluciona sus pesos mediante selecci贸n natural, mutaci贸n y cruzamiento.\n",
    "\n",
    "Componentes principales:\n",
    "- AgentNetwork: Red neuronal convolucional para procesar el estado visual\n",
    "- Agent: Wrapper que contiene la red y maneja la evaluaci贸n de fitness\n",
    "\n",
    "El agente procesa el estado del entorno (representado como una imagen de 3 canales)\n",
    "y produce directamente acciones sin necesidad de aprendizaje por refuerzo.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# La arquitectura de la red puede ser la misma CNN que ya ten铆amos.\n",
    "# Es una buena forma de procesar la \"visi贸n\" del agente.\n",
    "class AgentNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal convolucional para el agente gen茅tico.\n",
    "    \n",
    "    Esta red procesa la representaci贸n visual del entorno (estado como imagen de 3 canales)\n",
    "    y produce valores de acci贸n para las 4 direcciones posibles. La arquitectura utiliza\n",
    "    capas convolucionales para extraer caracter铆sticas espaciales, seguidas de capas\n",
    "    densas para la toma de decisiones.\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Conv2D (316 canales) + ReLU\n",
    "    - Conv2D (1632 canales) + ReLU  \n",
    "    - Flatten\n",
    "    - Dense (256) + ReLU\n",
    "    - Dense (4 acciones)\n",
    "    \n",
    "    Args:\n",
    "        h (int): Altura de la cuadr铆cula de entrada (default: 5)\n",
    "        w (int): Ancho de la cuadr铆cula de entrada (default: 5)\n",
    "        outputs (int): N煤mero de acciones posibles (default: 4)\n",
    "    \"\"\"\n",
    "    def __init__(self, h=5, w=5, outputs=4):\n",
    "        super(AgentNetwork, self).__init__()\n",
    "        \n",
    "        # Capas convolucionales para procesamiento espacial del estado visual\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            \"\"\"\n",
    "            Calcula el tama帽o de salida despu茅s de una operaci贸n de convoluci贸n.\n",
    "            Formula: (entrada + 2*padding - kernel_size) // stride + 1\n",
    "            \"\"\"\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        # Calcular dimensiones para la capa lineal despu茅s de las convoluciones\n",
    "        # Como usamos padding=1 y kernel=3, las dimensiones se mantienen iguales\n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32  # 32 es el n煤mero de canales de salida de conv2\n",
    "        \n",
    "        # Capas densas para la toma de decisiones\n",
    "        # Capas densas para la toma de decisiones\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)  # Capa oculta con 256 neuronas\n",
    "        self.fc2 = nn.Linear(256, outputs)            # Capa de salida con 4 acciones\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagaci贸n hacia adelante de la red neuronal.\n",
    "        \n",
    "        Procesa el estado visual del entorno a trav茅s de las capas convolucionales\n",
    "        y densas para producir valores de acci贸n.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Estado del entorno de forma (batch_size, 3, h, w)\n",
    "                            - Canal 0: Posici贸n del agente\n",
    "                            - Canal 1: Posiciones de frutas  \n",
    "                            - Canal 2: Posiciones de venenos\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Valores de acci贸n de forma (batch_size, 4)\n",
    "                         Cada valor representa la \"utilidad\" de una acci贸n:\n",
    "                         - ndice 0: Arriba\n",
    "                         - ndice 1: Abajo\n",
    "                         - ndice 2: Izquierda\n",
    "                         - ndice 3: Derecha\n",
    "        \"\"\"\n",
    "        # Primera capa convolucional + activaci贸n ReLU\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        \n",
    "        # Segunda capa convolucional + activaci贸n ReLU\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        \n",
    "        # Aplanar tensor para capas densas: (batch, channels*h*w)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Primera capa densa + activaci贸n ReLU\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        \n",
    "        # Capa de salida (sin activaci贸n, valores raw para argmax)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# El Agente es ahora solo una c谩scara con su red y una puntuaci贸n de fitness.\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Wrapper del agente para algoritmos gen茅ticos.\n",
    "    \n",
    "    Esta clase encapsula la red neuronal y proporciona la interfaz necesaria\n",
    "    para el algoritmo gen茅tico. A diferencia de los agentes de RL, este agente\n",
    "    no aprende durante la ejecuci贸n; su comportamiento est谩 completamente\n",
    "    determinado por los pesos de la red neuronal (sus \"genes\").\n",
    "    \n",
    "    El agente se eval煤a mediante su fitness (rendimiento en el entorno),\n",
    "    y los mejores agentes se seleccionan para reproducirse y crear la\n",
    "    siguiente generaci贸n mediante:\n",
    "    - Selecci贸n: Los mejores agentes tienen mayor probabilidad de reproducirse\n",
    "    - Cruzamiento: Combinaci贸n de genes de dos padres\n",
    "    - Mutaci贸n: Cambios aleatorios en los genes\n",
    "    \n",
    "    Attributes:\n",
    "        network (AgentNetwork): Red neuronal que define el comportamiento del agente\n",
    "        fitness (float): Puntuaci贸n de rendimiento en el entorno (mayor = mejor)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa un nuevo agente con red neuronal y fitness en cero.\n",
    "        \n",
    "        Los pesos de la red se inicializan aleatoriamente seg煤n la \n",
    "        inicializaci贸n por defecto de PyTorch. Estos pesos representan\n",
    "        los \"genes\" del agente que evolucionar谩n con el tiempo.\n",
    "        \"\"\"\n",
    "        self.network = AgentNetwork()\n",
    "        self.fitness = 0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Selecciona una acci贸n basada en el estado actual del entorno.\n",
    "        \n",
    "        El agente utiliza su red neuronal para evaluar el estado y selecciona\n",
    "        la acci贸n con el valor m谩s alto (estrategia greedy). No hay exploraci贸n\n",
    "        ya que el comportamiento del agente est谩 completamente determinado por\n",
    "        sus genes (pesos de la red).\n",
    "        \n",
    "        Este m茅todo es determin铆stico: dado el mismo estado y los mismos pesos,\n",
    "        siempre producir谩 la misma acci贸n. Esto es importante para la evaluaci贸n\n",
    "        consistente del fitness durante la evoluci贸n.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Estado del entorno de forma (3, h, w)\n",
    "                             - Canal 0: Posici贸n del agente (1.0 donde est谩, 0.0 resto)\n",
    "                             - Canal 1: Posiciones de frutas (1.0 donde hay frutas)\n",
    "                             - Canal 2: Posiciones de venenos (1.0 donde hay venenos)\n",
    "        \n",
    "        Returns:\n",
    "            int: Acci贸n seleccionada:\n",
    "                 - 0: Mover arriba (decrementar fila)\n",
    "                 - 1: Mover abajo (incrementar fila)\n",
    "                 - 2: Mover izquierda (decrementar columna)\n",
    "                 - 3: Mover derecha (incrementar columna)\n",
    "        \"\"\"\n",
    "        # Convertir estado NumPy a tensor PyTorch y agregar dimensi贸n de batch\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Evaluaci贸n sin gradientes (no hay backpropagation)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.network(state_tensor)\n",
    "        \n",
    "        # Seleccionar acci贸n con mayor valor Q (estrategia greedy)\n",
    "        return torch.argmax(action_values).item()\n",
    "\n",
    "    def load_genes(self, filepath):\n",
    "        \"\"\"\n",
    "        Carga los \"genes\" (pesos de la red) desde un archivo.\n",
    "        \n",
    "        Utilizado para cargar agentes previamente evolucionados y demostrar\n",
    "        su comportamiento. Los pesos representan el \"ADN\" del agente que\n",
    "        determina completamente su comportamiento en el entorno.\n",
    "        \n",
    "        Este m茅todo es 煤til para:\n",
    "        - Cargar el mejor agente de una evoluci贸n anterior\n",
    "        - Demostrar el comportamiento de agentes elite\n",
    "        - Continuar la evoluci贸n desde una generaci贸n guardada\n",
    "        - An谩lisis y visualizaci贸n del comportamiento aprendido\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Ruta al archivo con los pesos del modelo\n",
    "                           (normalmente un archivo .pth de PyTorch)\n",
    "                           \n",
    "        Raises:\n",
    "            FileNotFoundError: Si el archivo no existe\n",
    "            RuntimeError: Si los pesos no coinciden con la arquitectura\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.network.load_state_dict(torch.load(filepath))\n",
    "            print(f\" Genes cargados exitosamente desde: {filepath}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\" Error: No se encontr贸 el archivo {filepath}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\" Error cargando genes: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277ac25",
   "metadata": {},
   "source": [
    "#### genetico_agente.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demostrador interactivo para agentes entrenados con algoritmos gen茅ticos.\n",
    "\n",
    "Este m贸dulo implementa una interfaz gr谩fica completa que permite:\n",
    "1. Configurar escenarios personalizados con frutas, venenos y paredes\n",
    "2. Observar el comportamiento de un agente gen茅tico entrenado\n",
    "3. Interactuar en tiempo real con controles de teclado\n",
    "4. Visualizar el rendimiento del agente en diferentes configuraciones\n",
    "\n",
    "Caracter铆sticas principales:\n",
    "- Modo Setup: Configuraci贸n manual del entorno\n",
    "- Modo Playing: Demostraci贸n del agente en acci贸n\n",
    "- Controles intuitivos con teclado\n",
    "- Gr谩ficos mejorados con sprites\n",
    "- Interfaz informativa con instrucciones\n",
    "\n",
    "El agente carga pesos previamente evolucionados y demuestra su comportamiento\n",
    "determin铆stico en los escenarios configurados por el usuario.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from agent_ga import Agent\n",
    "\n",
    "# CONFIGURACIN VISUAL Y DIMENSIONES\n",
    "\"\"\"\n",
    "Constantes que definen la apariencia y dimensiones de la interfaz gr谩fica.\n",
    "\"\"\"\n",
    "GRID_WIDTH = 5          # Ancho de la cuadr铆cula en celdas\n",
    "GRID_HEIGHT = 5         # Alto de la cuadr铆cula en celdas  \n",
    "CELL_SIZE = 120         # Tama帽o de cada celda en p铆xeles (m谩s grande que en DQN)\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la ventana (600px)\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto total del 谩rea de juego (600px)\n",
    "\n",
    "# PALETA DE COLORES PROFESIONAL\n",
    "\"\"\"\n",
    "Esquema de colores dark theme para una interfaz moderna y profesional.\n",
    "\"\"\"\n",
    "COLOR_FONDO = (25, 25, 25)      # Gris muy oscuro para el fondo\n",
    "COLOR_LINEAS = (40, 40, 40)     # Gris oscuro para l铆neas de cuadr铆cula\n",
    "COLOR_CURSOR = (255, 255, 0)    # Amarillo brillante para el cursor de selecci贸n\n",
    "COLOR_TEXTO = (230, 230, 230)   # Gris claro para texto legible\n",
    "\n",
    "\n",
    "class EntornoGrid:\n",
    "    \"\"\"\n",
    "    Entorno de cuadr铆cula personalizado para la demostraci贸n del agente gen茅tico.\n",
    "    \n",
    "    Esta clase maneja la l贸gica del juego y la configuraci贸n del entorno,\n",
    "    incluyendo la colocaci贸n de elementos y la simulaci贸n de la interacci贸n\n",
    "    del agente. Incluye caracter铆sticas adicionales como paredes que no\n",
    "    est谩n presentes en los entornos de entrenamiento b谩sicos.\n",
    "    \n",
    "    Caracter铆sticas especiales:\n",
    "    - Soporte para paredes como obst谩culos\n",
    "    - Interfaz de configuraci贸n manual\n",
    "    - Reset autom谩tico en condiciones de terminaci贸n\n",
    "    - Estado visual compatible con el agente entrenado\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tama帽o de la cuadr铆cula\n",
    "        agent_pos (tuple): Posici贸n actual del agente (fila, columna)\n",
    "        frutas (set): Conjunto de posiciones con frutas\n",
    "        venenos (set): Conjunto de posiciones con venenos  \n",
    "        paredes (set): Conjunto de posiciones con paredes (obst谩culos)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuraci贸n vac铆a.\n",
    "        \n",
    "        Todos los conjuntos de elementos comienzan vac铆os, permitiendo\n",
    "        al usuario configurar el escenario manualmente.\n",
    "        \"\"\"\n",
    "        self.size = GRID_WIDTH\n",
    "        self.agent_pos = (0, 0)    # Agente siempre inicia en esquina superior izquierda\n",
    "        self.frutas = set()        # Conjunto de posiciones de frutas\n",
    "        self.venenos = set()       # Conjunto de posiciones de venenos\n",
    "        self.paredes = set()       # Conjunto de posiciones de paredes\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"\n",
    "        Resetea el agente a la posici贸n inicial sin modificar el entorno.\n",
    "        \n",
    "        Utilizado al inicio de cada demostraci贸n para colocar al agente\n",
    "        en la posici贸n de partida est谩ndar (0,0) manteniendo la configuraci贸n\n",
    "        de frutas, venenos y paredes establecida por el usuario.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno despu茅s del reset\n",
    "        \"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno (frutas, venenos, paredes).\n",
    "        \n",
    "        Funci贸n de utilidad para resetear completamente el escenario,\n",
    "        permitiendo al usuario comenzar con una cuadr铆cula vac铆a.\n",
    "        El agente permanece en su posici贸n actual.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acci贸n del agente en el entorno de demostraci贸n.\n",
    "        \n",
    "        Implementa la l贸gica del juego incluyendo movimiento, colisiones con\n",
    "        paredes, interacci贸n con elementos del entorno y c谩lculo de recompensas.\n",
    "        Incluye caracter铆sticas especiales como paredes que bloquean el movimiento.\n",
    "        \n",
    "        Diferencias con el entorno de entrenamiento:\n",
    "        - Incluye paredes como obst谩culos\n",
    "        - Movimientos inv谩lidos dan recompensa negativa\n",
    "        - Reset autom谩tico al completar nivel\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Acci贸n a ejecutar:\n",
    "                         0 = Arriba (decrementar fila)\n",
    "                         1 = Abajo (incrementar fila) \n",
    "                         2 = Izquierda (decrementar columna)\n",
    "                         3 = Derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (estado, recompensa, terminado)\n",
    "                - estado (np.array): Nuevo estado del entorno\n",
    "                - recompensa (float): Recompensa obtenida\n",
    "                - terminado (bool): Si el episodio ha terminado\n",
    "        \"\"\"\n",
    "        # Calcular nueva posici贸n basada en la acci贸n\n",
    "        fila, col = self.agent_pos\n",
    "        if accion == 0:     # Arriba\n",
    "            fila -= 1\n",
    "        elif accion == 1:   # Abajo\n",
    "            fila += 1\n",
    "        elif accion == 2:   # Izquierda\n",
    "            col -= 1\n",
    "        elif accion == 3:   # Derecha\n",
    "            col += 1\n",
    "\n",
    "        # Verificar colisiones: l铆mites del tablero o paredes\n",
    "        if (\n",
    "            fila < 0\n",
    "            or fila >= GRID_HEIGHT\n",
    "            or col < 0\n",
    "            or col >= GRID_WIDTH\n",
    "            or (fila, col) in self.paredes\n",
    "        ):\n",
    "            # Movimiento inv谩lido: peque帽a penalizaci贸n, posici贸n no cambia\n",
    "            return self.get_state(), -0.1, False\n",
    "\n",
    "        # Movimiento v谩lido: actualizar posici贸n\n",
    "        self.agent_pos = (fila, col)\n",
    "        recompensa = -0.05    # Costo base del movimiento\n",
    "        terminado = False\n",
    "\n",
    "        # Verificar interacciones con elementos del entorno\n",
    "        if self.agent_pos in self.venenos:\n",
    "            # Veneno tocado: penalizaci贸n severa y reset a inicio\n",
    "            recompensa = -10.0\n",
    "            self.agent_pos = (0, 0)\n",
    "        elif self.agent_pos in self.frutas:\n",
    "            # Fruta recogida: recompensa positiva\n",
    "            recompensa = 1.0\n",
    "            self.frutas.remove(self.agent_pos)\n",
    "            \n",
    "            # Verificar si se complet贸 el nivel\n",
    "            if not self.frutas:\n",
    "                recompensa += 10.0    # Bonus por completar\n",
    "                terminado = True\n",
    "                self.agent_pos = (0, 0)  # Reset para pr贸xima demostraci贸n\n",
    "\n",
    "        return self.get_state(), recompensa, terminado\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera la representaci贸n del estado compatible con el agente entrenado.\n",
    "        \n",
    "        Crea una representaci贸n de 3 canales id茅ntica a la utilizada durante\n",
    "        el entrenamiento, asegurando compatibilidad con los pesos evolucionados.\n",
    "        Las paredes no se incluyen en el estado ya que el agente original\n",
    "        no fue entrenado con ellas.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado del entorno de forma (3, size, size):\n",
    "                     - Canal 0: Posici贸n del agente\n",
    "                     - Canal 1: Posiciones de frutas\n",
    "                     - Canal 2: Posiciones de venenos\n",
    "        \"\"\"\n",
    "        estado = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posici贸n del agente\n",
    "        estado[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruta in self.frutas:\n",
    "            estado[1, fruta[0], fruta[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de venenos  \n",
    "        for veneno in self.venenos:\n",
    "            estado[2, veneno[0], veneno[1]] = 1.0\n",
    "            \n",
    "        return estado\n",
    "\n",
    "    def dibujar(\n",
    "        self,\n",
    "        pantalla,\n",
    "        modo_juego,\n",
    "        cursor_pos,\n",
    "        img_fruta,\n",
    "        img_veneno,\n",
    "        img_pared,\n",
    "        img_agente,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Renderiza el estado completo del entorno en la pantalla.\n",
    "        \n",
    "        Dibuja todos los elementos visuales incluyendo cuadr铆cula, sprites,\n",
    "        cursor de selecci贸n (en modo setup) e interfaz de usuario con\n",
    "        controles e informaci贸n del modo actual.\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde dibujar\n",
    "            modo_juego (str): Modo actual (\"SETUP\" o \"PLAYING\")\n",
    "            cursor_pos (tuple): Posici贸n del cursor en modo setup\n",
    "            img_fruta (pygame.Surface): Sprite de la fruta\n",
    "            img_veneno (pygame.Surface): Sprite del veneno\n",
    "            img_pared (pygame.Surface): Sprite de la pared\n",
    "            img_agente (pygame.Surface): Sprite del agente\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar cuadr铆cula de referencia\n",
    "        # L铆neas verticales\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        # L铆neas horizontales\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibujar elementos del entorno (orden importante para superposici贸n correcta)\n",
    "        # 1. Paredes (fondo)\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "        \n",
    "        # 2. Frutas\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "        \n",
    "        # 3. Venenos\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "\n",
    "        # 4. Agente (primer plano)\n",
    "        pantalla.blit(\n",
    "            img_agente, (self.agent_pos[0] * CELL_SIZE, self.agent_pos[1] * CELL_SIZE)\n",
    "        )\n",
    "\n",
    "        # 5. Cursor de selecci贸n (solo en modo setup)\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Dibujar interfaz de usuario en la parte inferior\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        \n",
    "        # Informaci贸n del modo actual\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Controles disponibles\n",
    "        controles1 = font.render(\n",
    "            \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\", True, COLOR_TEXTO\n",
    "        )\n",
    "        controles2 = font.render(\"P=Jugar, S=Setup\", True, COLOR_TEXTO)\n",
    "        \n",
    "        # Posicionar texto en la parte inferior\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles1, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles2, (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal que ejecuta la aplicaci贸n de demostraci贸n.\n",
    "    \n",
    "    Inicializa Pygame, carga recursos gr谩ficos, configura el agente gen茅tico\n",
    "    entrenado y ejecuta el bucle principal de la aplicaci贸n. Maneja dos modos\n",
    "    principales: configuraci贸n manual y demostraci贸n autom谩tica.\n",
    "    \n",
    "    Flujo de ejecuci贸n:\n",
    "    1. Inicializaci贸n de Pygame y recursos\n",
    "    2. Carga del agente entrenado\n",
    "    3. Bucle principal con manejo de eventos\n",
    "    4. Renderizado continuo\n",
    "    5. Limpieza al salir\n",
    "    \"\"\"\n",
    "    # INICIALIZACIN DE PYGAME\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente Gen茅tico - Come Frutas \")\n",
    "\n",
    "    # FUNCIN AUXILIAR PARA CARGA DE IMGENES\n",
    "    def cargar_img(nombre, color_fallback):\n",
    "        \"\"\"\n",
    "        Carga una imagen con fallback a color s贸lido si falla.\n",
    "        \n",
    "        Args:\n",
    "            nombre (str): Nombre del archivo de imagen\n",
    "            color_fallback (tuple): Color RGB de respaldo\n",
    "            \n",
    "        Returns:\n",
    "            pygame.Surface: Superficie escalada al tama帽o de celda\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "            img = pygame.image.load(ruta).convert_alpha()\n",
    "            return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "        except:\n",
    "            # Fallback: crear superficie de color s贸lido\n",
    "            surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "            surf.fill(color_fallback)\n",
    "            return surf\n",
    "\n",
    "    # CARGA DE RECURSOS GRFICOS\n",
    "    img_fruta = cargar_img(\"../fruta.png\", (0, 255, 0))      # Verde si no hay imagen\n",
    "    img_veneno = cargar_img(\"../veneno.png\", (255, 0, 0))    # Rojo si no hay imagen  \n",
    "    img_pared = cargar_img(\"../pared.png\", (100, 100, 100)) # Gris si no hay imagen\n",
    "    img_agente = cargar_img(\"../agente.png\", (0, 0, 255))   # Azul si no hay imagen\n",
    "\n",
    "    # INICIALIZACIN DEL ENTORNO Y AGENTE\n",
    "    entorno = EntornoGrid()\n",
    "    agente = Agent()\n",
    "    \n",
    "    # Cargar agente entrenado con algoritmos gen茅ticos\n",
    "    agente.load_genes(\"GENETICO/best_agent_genes.pth\")\n",
    "\n",
    "    # VARIABLES DE ESTADO DE LA APLICACIN\n",
    "    cursor_pos = [0, 0]        # Posici贸n del cursor en modo setup\n",
    "    modo_juego = \"SETUP\"       # Modo inicial: configuraci贸n\n",
    "    reloj = pygame.time.Clock() # Control de FPS\n",
    "    corriendo = True           # Flag de control del bucle principal\n",
    "\n",
    "    # BUCLE PRINCIPAL DE LA APLICACIN\n",
    "    while corriendo:\n",
    "        # MANEJO DE EVENTOS\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            # EVENTOS DE TECLADO\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # CONTROLES GLOBALES (disponibles en ambos modos)\n",
    "                if evento.key == pygame.K_p:\n",
    "                    print(\"--- INICIANDO MODO JUEGO ---\")\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    time.sleep(0.5)  # Pausa para transici贸n visual\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- INICIANDO MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # CONTROLES ESPECFICOS DEL MODO SETUP\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Navegaci贸n con flechas del cursor\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Colocaci贸n/eliminaci贸n de elementos\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    \n",
    "                    # F = Toggle Fruta\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        if pos in entorno.frutas:\n",
    "                            entorno.frutas.remove(pos)\n",
    "                            print(f\"Fruta eliminada en {pos}\")\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos)\n",
    "                            entorno.venenos.discard(pos)    # Remover otros elementos\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            print(f\"Fruta colocada en {pos}\")\n",
    "                    \n",
    "                    # V = Toggle Veneno\n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        if pos in entorno.venenos:\n",
    "                            entorno.venenos.remove(pos)\n",
    "                            print(f\"Veneno eliminado en {pos}\")\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos)\n",
    "                            entorno.frutas.discard(pos)     # Remover otros elementos\n",
    "                            entorno.paredes.discard(pos)\n",
    "                            print(f\"Veneno colocado en {pos}\")\n",
    "                    \n",
    "                    # W = Toggle Pared\n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        if pos in entorno.paredes:\n",
    "                            entorno.paredes.remove(pos)\n",
    "                            print(f\"Pared eliminada en {pos}\")\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos)\n",
    "                            entorno.frutas.discard(pos)     # Remover otros elementos\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            print(f\"Pared colocada en {pos}\")\n",
    "                    \n",
    "                    # C = Limpiar todo\n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        print(\"--- LIMPIANDO ENTORNO COMPLETO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "\n",
    "        # LGICA DEL MODO PLAYING (DEMOSTRACIN DEL AGENTE)\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # El agente toma decisiones autom谩ticamente\n",
    "            estado = entorno.get_state()\n",
    "            accion = agente.choose_action(estado)\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            \n",
    "            # Verificar si el episodio termin贸\n",
    "            if terminado:\n",
    "                print(\" 隆Agente complet贸 el nivel! Volviendo a modo SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "            \n",
    "            # Pausa para visualizaci贸n clara del movimiento\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # RENDERIZADO (COMN PARA AMBOS MODOS)\n",
    "        # Crear superficie temporal para el contenido completo\n",
    "        pantalla_con_info = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "        pantalla_con_info.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar entorno y elementos\n",
    "        entorno.dibujar(\n",
    "            pantalla_con_info,\n",
    "            modo_juego,\n",
    "            tuple(cursor_pos),\n",
    "            img_fruta,\n",
    "            img_veneno,\n",
    "            img_pared,\n",
    "            img_agente,\n",
    "        )\n",
    "        \n",
    "        # Copiar a pantalla principal y actualizar\n",
    "        pantalla.blit(pantalla_con_info, (0, 0))\n",
    "        pygame.display.flip()\n",
    "        \n",
    "        # Controlar FPS\n",
    "        reloj.tick(60)\n",
    "\n",
    "    # LIMPIEZA AL SALIR\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Punto de entrada del programa.\n",
    "    \n",
    "    Ejecuta la funci贸n main() solo si este archivo se ejecuta directamente.\n",
    "    Incluye mensaje de bienvenida con instrucciones b谩sicas.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"К DEMOSTRADOR DE AGENTE GENTICO К\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CONTROLES:\")\n",
    "    print(\" GLOBALES:\")\n",
    "    print(\"  P - Iniciar modo Playing (demostraci贸n)\")\n",
    "    print(\"  S - Cambiar a modo Setup (configuraci贸n)\")\n",
    "    print()\n",
    "    print(\"锔  MODO SETUP:\")\n",
    "    print(\"  猬锔猬锔猬锔★ - Mover cursor\")\n",
    "    print(\"  F - Toggle Fruta\")\n",
    "    print(\"  V - Toggle Veneno\") \n",
    "    print(\"  W - Toggle Pared\")\n",
    "    print(\"  C - Limpiar entorno\")\n",
    "    print()\n",
    "    print(\" MODO PLAYING:\")\n",
    "    print(\"  El agente toma control autom谩ticamente\")\n",
    "    print(\"  Observa el comportamiento evolucionado\")\n",
    "    print()\n",
    "    print(\"隆Configura un escenario y observa la inteligencia artificial!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7db34f",
   "metadata": {},
   "source": [
    "#### enviroment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entorno de cuadr铆cula especializado para algoritmos gen茅ticos.\n",
    "\n",
    "Este m贸dulo implementa un entorno modificado para el entrenamiento de agentes mediante\n",
    "algoritmos gen茅ticos. La principal diferencia con el entorno DQN es el manejo de venenos:\n",
    "en lugar de terminar el episodio, el agente es enviado de vuelta a la posici贸n inicial\n",
    "con una penalizaci贸n, permitiendo episodios m谩s largos y mejor evaluaci贸n de fitness.\n",
    "\n",
    "Caracter铆sticas espec铆ficas para GA:\n",
    "- Venenos no terminan el episodio, sino que resetean la posici贸n\n",
    "- Episodios m谩s largos para mejor evaluaci贸n de fitness\n",
    "- Recompensas ajustadas para discriminar mejor entre agentes\n",
    "- Seguimiento de posici贸n inicial para reset de venenos\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GridEnvironment:\n",
    "    \"\"\"\n",
    "    Entorno de cuadr铆cula optimizado para algoritmos gen茅ticos.\n",
    "    \n",
    "    Este entorno est谩 dise帽ado espec铆ficamente para la evaluaci贸n de agentes\n",
    "    mediante algoritmos gen茅ticos. La principal modificaci贸n es que tocar venenos\n",
    "    no termina el episodio, sino que env铆a al agente de vuelta al inicio,\n",
    "    permitiendo episodios m谩s largos y una mejor discriminaci贸n entre agentes.\n",
    "    \n",
    "    Caracter铆sticas para GA:\n",
    "    - Episodios m谩s largos para mejor evaluaci贸n de fitness\n",
    "    - Venenos causan reset de posici贸n en lugar de game over\n",
    "    - Recompensas ajustadas para mejor selecci贸n evolutiva\n",
    "    - Seguimiento de posici贸n inicial para mec谩nica de reset\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tama帽o de la cuadr铆cula (size x size)\n",
    "        start_pos (np.array): Posici贸n inicial del agente en el episodio\n",
    "        agent_pos (np.array): Posici贸n actual del agente\n",
    "        fruit_pos (list): Lista de posiciones de frutas\n",
    "        poison_pos (list): Lista de posiciones de venenos\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de cuadr铆cula para algoritmos gen茅ticos.\n",
    "        \n",
    "        Args:\n",
    "            size (int, optional): Tama帽o de la cuadr铆cula. Por defecto es 5x5.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.start_pos = (0, 0)  # Guardar posici贸n inicial para reset de venenos\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, agent_pos=(0, 0), fruit_pos=[], poison_pos=[]):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno con una configuraci贸n espec铆fica.\n",
    "        \n",
    "        Establece las posiciones iniciales y guarda la posici贸n de inicio del agente\n",
    "        para la mec谩nica de reset por venenos. Esta posici贸n inicial es crucial\n",
    "        en el paradigma de algoritmos gen茅ticos ya que permite que el agente\n",
    "        contin煤e intentando despu茅s de errores.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (tuple, optional): Posici贸n inicial del agente (fila, columna). \n",
    "                                       Por defecto (0, 0).\n",
    "            fruit_pos (list, optional): Lista de tuplas con posiciones de frutas.\n",
    "                                       Por defecto lista vac铆a.\n",
    "            poison_pos (list, optional): Lista de tuplas con posiciones de venenos.\n",
    "                                        Por defecto lista vac铆a.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado inicial del entorno como array 3D (3, size, size).\n",
    "        \"\"\"\n",
    "        self.start_pos = np.array(agent_pos)  # Guardar posici贸n inicial del episodio\n",
    "        self.agent_pos = np.array(agent_pos)\n",
    "        self.fruit_pos = [np.array(p) for p in fruit_pos]\n",
    "        self.poison_pos = [np.array(p) for p in poison_pos]\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera la representaci贸n del estado actual del entorno.\n",
    "        \n",
    "        Id茅ntica implementaci贸n al entorno DQN. El estado se representa como una \n",
    "        \"imagen\" de 3 canales que puede ser procesada por redes convolucionales.\n",
    "        \n",
    "        - Canal 0: Posici贸n del agente (1.0 donde est谩 el agente, 0.0 en el resto)\n",
    "        - Canal 1: Posiciones de frutas (1.0 donde hay frutas, 0.0 en el resto)  \n",
    "        - Canal 2: Posiciones de venenos (1.0 donde hay venenos, 0.0 en el resto)\n",
    "        \n",
    "        Esta representaci贸n permite que el agente \"vea\" todo el entorno de una vez\n",
    "        y es compatible con arquitecturas de redes neuronales convolucionales.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Estado del entorno como array 3D de forma (3, size, size)\n",
    "                     con valores float32.\n",
    "        \"\"\"\n",
    "        state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posici贸n del agente\n",
    "        # Canal 0: Posici贸n del agente\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de las frutas\n",
    "        for fruit in self.fruit_pos:\n",
    "            state[1, fruit[0], fruit[1]] = 1.0\n",
    "            \n",
    "        # Canal 2: Posiciones de los venenos\n",
    "        for poison in self.poison_pos:\n",
    "            state[2, poison[0], poison[1]] = 1.0\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Ejecuta una acci贸n en el entorno optimizado para algoritmos gen茅ticos.\n",
    "        \n",
    "        Esta funci贸n implementa la l贸gica principal del juego con modificaciones\n",
    "        espec铆ficas para algoritmos gen茅ticos. La diferencia clave es el manejo\n",
    "        de venenos: en lugar de terminar el episodio, el agente se resetea a la\n",
    "        posici贸n inicial, permitiendo episodios m谩s largos y mejor evaluaci贸n.\n",
    "        \n",
    "        Diferencias con DQN:\n",
    "        - Venenos NO terminan el episodio\n",
    "        - Venenos resetean la posici贸n del agente al inicio\n",
    "        - Penalizaci贸n mayor por venenos (-10.0 vs -1.0)\n",
    "        - Episodios m谩s largos para mejor discriminaci贸n de fitness\n",
    "        \n",
    "        Args:\n",
    "            action (int): Acci贸n a realizar:\n",
    "                         0 = Arriba (decrementar fila)\n",
    "                         1 = Abajo (incrementar fila)  \n",
    "                         2 = Izquierda (decrementar columna)\n",
    "                         3 = Derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, terminado)\n",
    "                - nuevo_estado (np.array): Estado del entorno despu茅s de la acci贸n\n",
    "                - recompensa (float): Recompensa obtenida por la acci贸n\n",
    "                - terminado (bool): True solo si todas las frutas fueron recogidas\n",
    "        \"\"\"\n",
    "        \n",
    "        # FASE 1: MOVIMIENTO DEL AGENTE\n",
    "        # L贸gica id茅ntica al entorno DQN\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] -= 1    # Arriba\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] += 1    # Abajo\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] -= 1    # Izquierda\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] += 1    # Derecha\n",
    "            \n",
    "        # Limitar posici贸n a los l铆mites del tablero\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.size - 1)\n",
    "\n",
    "        # FASE 2: INICIALIZACIN DE RECOMPENSAS\n",
    "        reward = -0.05  # Peque帽o castigo por cada movimiento\n",
    "        done = False\n",
    "\n",
    "        # FASE 2: INICIALIZACIN DE RECOMPENSAS\n",
    "        reward = -0.05  # Peque帽o castigo por cada movimiento\n",
    "        done = False\n",
    "\n",
    "        # FASE 3: MANEJO ESPECIAL DE VENENOS (DIFERENCIA CLAVE CON DQN)\n",
    "        if any(np.array_equal(self.agent_pos, p) for p in self.poison_pos):\n",
    "            # Veneno tocado: penalizaci贸n severa pero NO termina el episodio\n",
    "            reward = -10.0\n",
    "            # CARACTERSTICA PRINCIPAL: Reset a posici贸n inicial\n",
    "            self.agent_pos = np.copy(self.start_pos)\n",
    "            # CRTICO: done permanece False, el episodio contin煤a\n",
    "            print(\" Agente toc贸 veneno, reseteado a posici贸n inicial\")\n",
    "        else:\n",
    "            # FASE 4: LGICA NORMAL (SOLO SI NO HAY VENENO)\n",
    "            # Verificar si se recogi贸 una fruta\n",
    "            eaten_fruit_this_step = False\n",
    "            for i, fruit in enumerate(self.fruit_pos):\n",
    "                if np.array_equal(self.agent_pos, fruit):\n",
    "                    reward += 1.0  # Recompensa por fruta\n",
    "                    self.fruit_pos.pop(i)  # Remover fruta del entorno\n",
    "                    eaten_fruit_this_step = True\n",
    "                    print(\" Fruta recogida!\")\n",
    "                    break  # Solo una fruta por paso\n",
    "\n",
    "            # Reward shaping opcional (sin implementar aqu铆)\n",
    "            if not eaten_fruit_this_step and self.fruit_pos:\n",
    "                # Aqu铆 se podr铆a agregar l贸gica de distancia como en DQN\n",
    "                # Dejado como comentario para mantener simplicidad\n",
    "                pass\n",
    "\n",
    "            # FASE 5: CONDICIN DE VICTORIA\n",
    "            if not self.fruit_pos:\n",
    "                done = True\n",
    "                reward += 10.0  # Gran recompensa por completar el nivel\n",
    "                print(\" 隆Todas las frutas recogidas! Episodio completado\")\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d01628",
   "metadata": {},
   "source": [
    "#### train_ga.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefa629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ga.py\n",
    "\"\"\"\n",
    "Implementaci贸n completa de algoritmo gen茅tico para entrenar agentes de IA.\n",
    "\n",
    "Este m贸dulo implementa un algoritmo gen茅tico completo que evoluciona poblaciones\n",
    "de agentes para resolver el problema de recolecci贸n de frutas. El algoritmo\n",
    "simula la evoluci贸n natural mediante selecci贸n, cruzamiento y mutaci贸n.\n",
    "\n",
    "Proceso evolutivo:\n",
    "1. Inicializaci贸n: Crear poblaci贸n aleatoria de agentes\n",
    "2. Evaluaci贸n: Probar cada agente en escenarios aleatorios  \n",
    "3. Selecci贸n: Elegir los mejores agentes como padres\n",
    "4. Cruzamiento: Combinar genes de padres para crear hijos\n",
    "5. Mutaci贸n: Introducir variaci贸n aleatoria en los genes\n",
    "6. Reemplazo: Formar nueva generaci贸n con elitismo\n",
    "7. Repetir hasta convergencia\n",
    "\n",
    "Caracter铆sticas del algoritmo:\n",
    "- Evaluaci贸n en escenarios aleatorios para robustez\n",
    "- Elitismo para preservar mejores soluciones\n",
    "- Mutaci贸n gaussiana para exploraci贸n controlada\n",
    "- Cruzamiento uniforme para recombinaci贸n equilibrada\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from environment import GridEnvironment\n",
    "from agent_ga import Agent, AgentNetwork\n",
    "import random\n",
    "\n",
    "# HIPERPARMETROS DEL ALGORITMO GENTICO\n",
    "\"\"\"\n",
    "Configuraci贸n de par谩metros evolutivos que controlan el comportamiento\n",
    "del algoritmo gen茅tico. Estos valores han sido ajustados emp铆ricamente\n",
    "para balancear exploraci贸n vs. explotaci贸n.\n",
    "\"\"\"\n",
    "POPULATION_SIZE = 100    # Tama帽o de la poblaci贸n por generaci贸n\n",
    "NUM_GENERATIONS = 500    # N煤mero total de generaciones a evolucionar\n",
    "MUTATION_RATE = 0.05     # Probabilidad de mutaci贸n por gen (5%)\n",
    "ELITISM_COUNT = 25       # Mejores agentes que pasan directamente (25%)\n",
    "\n",
    "GRID_SIZE = 5           # Tama帽o del entorno de evaluaci贸n\n",
    "\n",
    "def create_initial_population():\n",
    "    \"\"\"\n",
    "    Crea la poblaci贸n inicial de agentes con genes aleatorios.\n",
    "    \n",
    "    Genera una poblaci贸n de agentes donde cada uno tiene pesos de red neuronal\n",
    "    inicializados aleatoriamente seg煤n la inicializaci贸n por defecto de PyTorch.\n",
    "    Esta diversidad inicial es crucial para el 茅xito del algoritmo gen茅tico.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de POPULATION_SIZE agentes con genes aleatorios\n",
    "        \n",
    "    Note:\n",
    "        La diversidad gen茅tica inicial determina el espacio de b煤squeda\n",
    "        que el algoritmo puede explorar durante la evoluci贸n.\n",
    "    \"\"\"\n",
    "    return [Agent() for _ in range(POPULATION_SIZE)]\n",
    "\n",
    "def evaluate_fitness(population, env):\n",
    "    \"\"\"\n",
    "    Eval煤a el fitness de cada agente en la poblaci贸n.\n",
    "    \n",
    "    Cada agente se prueba en un escenario aleatorio generado din谩micamente.\n",
    "    La variabilidad en los escenarios asegura que los agentes desarrollen\n",
    "    estrategias robustas y generalizables en lugar de sobreajustarse a\n",
    "    configuraciones espec铆ficas.\n",
    "    \n",
    "    Proceso de evaluaci贸n:\n",
    "    1. Generar escenario aleatorio (posiciones, frutas, venenos)\n",
    "    2. Ejecutar agente por m谩ximo 50 pasos\n",
    "    3. Acumular recompensas totales como fitness\n",
    "    4. Repetir para todos los agentes de la poblaci贸n\n",
    "    \n",
    "    Args:\n",
    "        population (list): Lista de agentes a evaluar\n",
    "        env (GridEnvironment): Entorno de evaluaci贸n\n",
    "        \n",
    "    Note:\n",
    "        El fitness se calcula como la suma total de recompensas obtenidas\n",
    "        durante el episodio, incluyendo penalizaciones por movimientos,\n",
    "        recompensas por frutas y penalizaciones por venenos.\n",
    "    \"\"\"\n",
    "    for agent in population:\n",
    "        # GENERACIN DE ESCENARIO ALEATORIO\n",
    "        # N煤mero variable de frutas (1-4) para diversidad de dificultad\n",
    "        num_fruits = np.random.randint(1, 5)\n",
    "        \n",
    "        # Crear lista de todas las posiciones posibles\n",
    "        all_pos = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)]\n",
    "        random.shuffle(all_pos)  # Barajar para selecci贸n aleatoria\n",
    "        \n",
    "        # Asignar posiciones sin solapamiento\n",
    "        agent_pos = all_pos.pop()   # Posici贸n inicial del agente\n",
    "        fruit_pos = [all_pos.pop() for _ in range(num_fruits)]  # Posiciones de frutas\n",
    "        poison_pos = [all_pos.pop() for _ in range(np.random.randint(1, 4))]  # 1-3 venenos\n",
    "\n",
    "        # EJECUCIN DEL EPISODIO\n",
    "        state = env.reset(agent_pos=agent_pos, fruit_pos=fruit_pos, poison_pos=poison_pos)\n",
    "        total_reward = 0\n",
    "        \n",
    "        # M谩ximo 50 pasos para evitar episodios infinitos\n",
    "        for _ in range(50):\n",
    "            action = agent.choose_action(state)\n",
    "            state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Terminar si el agente completa el objetivo\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Asignar fitness como recompensa total acumulada\n",
    "        agent.fitness = total_reward\n",
    "\n",
    "def selection(population):\n",
    "    \"\"\"\n",
    "    Selecciona los mejores agentes de la poblaci贸n para reproducci贸n.\n",
    "    \n",
    "    Implementa selecci贸n elitista donde solo los agentes con mayor fitness\n",
    "    se seleccionan como padres para la siguiente generaci贸n. Este m茅todo\n",
    "    asegura que las caracter铆sticas exitosas se preserven y propaguen.\n",
    "    \n",
    "    Estrategia de selecci贸n:\n",
    "    - Ordenar poblaci贸n por fitness (mayor a menor)\n",
    "    - Seleccionar el 20% superior como padres\n",
    "    - Estos padres participar谩n en cruzamiento y algunos en elitismo\n",
    "    \n",
    "    Args:\n",
    "        population (list): Poblaci贸n de agentes evaluados\n",
    "        \n",
    "    Returns:\n",
    "        list: Los mejores agentes seleccionados para reproducci贸n\n",
    "        \n",
    "    Note:\n",
    "        Un porcentaje del 20% permite suficiente diversidad gen茅tica\n",
    "        mientras mantiene presi贸n selectiva hacia mejores soluciones.\n",
    "    \"\"\"\n",
    "    # Ordenar por fitness descendente (mejores primero)\n",
    "    population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "    \n",
    "    # Seleccionar el 20% superior de la poblaci贸n\n",
    "    return population[:int(POPULATION_SIZE * 0.2)]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    \"\"\"\n",
    "    Crea un nuevo agente combinando genes de dos padres.\n",
    "    \n",
    "    Implementa cruzamiento uniforme donde cada par谩metro (gen) del hijo\n",
    "    se hereda aleatoriamente de uno de los dos padres. Este m茅todo mantiene\n",
    "    bloques funcionales de la red mientras permite recombinaci贸n gen茅tica.\n",
    "    \n",
    "    Proceso de cruzamiento:\n",
    "    1. Crear nuevo agente hijo\n",
    "    2. Para cada par谩metro de la red neuronal:\n",
    "       - Probabilidad 50%: heredar del padre 1\n",
    "       - Probabilidad 50%: heredar del padre 2\n",
    "    3. Cargar genes combinados en el hijo\n",
    "    \n",
    "    Args:\n",
    "        parent1 (Agent): Primer padre seleccionado\n",
    "        parent2 (Agent): Segundo padre seleccionado\n",
    "        \n",
    "    Returns:\n",
    "        Agent: Nuevo agente hijo con genes combinados\n",
    "        \n",
    "    Note:\n",
    "        El cruzamiento uniforme preserva mejor las estructuras funcionales\n",
    "        de las redes neuronales comparado con cruzamiento de un punto.\n",
    "    \"\"\"\n",
    "    # Crear nuevo agente hijo\n",
    "    child = Agent()\n",
    "    \n",
    "    # Obtener diccionarios de par谩metros (genes) de los padres\n",
    "    p1_genes = parent1.network.state_dict()\n",
    "    p2_genes = parent2.network.state_dict()\n",
    "    child_genes = child.network.state_dict()\n",
    "\n",
    "    # Cruzamiento uniforme: cada gen se hereda aleatoriamente\n",
    "    for key in child_genes.keys():\n",
    "        # 50% probabilidad de heredar cada gen de cada padre\n",
    "        if random.random() < 0.5:\n",
    "            child_genes[key] = p1_genes[key].clone()  # Heredar del padre 1\n",
    "        else:\n",
    "            child_genes[key] = p2_genes[key].clone()  # Heredar del padre 2\n",
    "    \n",
    "    # Cargar genes combinados en la red del hijo\n",
    "    child.network.load_state_dict(child_genes)\n",
    "    return child\n",
    "\n",
    "def mutate(agent):\n",
    "    \"\"\"\n",
    "    Introduce variaci贸n gen茅tica aleatoria en un agente.\n",
    "    \n",
    "    Implementa mutaci贸n gaussiana donde cada par谩metro tiene una probabilidad\n",
    "    MUTATION_RATE de ser alterado con ruido gaussiano. Esta mutaci贸n permite\n",
    "    explorar nuevas regiones del espacio de b煤squeda y evitar convergencia\n",
    "    prematura a 贸ptimos locales.\n",
    "    \n",
    "    Proceso de mutaci贸n:\n",
    "    1. Para cada par谩metro de la red neuronal:\n",
    "       - Probabilidad MUTATION_RATE: agregar ruido gaussiano\n",
    "       - Magnitud del ruido: distribuci贸n normal =0.1\n",
    "    2. Recargar par谩metros modificados en la red\n",
    "    \n",
    "    Args:\n",
    "        agent (Agent): Agente a mutar\n",
    "        \n",
    "    Returns:\n",
    "        Agent: El mismo agente con genes posiblemente mutados\n",
    "        \n",
    "    Note:\n",
    "        La mutaci贸n gaussiana con =0.1 proporciona un balance entre\n",
    "        exploraci贸n (nuevas soluciones) y explotaci贸n (preservar buenas soluciones).\n",
    "    \"\"\"\n",
    "    child_genes = agent.network.state_dict()\n",
    "    \n",
    "    # Aplicar mutaci贸n a cada par谩metro independientemente\n",
    "    for key in child_genes.keys():\n",
    "        if random.random() < MUTATION_RATE:\n",
    "            # Agregar ruido gaussiano con desviaci贸n est谩ndar 0.1\n",
    "            noise = torch.randn_like(child_genes[key]) * 0.1\n",
    "            child_genes[key] += noise\n",
    "    \n",
    "    # Recargar par谩metros mutados en la red\n",
    "    agent.network.load_state_dict(child_genes)\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Bucle principal del algoritmo gen茅tico.\n",
    "    \n",
    "    Ejecuta el proceso evolutivo completo a trav茅s de m煤ltiples generaciones,\n",
    "    implementando el ciclo: evaluaci贸n  selecci贸n  cruzamiento  mutaci贸n.\n",
    "    Incluye elitismo para preservar las mejores soluciones y logging detallado\n",
    "    del progreso evolutivo.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"К INICIANDO ENTRENAMIENTO CON ALGORITMO GENTICO К\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Par谩metros de evoluci贸n:\")\n",
    "    print(f\" Tama帽o de poblaci贸n: {POPULATION_SIZE}\")\n",
    "    print(f\" Generaciones: {NUM_GENERATIONS}\")\n",
    "    print(f\" Tasa de mutaci贸n: {MUTATION_RATE*100}%\")\n",
    "    print(f\" Elitismo: {ELITISM_COUNT} agentes\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # INICIALIZACIN\n",
    "    env = GridEnvironment()\n",
    "    population = create_initial_population()\n",
    "    \n",
    "    print(\" Poblaci贸n inicial creada\")\n",
    "    print(\" Comenzando evoluci贸n...\")\n",
    "    print()\n",
    "\n",
    "    # BUCLE EVOLUTIVO PRINCIPAL\n",
    "    for gen in range(NUM_GENERATIONS):\n",
    "        print(f\" Generaci贸n {gen+1}/{NUM_GENERATIONS}\")\n",
    "        \n",
    "        # FASE 1: EVALUACIN DE FITNESS\n",
    "        evaluate_fitness(population, env)\n",
    "        \n",
    "        # FASE 2: SELECCIN DE PADRES\n",
    "        parents = selection(population)\n",
    "        \n",
    "        # FASE 3: ANLISIS DE PROGRESO\n",
    "        best_agent_of_gen = parents[0]  # El mejor agente de esta generaci贸n\n",
    "        best_fitness = best_agent_of_gen.fitness\n",
    "        avg_fitness = np.mean([agent.fitness for agent in population])\n",
    "        \n",
    "        # Logging del progreso evolutivo\n",
    "        print(f\"    Mejor fitness: {best_fitness:.2f}\")\n",
    "        print(f\"    Fitness promedio: {avg_fitness:.2f}\")\n",
    "        print(f\"    Mejora: {best_fitness - avg_fitness:.2f}\")\n",
    "\n",
    "        # FASE 4: PRESERVACIN DEL MEJOR AGENTE\n",
    "        # Guardar genes del mejor agente de esta generaci贸n\n",
    "        torch.save(best_agent_of_gen.network.state_dict(), \"best_agent_genes.pth\")\n",
    "        print(f\"    Mejor agente guardado\")\n",
    "\n",
    "        # FASE 5: CREACIN DE NUEVA GENERACIN\n",
    "        new_population = []\n",
    "        \n",
    "        # ELITISMO: Los mejores agentes pasan directamente\n",
    "        new_population.extend(parents[:ELITISM_COUNT])\n",
    "        print(f\"    {ELITISM_COUNT} elite preservados\")\n",
    "\n",
    "        # REPRODUCCIN: Llenar resto con descendencia\n",
    "        offspring_count = 0\n",
    "        while len(new_population) < POPULATION_SIZE:\n",
    "            # Seleccionar dos padres aleatoriamente del pool de elite\n",
    "            parent1, parent2 = random.sample(parents, 2)\n",
    "            \n",
    "            # Cruzamiento: combinar genes de padres\n",
    "            child = crossover(parent1, parent2)\n",
    "            \n",
    "            # Mutaci贸n: introducir variaci贸n gen茅tica\n",
    "            child = mutate(child)\n",
    "            \n",
    "            new_population.append(child)\n",
    "            offspring_count += 1\n",
    "            \n",
    "        print(f\"    {offspring_count} descendientes creados\")\n",
    "        \n",
    "        # Reemplazar poblaci贸n anterior\n",
    "        population = new_population\n",
    "        \n",
    "        print(\"    Generaci贸n completada\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # FINALIZACIN\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" 隆ENTRENAMIENTO EVOLUTIVO COMPLETADO! \")\n",
    "    print(\"=\" * 80)\n",
    "    print(\" El mejor ADN evolutivo est谩 guardado en 'best_agent_genes.pth'\")\n",
    "    print(\" Puedes usar este agente en los demostradores para ver su comportamiento\")\n",
    "    print(\"К El agente ha evolucionado a trav茅s de\", NUM_GENERATIONS, \"generaciones\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3e3bc",
   "metadata": {},
   "source": [
    "### Imitaci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596289e",
   "metadata": {},
   "source": [
    "##### a_star_solver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_star_solver.py\n",
    "\"\"\"\n",
    "Implementaci贸n del algoritmo A* para navegaci贸n 贸ptima en grid.\n",
    "\n",
    "Este m贸dulo proporciona funcionalidades para encontrar el camino m谩s corto\n",
    "entre dos puntos en una cuadr铆cula, evitando obst谩culos (venenos). Utiliza\n",
    "el algoritmo A* con distancia Manhattan como heur铆stica para garantizar\n",
    "optimalidad en entornos de grid.\n",
    "\n",
    "Funciones:\n",
    "    heuristic(a, b): Calcula distancia Manhattan entre dos puntos\n",
    "    a_star_search(grid_size, agent_pos, goal_pos, poisons): Encuentra camino 贸ptimo\n",
    "\"\"\"\n",
    "import heapq\n",
    "\n",
    "def heuristic(a, b):\n",
    "    \"\"\"\n",
    "    Calcula la distancia heur铆stica entre dos puntos usando distancia Manhattan.\n",
    "    \n",
    "    La distancia Manhattan es la suma de las diferencias absolutas de sus\n",
    "    coordenadas cartesianas. Es una heur铆stica admisible para movimiento\n",
    "    en grid con 4 direcciones, garantizando optimalidad en A*.\n",
    "    \n",
    "    Args:\n",
    "        a (tuple): Coordenadas (x, y) del primer punto\n",
    "        b (tuple): Coordenadas (x, y) del segundo punto\n",
    "    \n",
    "    Returns:\n",
    "        int: Distancia Manhattan entre los puntos a y b\n",
    "    \n",
    "    Example:\n",
    "        >>> heuristic((0, 0), (3, 4))\n",
    "        7\n",
    "        >>> heuristic((1, 1), (1, 1))\n",
    "        0\n",
    "    \"\"\"\n",
    "    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "def a_star_search(grid_size, agent_pos, goal_pos, poisons):\n",
    "    \"\"\"\n",
    "    Implementa el algoritmo A* para encontrar el camino 贸ptimo en una cuadr铆cula.\n",
    "    \n",
    "    Busca el camino m谩s corto desde la posici贸n del agente hasta el objetivo,\n",
    "    evitando venenos y respetando los l铆mites del grid. Utiliza una heur铆stica\n",
    "    admisible (distancia Manhattan) para garantizar optimalidad.\n",
    "    \n",
    "    Algoritmo A*:\n",
    "        1. Mantiene conjunto abierto (por explorar) y cerrado (explorados)\n",
    "        2. Eval煤a nodos usando f(n) = g(n) + h(n):\n",
    "           - g(n): Costo real desde inicio hasta nodo n\n",
    "           - h(n): Heur铆stica desde nodo n hasta objetivo\n",
    "        3. Expande el nodo con menor f(n) hasta encontrar objetivo\n",
    "        4. Reconstruye camino desde objetivo hasta inicio\n",
    "    \n",
    "    Args:\n",
    "        grid_size (int): Tama帽o de la cuadr铆cula (grid_size x grid_size)\n",
    "        agent_pos (tuple): Posici贸n inicial del agente (x, y)\n",
    "        goal_pos (tuple): Posici贸n objetivo a alcanzar (x, y)\n",
    "        poisons (list): Lista de posiciones de venenos [(x, y), ...]\n",
    "    \n",
    "    Returns:\n",
    "        list: Secuencia de posiciones [(x, y), ...] del camino 贸ptimo,\n",
    "              excluyendo posici贸n inicial. None si no existe camino.\n",
    "    \n",
    "    Example:\n",
    "        >>> a_star_search(5, (0, 0), (4, 4), [(2, 2)])\n",
    "        [(1, 0), (2, 0), (3, 0), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]\n",
    "    \n",
    "    Note:\n",
    "        - Movimiento limitado a 4 direcciones (arriba, abajo, izquierda, derecha)\n",
    "        - Venenos son obst谩culos impasables\n",
    "        - Retorna None si el objetivo es inalcanzable\n",
    "    \"\"\"\n",
    "    # Convertir posiciones a tuplas para consistencia\n",
    "    start = tuple(agent_pos)\n",
    "    goal = tuple(goal_pos)\n",
    "    \n",
    "    # Inicializar estructuras de datos del algoritmo A*\n",
    "    close_set = set()           # Nodos ya explorados\n",
    "    came_from = {}              # Mapeo para reconstruir camino\n",
    "    g_score = {start: 0}        # Costo real desde inicio\n",
    "    f_score = {start: heuristic(start, goal)}  # Costo estimado total\n",
    "    \n",
    "    # Heap para mantener nodos ordenados por f_score\n",
    "    open_heap = [(f_score[start], start)]\n",
    "\n",
    "    # Bucle principal del algoritmo A*\n",
    "    while open_heap:\n",
    "        # Extraer nodo con menor f_score\n",
    "        current = heapq.heappop(open_heap)[1]\n",
    "\n",
    "        # 驴Hemos llegado al objetivo?\n",
    "        if current == goal:\n",
    "            # Reconstruir camino desde objetivo hasta inicio\n",
    "            path = [current]\n",
    "            while current in came_from:\n",
    "                current = came_from[current]\n",
    "                path.insert(0, current)\n",
    "            # Excluir posici贸n inicial del camino retornado\n",
    "            path.pop(0)\n",
    "            return path\n",
    "\n",
    "        # Marcar nodo actual como explorado\n",
    "        close_set.add(current)\n",
    "        \n",
    "        # Explorar todos los vecinos (4 direcciones)\n",
    "        for i, j in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n",
    "            neighbor = (current[0] + i, current[1] + j)\n",
    "            \n",
    "            # Verificar l铆mites del grid\n",
    "            if not (0 <= neighbor[0] < grid_size and 0 <= neighbor[1] < grid_size):\n",
    "                continue\n",
    "            \n",
    "            # Verificar obst谩culos: nodos explorados y venenos\n",
    "            if neighbor in close_set or any(tuple(p) == neighbor for p in poisons):\n",
    "                continue\n",
    "\n",
    "            # Calcular nuevo costo para llegar al vecino\n",
    "            tentative_g_score = g_score[current] + 1\n",
    "            \n",
    "            # 驴Es este un mejor camino al vecino?\n",
    "            if tentative_g_score < g_score.get(neighbor, float('inf')):\n",
    "                # Registrar mejor camino encontrado\n",
    "                came_from[neighbor] = current\n",
    "                g_score[neighbor] = tentative_g_score\n",
    "                f_score[neighbor] = tentative_g_score + heuristic(neighbor, goal)\n",
    "                # Agregar vecino a conjunto de exploraci贸n\n",
    "                heapq.heappush(open_heap, (f_score[neighbor], neighbor))\n",
    "    \n",
    "    # No se encontr贸 camino al objetivo\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80e546",
   "metadata": {},
   "source": [
    "#### generate_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_data.py\n",
    "\"\"\"\n",
    "Generador de datos de demostraci贸n experta para aprendizaje por imitaci贸n.\n",
    "\n",
    "Este m贸dulo crea datasets de pares estado-acci贸n obtenidos de un agente experto\n",
    "que utiliza el algoritmo A* para navegaci贸n 贸ptima. Los datos generados se\n",
    "utilizan para entrenar agentes mediante aprendizaje supervisado, imitando\n",
    "comportamiento experto en diferentes configuraciones de complejidad.\n",
    "\n",
    "Funciones:\n",
    "    get_action(from_pos, to_pos): Convierte movimiento posicional a 铆ndice de acci贸n\n",
    "    generate_expert_data_for_n_fruits(num_fruits, num_samples, output_file): \n",
    "        Genera dataset para configuraci贸n espec铆fica de frutas\n",
    "        \n",
    "Constantes:\n",
    "    GRID_SIZE: Tama帽o del entorno de cuadr铆cula (5x5)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from environment import GridEnvironment\n",
    "from a_star_solver import a_star_search\n",
    "\n",
    "# Configuraci贸n del entorno\n",
    "GRID_SIZE = 5\n",
    "\n",
    "def get_action(from_pos, to_pos):\n",
    "    \"\"\"\n",
    "    Convierte un movimiento entre posiciones adyacentes en 铆ndice de acci贸n.\n",
    "    \n",
    "    Calcula la diferencia vectorial entre posiciones y la mapea al 铆ndice\n",
    "    de acci贸n correspondiente. Utilizada para convertir el camino 贸ptimo\n",
    "    de A* en secuencia de acciones ejecutables por el agente.\n",
    "    \n",
    "    Args:\n",
    "        from_pos (tuple/np.ndarray): Posici贸n inicial (x, y)\n",
    "        to_pos (tuple/np.ndarray): Posici贸n objetivo (x, y)\n",
    "    \n",
    "    Returns:\n",
    "        int: ndice de acci贸n correspondiente al movimiento:\n",
    "             0 = Arriba (decrementar x)\n",
    "             1 = Abajo (incrementar x)  \n",
    "             2 = Izquierda (decrementar y)\n",
    "             3 = Derecha (incrementar y)\n",
    "             -1 = Movimiento inv谩lido (no adyacente)\n",
    "    \n",
    "    Example:\n",
    "        >>> get_action((1, 1), (0, 1))  # Movimiento hacia arriba\n",
    "        0\n",
    "        >>> get_action((1, 1), (1, 2))  # Movimiento hacia derecha\n",
    "        3\n",
    "    \n",
    "    Note:\n",
    "        Solo funciona para posiciones adyacentes. Movimientos diagonales\n",
    "        o de m煤ltiples celdas retornan -1.\n",
    "    \"\"\"\n",
    "    # Calcular vector de diferencia entre posiciones\n",
    "    delta = np.array(to_pos) - np.array(from_pos)\n",
    "    \n",
    "    # Mapear diferencia a 铆ndice de acci贸n\n",
    "    if delta[0] == -1: return 0    # Arriba\n",
    "    if delta[0] == 1: return 1     # Abajo\n",
    "    if delta[1] == -1: return 2    # Izquierda\n",
    "    if delta[1] == 1: return 3     # Derecha\n",
    "    return -1  # Movimiento inv谩lido\n",
    "\n",
    "def generate_expert_data_for_n_fruits(num_fruits, num_samples, output_file):\n",
    "    \"\"\"\n",
    "    Genera dataset de demostraciones expertas para configuraci贸n espec铆fica.\n",
    "    \n",
    "    Crea escenarios aleatorios con n煤mero fijo de frutas y utiliza A* para\n",
    "    generar comportamiento experto 贸ptimo. Implementa estrategia greedy de\n",
    "    ir siempre a la fruta m谩s cercana, creando datos de entrenamiento para\n",
    "    aprendizaje por imitaci贸n con curriculum learning.\n",
    "    \n",
    "    Proceso de generaci贸n:\n",
    "        1. Crear escenario aleatorio (agente, frutas, venenos)\n",
    "        2. Calcular fruta m谩s cercana al agente\n",
    "        3. Usar A* para encontrar camino 贸ptimo\n",
    "        4. Ejecutar primer paso y registrar par (estado, acci贸n)\n",
    "        5. Repetir hasta completar episodio o fallar\n",
    "        6. Continuar hasta obtener muestras suficientes\n",
    "    \n",
    "    Args:\n",
    "        num_fruits (int): N煤mero de frutas en cada escenario\n",
    "        num_samples (int): Cantidad objetivo de muestras estado-acci贸n\n",
    "        output_file (str): Archivo pickle donde guardar el dataset\n",
    "    \n",
    "    Raises:\n",
    "        IOError: Si no se puede escribir el archivo de salida\n",
    "    \n",
    "    Example:\n",
    "        >>> generate_expert_data_for_n_fruits(2, 1000, \"data_2_fruits.pkl\")\n",
    "        Generando 1000 muestras para 2 fruta(s)...\n",
    "        Dataset 'data_2_fruits.pkl' creado con 1000 muestras.\n",
    "    \n",
    "    Note:\n",
    "        - Venenos colocados aleatoriamente (2-4 por escenario)\n",
    "        - M谩ximo 50 pasos por episodio para evitar bucles infinitos\n",
    "        - Estrategia greedy: siempre ir a fruta m谩s cercana (euclidiana)\n",
    "        - Solo se registran acciones v谩lidas (con camino A* factible)\n",
    "    \"\"\"\n",
    "    # Inicializar entorno y contenedor de datos\n",
    "    env = GridEnvironment()\n",
    "    expert_data = []\n",
    "    print(f\"Generando {num_samples} muestras para {num_fruits} fruta(s)...\")\n",
    "    \n",
    "    generated_episodes = 0\n",
    "    while len(expert_data) < num_samples:\n",
    "        generated_episodes += 1\n",
    "        \n",
    "        # Configurar escenario aleatorio\n",
    "        num_poisons = np.random.randint(2, 5)  # 2-4 venenos por escenario\n",
    "        \n",
    "        # Generar posiciones 煤nicas aleatorias\n",
    "        all_pos = [(r, c) for r in range(GRID_SIZE) for c in range(GRID_SIZE)]\n",
    "        random.shuffle(all_pos)\n",
    "        \n",
    "        # Asignar posiciones a elementos del entorno\n",
    "        agent_p = all_pos.pop()\n",
    "        fruit_p = [all_pos.pop() for _ in range(num_fruits)]\n",
    "        poison_p = [all_pos.pop() for _ in range(num_poisons)]\n",
    "        \n",
    "        # Inicializar entorno con configuraci贸n generada\n",
    "        env.reset(agent_pos=agent_p, fruit_pos=fruit_p, poison_pos=poison_p)\n",
    "\n",
    "        # Simular episodio con comportamiento experto\n",
    "        for _ in range(50):  # M谩ximo 50 pasos por episodio\n",
    "            # Verificar condici贸n de terminaci贸n por victoria\n",
    "            if not env.fruit_pos: \n",
    "                break\n",
    "            \n",
    "            # Implementar estrategia greedy: ir a fruta m谩s cercana\n",
    "            agent_pos = env.agent_pos\n",
    "            # Calcular distancias euclidianas a todas las frutas\n",
    "            distances = [np.linalg.norm(agent_pos - f) for f in env.fruit_pos]\n",
    "            # Seleccionar fruta m谩s cercana como objetivo\n",
    "            goal_fruit = env.fruit_pos[np.argmin(distances)]\n",
    "            \n",
    "            # Usar A* para encontrar camino 贸ptimo al objetivo\n",
    "            path = a_star_search(GRID_SIZE, agent_pos, goal_fruit, env.poison_pos)\n",
    "\n",
    "            # Verificar si existe camino factible\n",
    "            if path and len(path) > 0:\n",
    "                # Convertir primer paso del camino a acci贸n\n",
    "                action = get_action(agent_pos, path[0])\n",
    "                # Registrar par estado-acci贸n para entrenamiento\n",
    "                state = env.get_state()\n",
    "                expert_data.append((state, action))\n",
    "                # Ejecutar acci贸n en el entorno\n",
    "                env.step(action)\n",
    "            else:\n",
    "                # No hay camino factible: terminar episodio\n",
    "                break\n",
    "        \n",
    "        # Reporte de progreso cada 200 episodios\n",
    "        if generated_episodes % 200 == 0:\n",
    "            print(f\"  Partidas procesadas: {generated_episodes}, Muestras actuales: {len(expert_data)}\")\n",
    "\n",
    "    # Guardar dataset en archivo pickle\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(expert_data[:num_samples], f)\n",
    "    print(f\"Dataset '{output_file}' creado con {len(expert_data[:num_samples])} muestras.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Script principal para generaci贸n de curriculum de datasets.\n",
    "    \n",
    "    Genera m煤ltiples datasets con diferentes niveles de complejidad para\n",
    "    implementar curriculum learning en el entrenamiento por imitaci贸n.\n",
    "    Los datasets se ordenan por dificultad creciente (n煤mero de frutas).\n",
    "    \n",
    "    Curriculum generado:\n",
    "        - 1 fruta: 4000 muestras (nivel b谩sico)\n",
    "        - 2 frutas: 4000 muestras (nivel intermedio bajo)\n",
    "        - 3 frutas: 4000 muestras (nivel intermedio alto)\n",
    "        - 4 frutas: 5000 muestras (nivel avanzado, m谩s muestras)\n",
    "    \n",
    "    Beneficios del curriculum learning:\n",
    "        - Aprendizaje gradual de complejidad creciente\n",
    "        - Mejor convergencia del entrenamiento\n",
    "        - Pol铆ticas m谩s robustas y generalizables\n",
    "        - Reducci贸n de overfitting a configuraciones espec铆ficas\n",
    "    \"\"\"\n",
    "    # Generar datasets con complejidad creciente\n",
    "    generate_expert_data_for_n_fruits(1, 4000, \"expert_data_1_fruit.pkl\")\n",
    "    generate_expert_data_for_n_fruits(2, 4000, \"expert_data_2_fruits.pkl\")\n",
    "    generate_expert_data_for_n_fruits(3, 4000, \"expert_data_3_fruits.pkl\")\n",
    "    generate_expert_data_for_n_fruits(4, 5000, \"expert_data_4_fruits.pkl\")\n",
    "    print(\"\\nTodos los datasets del curr铆culo han sido generados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e625956",
   "metadata": {},
   "source": [
    "#### agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277310f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\"\"\"\n",
    "Implementaci贸n de agente con red neuronal convolucional para aprendizaje por imitaci贸n.\n",
    "\n",
    "Este m贸dulo define la arquitectura de red neuronal y la clase agente utilizadas\n",
    "en el aprendizaje por imitaci贸n. La red procesa representaciones visuales del\n",
    "entorno (grids 3D) y predice acciones 贸ptimas imitando comportamiento experto.\n",
    "\n",
    "Clases:\n",
    "    AgentNetwork: Red neuronal convolucional para procesamiento de estados visuales\n",
    "    Agent: Interfaz del agente que utiliza la red para toma de decisiones\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AgentNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal convolucional para procesamiento de estados de grid y predicci贸n de acciones.\n",
    "    \n",
    "    Arquitectura dise帽ada espec铆ficamente para entornos de cuadr铆cula donde el estado\n",
    "    se representa como im谩genes de 3 canales (agente, frutas, venenos). Utiliza\n",
    "    capas convolucionales para extracci贸n de caracter铆sticas espaciales seguidas\n",
    "    de capas densas para predicci贸n de acciones.\n",
    "    \n",
    "    Arquitectura:\n",
    "        - Conv2D (3->16): Extracci贸n de caracter铆sticas b谩sicas\n",
    "        - Conv2D (16->32): Caracter铆sticas de nivel medio\n",
    "        - Flatten: Preparaci贸n para capas densas\n",
    "        - FC (flattened->256): Representaci贸n de alto nivel\n",
    "        - FC (256->4): Predicci贸n de acciones (4 direcciones)\n",
    "    \n",
    "    Args:\n",
    "        h (int): Altura del grid de entrada (default: 5)\n",
    "        w (int): Ancho del grid de entrada (default: 5)\n",
    "        outputs (int): N煤mero de acciones posibles (default: 4)\n",
    "    \"\"\"\n",
    "    def __init__(self, h=5, w=5, outputs=4):\n",
    "        super(AgentNetwork, self).__init__()\n",
    "        \n",
    "        # Capas convolucionales para procesamiento espacial\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Funci贸n auxiliar para calcular tama帽o de salida convolucional\n",
    "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
    "            \"\"\"Calcula dimensi贸n de salida despu茅s de operaci贸n convolucional.\"\"\"\n",
    "            return (size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        # Calcular dimensiones despu茅s de capas convolucionales\n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        # Capas densas para predicci贸n final\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagaci贸n hacia adelante de la red neuronal.\n",
    "        \n",
    "        Procesa el estado visual del entorno a trav茅s de las capas convolucionales\n",
    "        y densas para generar valores de acci贸n. Utiliza ReLU como funci贸n de\n",
    "        activaci贸n para introducir no-linealidad.\n",
    "        \n",
    "        Flujo de procesamiento:\n",
    "            1. Conv1 + ReLU: Extracci贸n de caracter铆sticas b谩sicas\n",
    "            2. Conv2 + ReLU: Caracter铆sticas de nivel medio\n",
    "            3. Flatten: Conversi贸n a vector 1D\n",
    "            4. FC1 + ReLU: Representaci贸n de alto nivel\n",
    "            5. FC2: Valores de acci贸n finales (sin activaci贸n)\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Estado del entorno con forma (batch_size, 3, h, w)\n",
    "                             Canales: [agente, frutas, venenos]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Valores de acci贸n con forma (batch_size, 4)\n",
    "                         ndices corresponden a [arriba, abajo, izquierda, derecha]\n",
    "        \"\"\"\n",
    "        # Primera capa convolucional con activaci贸n ReLU\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # Segunda capa convolucional con activaci贸n ReLU\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # Aplanar para conexi贸n con capas densas\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Primera capa densa con activaci贸n ReLU\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        # Capa de salida sin activaci贸n (valores de acci贸n)\n",
    "        return self.fc2(x)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agente que utiliza red neuronal para toma de decisiones por imitaci贸n.\n",
    "    \n",
    "    Implementa la interfaz de agente que encapsula la red neuronal y proporciona\n",
    "    m茅todos para selecci贸n de acciones y carga de modelos pre-entrenados.\n",
    "    Dise帽ado para imitar comportamiento experto aprendido de datos de demostraci贸n.\n",
    "    \n",
    "    Attributes:\n",
    "        network (AgentNetwork): Red neuronal convolucional para predicci贸n de acciones\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el agente con red neuronal por defecto.\n",
    "        \n",
    "        Crea una instancia de AgentNetwork con par谩metros est谩ndar\n",
    "        para entornos de grid 5x5 con 4 acciones posibles.\n",
    "        \"\"\"\n",
    "        self.network = AgentNetwork()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Selecciona la acci贸n 贸ptima basada en el estado actual del entorno.\n",
    "        \n",
    "        Utiliza la red neuronal para evaluar el estado y selecciona la acci贸n\n",
    "        con mayor valor predicho. Implementa una pol铆tica determin铆stica\n",
    "        (greedy) que siempre elige la mejor acci贸n seg煤n el modelo.\n",
    "        \n",
    "        Proceso:\n",
    "            1. Convierte estado a tensor PyTorch\n",
    "            2. Agrega dimensi贸n de batch (unsqueeze)\n",
    "            3. Realiza inferencia sin gradientes\n",
    "            4. Selecciona acci贸n con mayor valor (argmax)\n",
    "        \n",
    "        Args:\n",
    "            state (numpy.ndarray): Estado del entorno con forma (3, h, w)\n",
    "                                  Canales: [agente, frutas, venenos]\n",
    "        \n",
    "        Returns:\n",
    "            int: ndice de la acci贸n seleccionada\n",
    "                 0=Arriba, 1=Abajo, 2=Izquierda, 3=Derecha\n",
    "        \n",
    "        Note:\n",
    "            Utiliza torch.no_grad() para optimizar inferencia y evitar\n",
    "            construcci贸n del grafo computacional durante evaluaci贸n.\n",
    "        \"\"\"\n",
    "        # Convertir estado a tensor y agregar dimensi贸n de batch\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Realizar inferencia sin c谩lculo de gradientes\n",
    "        with torch.no_grad():\n",
    "            action_values = self.network(state_tensor)\n",
    "        \n",
    "        # Seleccionar acci贸n con mayor valor predicho\n",
    "        return torch.argmax(action_values).item()\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Carga pesos pre-entrenados en la red neuronal del agente.\n",
    "        \n",
    "        Permite cargar modelos entrenados mediante aprendizaje por imitaci贸n\n",
    "        para utilizar pol铆ticas aprendidas de datos de demostraci贸n experta.\n",
    "        Los pesos se cargan directamente en la red neuronal existente.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Ruta al archivo de modelo PyTorch (.pth)\n",
    "                           que contiene los state_dict de la red\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: Si el archivo de modelo no existe\n",
    "            RuntimeError: Si hay incompatibilidad en arquitectura de red\n",
    "        \n",
    "        Example:\n",
    "            >>> agent = Agent()\n",
    "            >>> agent.load_model('imitacion_model.pth')\n",
    "            >>> action = agent.choose_action(current_state)\n",
    "        \n",
    "        Note:\n",
    "            El modelo cargado debe tener la misma arquitectura que\n",
    "            AgentNetwork para evitar errores de compatibilidad.\n",
    "        \"\"\"\n",
    "        self.network.load_state_dict(torch.load(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e0b91",
   "metadata": {},
   "source": [
    "#### imitacion_agente.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demostraci贸n interactiva de agente entrenado por aprendizaje por imitaci贸n.\n",
    "\n",
    "Este m贸dulo proporciona una interfaz gr谩fica para demostrar el comportamiento\n",
    "de un agente que ha aprendido por imitaci贸n de datos expertos. Permite al\n",
    "usuario dise帽ar niveles y observar c贸mo el agente navega utilizando la\n",
    "pol铆tica aprendida mediante redes neuronales convolucionales.\n",
    "\n",
    "Caracter铆sticas:\n",
    "    - Modo configuraci贸n: Dise帽o interactivo de niveles\n",
    "    - Modo juego: Demostraci贸n autom谩tica del agente entrenado\n",
    "    - Interfaz visual: Pygame con sprites y feedback en tiempo real\n",
    "    - Carga de modelos: Integraci贸n con modelos PyTorch pre-entrenados\n",
    "\n",
    "Constantes:\n",
    "    GRID_WIDTH, GRID_HEIGHT: Dimensiones del entorno (5x5)\n",
    "    CELL_SIZE: Tama帽o de cada celda en p铆xeles (120px)\n",
    "    SCREEN_WIDTH, SCREEN_HEIGHT: Dimensiones de la ventana\n",
    "    COLOR_*: Esquema de colores para la interfaz\n",
    "\n",
    "Clases:\n",
    "    EntornoGrid: Entorno de demostraci贸n con funcionalidades completas\n",
    "\"\"\"\n",
    "import pygame\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from agent import Agent\n",
    "\n",
    "# Configuraci贸n del entorno y pantalla\n",
    "GRID_WIDTH = 5\n",
    "GRID_HEIGHT = 5\n",
    "CELL_SIZE = 120\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE\n",
    "\n",
    "# Esquema de colores para interfaz oscura\n",
    "COLOR_FONDO = (25, 25, 25)      # Fondo principal oscuro\n",
    "COLOR_LINEAS = (40, 40, 40)     # L铆neas de grid sutiles\n",
    "COLOR_CURSOR = (255, 255, 0)    # Cursor amarillo brillante\n",
    "COLOR_TEXTO = (230, 230, 230)   # Texto claro para legibilidad\n",
    "\n",
    "\n",
    "class EntornoGrid:\n",
    "    \"\"\"\n",
    "    Entorno de demostraci贸n para agente entrenado por imitaci贸n.\n",
    "    \n",
    "    Implementa un entorno de grid completo con capacidades de configuraci贸n\n",
    "    interactiva y simulaci贸n de episodios. Dise帽ado para demostrar el\n",
    "    comportamiento aprendido del agente en diferentes escenarios.\n",
    "    \n",
    "    Funcionalidades:\n",
    "        - Configuraci贸n manual de elementos (frutas, venenos, paredes)\n",
    "        - Simulaci贸n de episodios con agente autom谩tico\n",
    "        - Sistema de recompensas completo para feedback\n",
    "        - Renderizado visual con sprites\n",
    "        - Detecci贸n de colisiones y condiciones de terminaci贸n\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tama帽o del grid (5x5)\n",
    "        agent_pos (tuple): Posici贸n actual del agente (fila, columna)\n",
    "        frutas (set): Conjunto de posiciones de frutas\n",
    "        venenos (set): Conjunto de posiciones de venenos\n",
    "        paredes (set): Conjunto de posiciones de paredes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuraci贸n por defecto.\n",
    "        \n",
    "        Establece grid vac铆o con agente en posici贸n (0,0) y\n",
    "        conjuntos vac铆os para elementos del entorno.\n",
    "        \"\"\"\n",
    "        self.size = GRID_WIDTH\n",
    "        self.agent_pos = (0, 0)\n",
    "        self.frutas = set()\n",
    "        self.venenos = set()\n",
    "        self.paredes = set()\n",
    "\n",
    "    def reset_a_configuracion_inicial(self):\n",
    "        \"\"\"\n",
    "        Reinicia el agente a la posici贸n inicial del episodio.\n",
    "        \n",
    "        Coloca al agente en (0,0) manteniendo la configuraci贸n actual\n",
    "        del entorno. Utilizado al iniciar nuevos episodios de demostraci贸n.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Estado inicial del entorno con forma (3, size, size)\n",
    "        \"\"\"\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def limpiar_entorno(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno.\n",
    "        \n",
    "        Limpia completamente frutas, venenos y paredes, dejando\n",
    "        un grid vac铆o para nueva configuraci贸n. El agente mantiene\n",
    "        su posici贸n actual.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acci贸n del agente y actualiza el estado del entorno.\n",
    "        \n",
    "        Procesa el movimiento del agente, verifica colisiones y calcula\n",
    "        recompensas seg煤n las interacciones con elementos del entorno.\n",
    "        Implementa la l贸gica completa de simulaci贸n para demostraci贸n.\n",
    "        \n",
    "        Sistema de recompensas:\n",
    "            - Movimiento normal: -0.05 (costo por paso)\n",
    "            - Movimiento inv谩lido: -0.1 (penalizaci贸n)\n",
    "            - Fruta recolectada: +1.0 (objetivo positivo)\n",
    "            - Todas las frutas: +10.0 adicional (victoria)\n",
    "            - Veneno tocado: -10.0 (penalizaci贸n grave)\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Acci贸n a ejecutar\n",
    "                         0 = Arriba (decrementar fila)\n",
    "                         1 = Abajo (incrementar fila)\n",
    "                         2 = Izquierda (decrementar columna)\n",
    "                         3 = Derecha (incrementar columna)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, recompensa, terminado)\n",
    "                - nuevo_estado (np.ndarray): Estado resultante (3, size, size)\n",
    "                - recompensa (float): Recompensa por la acci贸n ejecutada\n",
    "                - terminado (bool): True si episodio termin贸, False en caso contrario\n",
    "        \"\"\"\n",
    "        # Calcular nueva posici贸n basada en la acci贸n\n",
    "        fila, col = self.agent_pos\n",
    "        if accion == 0:\n",
    "            fila -= 1    # Arriba\n",
    "        elif accion == 1:\n",
    "            fila += 1    # Abajo\n",
    "        elif accion == 2:\n",
    "            col -= 1     # Izquierda\n",
    "        elif accion == 3:\n",
    "            col += 1     # Derecha\n",
    "\n",
    "        # Verificar l铆mites del entorno y colisiones con paredes\n",
    "        if (\n",
    "            fila < 0\n",
    "            or fila >= GRID_HEIGHT\n",
    "            or col < 0\n",
    "            or col >= GRID_WIDTH\n",
    "            or (fila, col) in self.paredes\n",
    "        ):\n",
    "            # Movimiento inv谩lido: mantener posici贸n y penalizar\n",
    "            return self.get_state(), -0.1, False\n",
    "\n",
    "        # Movimiento v谩lido: actualizar posici贸n\n",
    "        self.agent_pos = (fila, col)\n",
    "        recompensa = -0.05  # Costo base por movimiento\n",
    "        terminado = False\n",
    "\n",
    "        # Procesar interacciones con elementos del entorno\n",
    "        if self.agent_pos in self.venenos:\n",
    "            # Veneno tocado: penalizaci贸n grave y reset a inicio\n",
    "            recompensa = -10.0\n",
    "            self.agent_pos = (0, 0)\n",
    "        elif self.agent_pos in self.frutas:\n",
    "            # Fruta recolectada: recompensa positiva\n",
    "            recompensa = 1.0\n",
    "            self.frutas.remove(self.agent_pos)\n",
    "            # Verificar victoria (todas las frutas recolectadas)\n",
    "            if not self.frutas:\n",
    "                recompensa += 10.0  # Bonus por completar nivel\n",
    "                terminado = True\n",
    "                self.agent_pos = (0, 0)  # Reset a posici贸n inicial\n",
    "\n",
    "        return self.get_state(), recompensa, terminado\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera representaci贸n visual del estado actual del entorno.\n",
    "        \n",
    "        Crea tensor 3D donde cada canal representa un tipo de elemento,\n",
    "        compatible con la arquitectura CNN del agente entrenado.\n",
    "        \n",
    "        Estructura de canales:\n",
    "            - Canal 0: Posici贸n del agente (binario)\n",
    "            - Canal 1: Posiciones de frutas (binario)\n",
    "            - Canal 2: Posiciones de venenos (binario)\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Estado con forma (3, size, size) y dtype float32\n",
    "                       Valores 1.0 indican presencia, 0.0 ausencia\n",
    "        \n",
    "        Note:\n",
    "            Las paredes no se incluyen en el estado ya que el agente\n",
    "            entrenado no las consideraba en los datos de demostraci贸n.\n",
    "        \"\"\"\n",
    "        # Inicializar tensor de estado\n",
    "        estado = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posici贸n del agente\n",
    "        estado[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruta in self.frutas:\n",
    "            estado[1, fruta[0], fruta[1]] = 1.0\n",
    "        \n",
    "        # Canal 2: Posiciones de venenos\n",
    "        for veneno in self.venenos:\n",
    "            estado[2, veneno[0], veneno[1]] = 1.0\n",
    "        \n",
    "        return estado\n",
    "\n",
    "    def dibujar(\n",
    "        self,\n",
    "        pantalla,\n",
    "        modo_juego,\n",
    "        cursor_pos,\n",
    "        img_fruta,\n",
    "        img_veneno,\n",
    "        img_pared,\n",
    "        img_agente,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Renderiza el estado completo del entorno con interfaz de usuario.\n",
    "        \n",
    "        Dibuja todos los elementos visuales del entorno, grid de navegaci贸n,\n",
    "        cursor de configuraci贸n e informaci贸n de controles. Proporciona\n",
    "        feedback visual completo para ambos modos de operaci贸n.\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde renderizar\n",
    "            modo_juego (str): Modo actual (\"SETUP\" o \"PLAYING\")\n",
    "            cursor_pos (tuple): Posici贸n del cursor en modo configuraci贸n\n",
    "            img_fruta (pygame.Surface): Sprite de las frutas\n",
    "            img_veneno (pygame.Surface): Sprite de los venenos\n",
    "            img_pared (pygame.Surface): Sprite de las paredes\n",
    "            img_agente (pygame.Surface): Sprite del agente\n",
    "        \n",
    "        Note:\n",
    "            Renderiza en orden espec铆fico para evitar superposiciones:\n",
    "            fondo  grid  paredes  frutas  venenos  agente  cursor  UI\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con fondo oscuro\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "        \n",
    "        # Dibujar l铆neas del grid para navegaci贸n visual\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Renderizar elementos del entorno (orden: paredes  frutas  venenos)\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0] * CELL_SIZE, pared[1] * CELL_SIZE))\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0] * CELL_SIZE, fruta[1] * CELL_SIZE))\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0] * CELL_SIZE, veneno[1] * CELL_SIZE))\n",
    "\n",
    "        # Dibujar agente (siempre en primer plano)\n",
    "        pantalla.blit(\n",
    "            img_agente, (self.agent_pos[0] * CELL_SIZE, self.agent_pos[1] * CELL_SIZE)\n",
    "        )\n",
    "\n",
    "        # Mostrar cursor en modo configuraci贸n\n",
    "        if modo_juego == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(\n",
    "                cursor_pos[0] * CELL_SIZE,\n",
    "                cursor_pos[1] * CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "                CELL_SIZE,\n",
    "            )\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar informaci贸n de interfaz\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        texto_modo = font.render(f\"Modo: {modo_juego}\", True, COLOR_TEXTO)\n",
    "        controles1 = font.render(\n",
    "            \"SETUP: Flechas, F=Fruta, V=Veneno, W=Pared, C=Limpiar\", True, COLOR_TEXTO\n",
    "        )\n",
    "        controles2 = font.render(\"P=Jugar, S=Setup\", True, COLOR_TEXTO)\n",
    "        pantalla.blit(texto_modo, (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(controles1, (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(controles2, (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal de la demostraci贸n interactiva del agente por imitaci贸n.\n",
    "    \n",
    "    Inicializa la interfaz gr谩fica y gestiona el bucle principal que permite\n",
    "    alternar entre modo configuraci贸n (dise帽o de niveles) y modo demostraci贸n\n",
    "    (agente autom谩tico). Proporciona una experiencia completa para evaluar\n",
    "    el rendimiento del agente entrenado.\n",
    "    \n",
    "    Flujo de la aplicaci贸n:\n",
    "        1. Inicializaci贸n de Pygame y recursos\n",
    "        2. Carga del modelo entrenado\n",
    "        3. Bucle principal con dos modos:\n",
    "           - SETUP: Configuraci贸n manual de niveles\n",
    "           - PLAYING: Demostraci贸n autom谩tica del agente\n",
    "        4. Manejo de eventos y renderizado en tiempo real\n",
    "    \n",
    "    Controles disponibles:\n",
    "        Modo SETUP:\n",
    "            - Flechas: Mover cursor de configuraci贸n\n",
    "            - F: Colocar/quitar fruta\n",
    "            - V: Colocar/quitar veneno\n",
    "            - W: Colocar/quitar pared\n",
    "            - C: Limpiar entorno completamente\n",
    "            - P: Iniciar demostraci贸n autom谩tica\n",
    "        \n",
    "        Modo PLAYING:\n",
    "            - S: Volver a modo configuraci贸n\n",
    "            - Agente se mueve autom谩ticamente cada 0.1 segundos\n",
    "    \n",
    "    Note:\n",
    "        Requiere modelo entrenado en \"IMITACION/imitacion_model.pth\"\n",
    "        y sprites en directorio padre (../fruta.png, etc.)\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y configurar ventana\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "    pygame.display.set_caption(\"Agente por Imitaci贸n - Come Frutas \")\n",
    "\n",
    "    def cargar_img(nombre, color_fallback):\n",
    "        \"\"\"\n",
    "        Funci贸n auxiliar para carga robusta de sprites.\n",
    "        \n",
    "        Intenta cargar imagen desde archivo, si falla crea superficie\n",
    "        de color s贸lido como respaldo para mantener funcionalidad.\n",
    "        \n",
    "        Args:\n",
    "            nombre (str): Nombre del archivo de imagen\n",
    "            color_fallback (tuple): Color RGB de respaldo\n",
    "        \n",
    "        Returns:\n",
    "            pygame.Surface: Sprite cargado o superficie de color\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "            img = pygame.image.load(ruta).convert_alpha()\n",
    "            return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "        except:\n",
    "            surf = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "            surf.fill(color_fallback)\n",
    "            return surf\n",
    "\n",
    "    # Cargar sprites con colores de respaldo\n",
    "    img_fruta = cargar_img(\"../fruta.png\", (0, 255, 0))        # Verde\n",
    "    img_veneno = cargar_img(\"../veneno.png\", (255, 0, 0))      # Rojo\n",
    "    img_pared = cargar_img(\"../pared.png\", (100, 100, 100))    # Gris\n",
    "    img_agente = cargar_img(\"../agente.png\", (0, 0, 255))      # Azul\n",
    "\n",
    "    # Inicializar entorno y agente\n",
    "    entorno = EntornoGrid()\n",
    "    agente = Agent()\n",
    "    agente.load_model(\"IMITACION/imitacion_model.pth\")\n",
    "\n",
    "    # Variables de estado de la aplicaci贸n\n",
    "    cursor_pos = [0, 0]\n",
    "    modo_juego = \"SETUP\"\n",
    "    reloj = pygame.time.Clock()\n",
    "    corriendo = True\n",
    "\n",
    "    # Bucle principal de la aplicaci贸n\n",
    "    while corriendo:\n",
    "        # Procesar eventos de entrada\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "\n",
    "            if evento.type == pygame.KEYDOWN:\n",
    "                # Cambio de modos globales\n",
    "                if evento.key == pygame.K_p:\n",
    "                    print(\"--- MODO JUEGO ---\")\n",
    "                    entorno.reset_a_configuracion_inicial()\n",
    "                    modo_juego = \"PLAYING\"\n",
    "                    time.sleep(0.5)  # Pausa para visibilidad del cambio\n",
    "\n",
    "                elif evento.key == pygame.K_s:\n",
    "                    print(\"--- MODO SETUP ---\")\n",
    "                    modo_juego = \"SETUP\"\n",
    "\n",
    "                # Controles espec铆ficos del modo SETUP\n",
    "                if modo_juego == \"SETUP\":\n",
    "                    # Navegaci贸n del cursor con flechas\n",
    "                    if evento.key == pygame.K_UP:\n",
    "                        cursor_pos[1] = max(0, cursor_pos[1] - 1)\n",
    "                    elif evento.key == pygame.K_DOWN:\n",
    "                        cursor_pos[1] = min(GRID_HEIGHT - 1, cursor_pos[1] + 1)\n",
    "                    elif evento.key == pygame.K_LEFT:\n",
    "                        cursor_pos[0] = max(0, cursor_pos[0] - 1)\n",
    "                    elif evento.key == pygame.K_RIGHT:\n",
    "                        cursor_pos[0] = min(GRID_WIDTH - 1, cursor_pos[0] + 1)\n",
    "\n",
    "                    # Colocaci贸n/eliminaci贸n de elementos\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    if evento.key == pygame.K_f:\n",
    "                        # Toggle fruta: agregar/quitar y limpiar otros elementos\n",
    "                        if pos in entorno.frutas:\n",
    "                            entorno.frutas.remove(pos)\n",
    "                        else:\n",
    "                            entorno.frutas.add(pos)\n",
    "                            entorno.venenos.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_v:\n",
    "                        # Toggle veneno: agregar/quitar y limpiar otros elementos\n",
    "                        if pos in entorno.venenos:\n",
    "                            entorno.venenos.remove(pos)\n",
    "                        else:\n",
    "                            entorno.venenos.add(pos)\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_w:\n",
    "                        # Toggle pared: agregar/quitar y limpiar otros elementos\n",
    "                        if pos in entorno.paredes:\n",
    "                            entorno.paredes.remove(pos)\n",
    "                        else:\n",
    "                            entorno.paredes.add(pos)\n",
    "                            entorno.frutas.discard(pos)\n",
    "                            entorno.venenos.discard(pos)\n",
    "                    elif evento.key == pygame.K_c:\n",
    "                        # Limpiar entorno completamente\n",
    "                        print(\"--- LIMPIANDO ENTORNO ---\")\n",
    "                        entorno.limpiar_entorno()\n",
    "\n",
    "        # L贸gica del modo PLAYING (agente autom谩tico)\n",
    "        if modo_juego == \"PLAYING\":\n",
    "            # Obtener estado actual y decidir acci贸n\n",
    "            estado = entorno.get_state()\n",
    "            accion = agente.choose_action(estado)\n",
    "            # Ejecutar acci贸n y verificar terminaci贸n\n",
    "            _, _, terminado = entorno.step(accion)\n",
    "            if terminado:\n",
    "                print(\"Juego terminado. Volviendo a SETUP.\")\n",
    "                modo_juego = \"SETUP\"\n",
    "            time.sleep(0.1)  # Velocidad de demostraci贸n controlada\n",
    "\n",
    "        # Renderizado del estado actual\n",
    "        pantalla_con_info = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT + 80))\n",
    "        pantalla_con_info.fill(COLOR_FONDO)\n",
    "        entorno.dibujar(\n",
    "            pantalla_con_info,\n",
    "            modo_juego,\n",
    "            tuple(cursor_pos),\n",
    "            img_fruta,\n",
    "            img_veneno,\n",
    "            img_pared,\n",
    "            img_agente,\n",
    "        )\n",
    "        pantalla.blit(pantalla_con_info, (0, 0))\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(60)  # 60 FPS para fluidez visual\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568fc5b",
   "metadata": {},
   "source": [
    "#### train_curriculum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fe5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_curriculum.py\n",
    "\"\"\"\n",
    "Entrenamiento por curriculum learning para aprendizaje por imitaci贸n.\n",
    "\n",
    "Este m贸dulo implementa el entrenamiento de la red neuronal convolucional\n",
    "utilizando curriculum learning con datasets de complejidad creciente.\n",
    "El entrenamiento progresa desde escenarios simples (1 fruta) hasta\n",
    "complejos (4 frutas), mejorando la convergencia y generalizaci贸n.\n",
    "\n",
    "Caracter铆sticas:\n",
    "    - Curriculum learning con 4 niveles de dificultad\n",
    "    - Entrenamiento supervisado con pares estado-acci贸n\n",
    "    - Optimizaci贸n Adam con learning rate adaptado\n",
    "    - CrossEntropyLoss para clasificaci贸n de acciones\n",
    "    - Progresi贸n gradual de 茅pocas por complejidad\n",
    "\n",
    "Constantes:\n",
    "    LEARNING_RATE: Tasa de aprendizaje para optimizador Adam (0.0005)\n",
    "    BATCH_SIZE: Tama帽o de lote para entrenamiento (128 muestras)\n",
    "    CURRICULUM: Secuencia de datasets y 茅pocas de entrenamiento\n",
    "\n",
    "Flujo del entrenamiento:\n",
    "    1. Lecci贸n 1: 1 fruta  25 茅pocas (fundamentos b谩sicos)\n",
    "    2. Lecci贸n 2: 2 frutas  30 茅pocas (navegaci贸n intermedia)\n",
    "    3. Lecci贸n 3: 3 frutas  40 茅pocas (planificaci贸n compleja)\n",
    "    4. Lecci贸n 4: 4 frutas  50 茅pocas (maestr铆a y refinamiento)\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "from agent import AgentNetwork\n",
    "\n",
    "# Hiperpar谩metros de entrenamiento\n",
    "LEARNING_RATE = 0.0005  # Tasa de aprendizaje conservadora para estabilidad\n",
    "BATCH_SIZE = 128        # Tama帽o de lote balanceado para memoria y convergencia\n",
    "\n",
    "# Curriculum de entrenamiento: (archivo_dataset, num_茅pocas)\n",
    "CURRICULUM = [\n",
    "    (\"expert_data_1_fruit.pkl\", 25),   # Nivel b谩sico: conceptos fundamentales\n",
    "    (\"expert_data_2_fruits.pkl\", 30),  # Nivel intermedio: decisiones m煤ltiples\n",
    "    (\"expert_data_3_fruits.pkl\", 40),  # Nivel avanzado: planificaci贸n compleja\n",
    "    (\"expert_data_4_fruits.pkl\", 50)   # Nivel experto: refinamiento y maestr铆a\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Script principal de entrenamiento por curriculum learning.\n",
    "    \n",
    "    Implementa el entrenamiento secuencial de la red neuronal utilizando\n",
    "    datasets de complejidad creciente. Cada lecci贸n del curriculum se\n",
    "    enfoca en un nivel espec铆fico de dificultad, permitiendo al modelo\n",
    "    aprender gradualmente conceptos m谩s complejos.\n",
    "    \n",
    "    Proceso de entrenamiento:\n",
    "        1. Inicializaci贸n del modelo, optimizador y funci贸n de p茅rdida\n",
    "        2. Para cada lecci贸n del curriculum:\n",
    "           a. Cargar dataset correspondiente\n",
    "           b. Preparar DataLoader con batches mezclados\n",
    "           c. Entrenar por n煤mero espec铆fico de 茅pocas\n",
    "           d. Monitorear p茅rdida promedio por 茅poca\n",
    "        3. Guardar modelo final entrenado\n",
    "    \n",
    "    Beneficios del curriculum learning:\n",
    "        - Convergencia m谩s r谩pida y estable\n",
    "        - Mejor generalizaci贸n a nuevos escenarios\n",
    "        - Reducci贸n de overfitting a configuraciones espec铆ficas\n",
    "        - Aprendizaje progresivo de conceptos complejos\n",
    "    \"\"\"\n",
    "    # Inicializar componentes del entrenamiento\n",
    "    model = AgentNetwork()                              # Red convolucional para predicci贸n de acciones\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # Optimizador Adam para gradiente adaptativo\n",
    "    criterion = nn.CrossEntropyLoss()                  # Funci贸n de p茅rdida para clasificaci贸n multiclase\n",
    "\n",
    "    # Ejecutar curriculum learning secuencial\n",
    "    for i, (dataset_file, num_epochs) in enumerate(CURRICULUM):\n",
    "        print(f\"\\n--- Iniciando Lecci贸n {i+1}/{len(CURRICULUM)}: {dataset_file} ---\")\n",
    "        \n",
    "        # Cargar dataset de demostraci贸n experta\n",
    "        with open(dataset_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Preparar datos para entrenamiento\n",
    "        # Separar estados (entrada CNN) y acciones (etiquetas de clasificaci贸n)\n",
    "        states = torch.FloatTensor(np.array([item[0] for item in data]))   # Estados visuales (3, 5, 5)\n",
    "        actions = torch.LongTensor(np.array([item[1] for item in data]))   # ndices de acciones (0-3)\n",
    "        \n",
    "        # Crear DataLoader para entrenamiento por lotes\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(states, actions),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True  # Mezclar datos para evitar patrones de orden\n",
    "        )\n",
    "\n",
    "        # Entrenar modelo en el dataset actual\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0  # Acumulador de p茅rdida para la 茅poca\n",
    "            \n",
    "            # Procesar todos los lotes del dataset\n",
    "            for batch_states, batch_actions in dataloader:\n",
    "                # Paso hacia adelante: predicci贸n del modelo\n",
    "                optimizer.zero_grad()           # Limpiar gradientes acumulados\n",
    "                outputs = model(batch_states)   # Inferencia: estados  valores de acci贸n\n",
    "                \n",
    "                # Calcular p茅rdida de clasificaci贸n\n",
    "                loss = criterion(outputs, batch_actions)  # CrossEntropy entre predicci贸n y etiqueta\n",
    "                \n",
    "                # Retropropagaci贸n y optimizaci贸n\n",
    "                loss.backward()    # Calcular gradientes por backpropagation\n",
    "                optimizer.step()   # Actualizar pesos de la red\n",
    "                \n",
    "                # Acumular p茅rdida para monitoreo\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            # Reporte de progreso por 茅poca\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            print(f\"  poca {epoch+1}/{num_epochs}, P茅rdida: {avg_loss:.4f}\")\n",
    "\n",
    "    # Guardar modelo entrenado final\n",
    "    torch.save(model.state_dict(), \"imitacion_model.pth\")\n",
    "    print(\"\\n隆Entrenamiento por curr铆culo completado! Modelo final guardado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8bdd4",
   "metadata": {},
   "source": [
    "#### environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment.py\n",
    "\"\"\"\n",
    "Entorno de cuadr铆cula para aprendizaje por imitaci贸n de agentes.\n",
    "\n",
    "Este m贸dulo implementa un entorno de grid simplificado donde un agente\n",
    "debe navegar para recolectar frutas mientras evita venenos. Est谩 dise帽ado\n",
    "espec铆ficamente para generar datos de demostraci贸n experta y entrenar\n",
    "agentes mediante aprendizaje por imitaci贸n.\n",
    "\n",
    "Clases:\n",
    "    GridEnvironment: Entorno de cuadr铆cula con estados visuales 3D\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class GridEnvironment:\n",
    "    \"\"\"\n",
    "    Entorno de cuadr铆cula para simulaci贸n de navegaci贸n y recolecci贸n.\n",
    "    \n",
    "    Implementa un mundo de grid 2D donde el agente debe recolectar todas\n",
    "    las frutas evitando venenos. El estado se representa como una imagen\n",
    "    de 3 canales (agente, frutas, venenos) ideal para redes convolucionales.\n",
    "    \n",
    "    Caracter铆sticas:\n",
    "        - Grid cuadrado de tama帽o configurable\n",
    "        - Estados visuales como tensores 3D\n",
    "        - Movimiento con l铆mites del entorno\n",
    "        - Detecci贸n autom谩tica de colisiones\n",
    "        - Condiciones de terminaci贸n por victoria/derrota\n",
    "    \n",
    "    Attributes:\n",
    "        size (int): Tama帽o del grid (size x size)\n",
    "        agent_pos (np.ndarray): Posici贸n actual del agente [x, y]\n",
    "        fruit_pos (list): Lista de posiciones de frutas [np.ndarray, ...]\n",
    "        poison_pos (list): Lista de posiciones de venenos [np.ndarray, ...]\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno de grid con tama帽o especificado.\n",
    "        \n",
    "        Args:\n",
    "            size (int): Dimensi贸n del grid cuadrado (default: 5)\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, agent_pos=(0, 0), fruit_pos=[], poison_pos=[]):\n",
    "        \"\"\"\n",
    "        Reinicia el entorno con configuraci贸n espec铆fica de elementos.\n",
    "        \n",
    "        Establece posiciones iniciales del agente, frutas y venenos.\n",
    "        Utilizado para crear escenarios espec铆ficos para generaci贸n\n",
    "        de datos de demostraci贸n o evaluaci贸n de pol铆ticas.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (tuple): Posici贸n inicial del agente (x, y) (default: (0,0))\n",
    "            fruit_pos (list): Lista de posiciones de frutas [(x,y), ...] (default: [])\n",
    "            poison_pos (list): Lista de posiciones de venenos [(x,y), ...] (default: [])\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Estado inicial del entorno con forma (3, size, size)\n",
    "        \n",
    "        Note:\n",
    "            Las listas de posiciones se convierten a arrays numpy para\n",
    "            operaciones vectorizadas eficientes durante la simulaci贸n.\n",
    "        \"\"\"\n",
    "        self.agent_pos = np.array(agent_pos)\n",
    "        self.fruit_pos = [np.array(p) for p in fruit_pos]\n",
    "        self.poison_pos = [np.array(p) for p in poison_pos]\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Genera representaci贸n visual del estado actual como tensor 3D.\n",
    "        \n",
    "        Crea una imagen de 3 canales donde cada canal representa un tipo\n",
    "        de elemento del entorno. Esta representaci贸n es ideal para redes\n",
    "        convolucionales que procesan informaci贸n espacial.\n",
    "        \n",
    "        Estructura de canales:\n",
    "            - Canal 0: Posici贸n del agente (binario)\n",
    "            - Canal 1: Posiciones de frutas (binario)\n",
    "            - Canal 2: Posiciones de venenos (binario)\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Estado con forma (3, size, size) y dtype float32\n",
    "                       Valores: 1.0 para presencia de elemento, 0.0 para ausencia\n",
    "        \n",
    "        Example:\n",
    "            Para grid 3x3 con agente en (0,0) y fruta en (1,1):\n",
    "            Canal 0: [[1, 0, 0],    Canal 1: [[0, 0, 0],    Canal 2: [[0, 0, 0],\n",
    "                      [0, 0, 0],              [0, 1, 0],              [0, 0, 0],\n",
    "                      [0, 0, 0]]              [0, 0, 0]]              [0, 0, 0]]\n",
    "        \"\"\"\n",
    "        # Inicializar tensor de estado con ceros\n",
    "        state = np.zeros((3, self.size, self.size), dtype=np.float32)\n",
    "        \n",
    "        # Canal 0: Posici贸n del agente\n",
    "        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
    "        \n",
    "        # Canal 1: Posiciones de frutas\n",
    "        for fruit in self.fruit_pos:\n",
    "            state[1, fruit[0], fruit[1]] = 1.0\n",
    "        \n",
    "        # Canal 2: Posiciones de venenos\n",
    "        for poison in self.poison_pos:\n",
    "            state[2, poison[0], poison[1]] = 1.0\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Ejecuta una acci贸n en el entorno y actualiza el estado.\n",
    "        \n",
    "        Procesa el movimiento del agente, maneja colisiones con l铆mites,\n",
    "        detecta recolecci贸n de frutas y verifica condiciones de terminaci贸n.\n",
    "        Implementa la l贸gica core del entorno para simulaci贸n de episodios.\n",
    "        \n",
    "        Flujo de ejecuci贸n:\n",
    "            1. Actualizar posici贸n seg煤n acci贸n\n",
    "            2. Aplicar l铆mites del entorno\n",
    "            3. Procesar recolecci贸n de frutas\n",
    "            4. Verificar colisiones con venenos\n",
    "            5. Evaluar condiciones de terminaci贸n\n",
    "        \n",
    "        Args:\n",
    "            action (int): Acci贸n a ejecutar\n",
    "                         0 = Arriba (decrementar x)\n",
    "                         1 = Abajo (incrementar x)\n",
    "                         2 = Izquierda (decrementar y)\n",
    "                         3 = Derecha (incrementar y)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (nuevo_estado, reward, done)\n",
    "                - nuevo_estado (np.ndarray): Estado resultante (3, size, size)\n",
    "                - reward (float): Recompensa por la acci贸n (-0.1 por defecto)\n",
    "                - done (bool): True si episodio termin贸, False en caso contrario\n",
    "        \n",
    "        Note:\n",
    "            El reward no se utiliza en aprendizaje por imitaci贸n pero se\n",
    "            mantiene para compatibilidad con interfaces de RL est谩ndar.\n",
    "        \"\"\"\n",
    "        # Actualizar posici贸n del agente seg煤n la acci贸n\n",
    "        if action == 0: \n",
    "            self.agent_pos[0] -= 1  # Arriba\n",
    "        elif action == 1: \n",
    "            self.agent_pos[0] += 1  # Abajo\n",
    "        elif action == 2: \n",
    "            self.agent_pos[1] -= 1  # Izquierda\n",
    "        elif action == 3: \n",
    "            self.agent_pos[1] += 1  # Derecha\n",
    "        \n",
    "        # Aplicar l铆mites del entorno (clipping)\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.size - 1)\n",
    "\n",
    "        # Inicializar variables de terminaci贸n\n",
    "        done = False\n",
    "        reward = -0.1  # Penalizaci贸n por paso (no usado en imitaci贸n)\n",
    "\n",
    "        # Verificar recolecci贸n de frutas\n",
    "        for i, fruit in enumerate(self.fruit_pos):\n",
    "            if np.array_equal(self.agent_pos, fruit):\n",
    "                # Fruta recolectada: eliminar de la lista\n",
    "                self.fruit_pos.pop(i)\n",
    "                break\n",
    "        \n",
    "        # Verificar colisi贸n con venenos (derrota)\n",
    "        if any(np.array_equal(self.agent_pos, p) for p in self.poison_pos):\n",
    "            done = True\n",
    "\n",
    "        # Verificar victoria (todas las frutas recolectadas)\n",
    "        if not self.fruit_pos:\n",
    "            done = True\n",
    "        \n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4dd8ba",
   "metadata": {},
   "source": [
    "#### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\"\"\"\n",
    "Demostraci贸n simple del agente entrenado por aprendizaje por imitaci贸n.\n",
    "\n",
    "Este m贸dulo proporciona una interfaz minimalista para configurar escenarios\n",
    "y observar el comportamiento del agente entrenado. Utiliza formas geom茅tricas\n",
    "simples para representar elementos, enfoc谩ndose en la funcionalidad core\n",
    "sin distracciones visuales complejas.\n",
    "\n",
    "Caracter铆sticas:\n",
    "    - Configuraci贸n interactiva con mouse (clic izquierdo=fruta, clic derecho=veneno)\n",
    "    - Demostraci贸n autom谩tica del agente entrenado\n",
    "    - Representaci贸n visual simple con formas geom茅tricas\n",
    "    - Ciclo continuo configuraci贸n  demostraci贸n  reset\n",
    "\n",
    "Constantes:\n",
    "    GRID_SIZE: Tama帽o del entorno (5x5)\n",
    "    CELL_SIZE: Tama帽o de cada celda en p铆xeles (100px)\n",
    "    WIDTH, HEIGHT: Dimensiones de la ventana (500x500)\n",
    "    COLOR_*: Esquema de colores para elementos visuales\n",
    "\n",
    "Funciones:\n",
    "    draw_elements: Renderizado de elementos con formas geom茅tricas\n",
    "    main: Bucle principal con modos configuraci贸n y demostraci贸n\n",
    "\"\"\"\n",
    "import pygame\n",
    "import numpy as np\n",
    "from environment import GridEnvironment\n",
    "from agent import Agent\n",
    "\n",
    "# Configuraci贸n del entorno y ventana\n",
    "GRID_SIZE = 5\n",
    "CELL_SIZE = 100\n",
    "WIDTH, HEIGHT = GRID_SIZE * CELL_SIZE, GRID_SIZE * CELL_SIZE\n",
    "\n",
    "# Esquema de colores simple y claro\n",
    "COLOR_GRID = (200, 200, 200)    # Gris claro para grid\n",
    "COLOR_AGENT = (0, 0, 255)       # Azul para agente\n",
    "COLOR_FRUIT = (0, 255, 0)       # Verde para frutas\n",
    "COLOR_POISON = (255, 0, 0)      # Rojo para venenos\n",
    "\n",
    "def draw_elements(win, agent_pos, fruits, poisons):\n",
    "    \"\"\"\n",
    "    Renderiza todos los elementos del entorno usando formas geom茅tricas simples.\n",
    "    \n",
    "    Dibuja el grid de navegaci贸n y representa cada elemento del entorno\n",
    "    con formas distintivas: rect谩ngulos para agente, c铆rculos para frutas\n",
    "    y cuadrados peque帽os para venenos. Dise帽o minimalista para claridad.\n",
    "    \n",
    "    Representaci贸n visual:\n",
    "        - Agente: Rect谩ngulo azul de celda completa\n",
    "        - Frutas: C铆rculos verdes centrados (1/3 del tama帽o de celda)\n",
    "        - Venenos: Cuadrados rojos con margen (80% del tama帽o de celda)\n",
    "        - Grid: L铆neas grises para delimitaci贸n de celdas\n",
    "    \n",
    "    Args:\n",
    "        win (pygame.Surface): Superficie donde renderizar\n",
    "        agent_pos (np.ndarray): Posici贸n del agente [fila, columna]\n",
    "        fruits (list): Lista de posiciones de frutas [(fila, col), ...]\n",
    "        poisons (list): Lista de posiciones de venenos [(fila, col), ...]\n",
    "    \n",
    "    Note:\n",
    "        Convierte coordenadas (fila, columna) a p铆xeles (x, y) para Pygame.\n",
    "        Agente en posici贸n (-1, -1) no se dibuja (modo configuraci贸n).\n",
    "    \"\"\"\n",
    "    # Limpiar pantalla con fondo negro\n",
    "    win.fill((0,0,0))\n",
    "    \n",
    "    # Dibujar grid de navegaci贸n\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        pygame.draw.line(win, COLOR_GRID, (x, 0), (x, HEIGHT))\n",
    "    for y in range(0, HEIGHT, CELL_SIZE):\n",
    "        pygame.draw.line(win, COLOR_GRID, (0, y), (WIDTH, y))\n",
    "    # Dibujar agente (solo si posici贸n v谩lida)\n",
    "    if agent_pos[0] >= 0 and agent_pos[1] >= 0:\n",
    "        pygame.draw.rect(win, COLOR_AGENT, \n",
    "                        (agent_pos[1] * CELL_SIZE, agent_pos[0] * CELL_SIZE, \n",
    "                         CELL_SIZE, CELL_SIZE))\n",
    "    \n",
    "    # Dibujar frutas como c铆rculos verdes\n",
    "    for f in fruits:\n",
    "        center_x = f[1] * CELL_SIZE + CELL_SIZE//2\n",
    "        center_y = f[0] * CELL_SIZE + CELL_SIZE//2\n",
    "        radius = CELL_SIZE//3\n",
    "        pygame.draw.circle(win, COLOR_FRUIT, (center_x, center_y), radius)\n",
    "    \n",
    "    # Dibujar venenos como cuadrados rojos con margen\n",
    "    for p in poisons:\n",
    "        margin = 20  # Margen de 20px para distinguir de agente\n",
    "        rect_x = p[1] * CELL_SIZE + margin\n",
    "        rect_y = p[0] * CELL_SIZE + margin\n",
    "        rect_size = CELL_SIZE - 2 * margin\n",
    "        pygame.draw.rect(win, COLOR_POISON, (rect_x, rect_y, rect_size, rect_size))\n",
    "    \n",
    "    # Actualizar display para mostrar cambios\n",
    "    pygame.display.update()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal de la demostraci贸n simple del agente por imitaci贸n.\n",
    "    \n",
    "    Implementa un ciclo de dos modos: configuraci贸n interactiva donde el usuario\n",
    "    coloca elementos con el mouse, y demostraci贸n autom谩tica donde el agente\n",
    "    entrenado navega el escenario. Dise帽ado para evaluaci贸n r谩pida y directa\n",
    "    del rendimiento del modelo.\n",
    "    \n",
    "    Flujo de la aplicaci贸n:\n",
    "        1. Modo \"setup\": Usuario configura escenario con mouse\n",
    "           - Clic izquierdo: Colocar fruta\n",
    "           - Clic derecho: Colocar veneno\n",
    "           - Espacio: Iniciar demostraci贸n\n",
    "        \n",
    "        2. Modo \"run\": Agente navega autom谩ticamente\n",
    "           - Inferencia con modelo entrenado\n",
    "           - Movimiento autom谩tico cada 300ms\n",
    "           - Terminaci贸n por victoria/derrota\n",
    "           - Reset autom谩tico a configuraci贸n\n",
    "    \n",
    "    Controles:\n",
    "        - Clic izquierdo: Agregar fruta en posici贸n del mouse\n",
    "        - Clic derecho: Agregar veneno en posici贸n del mouse\n",
    "        - Espacio: Iniciar demostraci贸n (solo si hay frutas)\n",
    "        - Autom谩tico: Reset a configuraci贸n al terminar episodio\n",
    "    \n",
    "    Note:\n",
    "        Requiere modelo entrenado \"imitacion_model.pth\" en directorio actual.\n",
    "        El agente siempre inicia en posici贸n (0,0) del grid.\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y configurar ventana\n",
    "    pygame.init()\n",
    "    win = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "    pygame.display.set_caption(\"Agente Come-Frutas (IA)\")\n",
    "    \n",
    "    # Inicializar entorno y agente con modelo pre-entrenado\n",
    "    env = GridEnvironment(size=GRID_SIZE)\n",
    "    agent = Agent()\n",
    "    agent.load_model(\"imitacion_model.pth\")\n",
    "\n",
    "    # Variables de estado de la aplicaci贸n\n",
    "    fruits, poisons = [], []  # Listas de posiciones de elementos\n",
    "    mode = \"setup\"           # Modo inicial: configuraci贸n\n",
    "    run = True              # Control del bucle principal\n",
    "    # Bucle principal de la aplicaci贸n\n",
    "    while run:\n",
    "        # Procesar eventos de entrada\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "\n",
    "            # L贸gica espec铆fica del modo configuraci贸n\n",
    "            if mode == \"setup\":\n",
    "                if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                    # Convertir posici贸n del mouse a coordenadas de grid\n",
    "                    pos = pygame.mouse.get_pos()\n",
    "                    col, row = pos[0] // CELL_SIZE, pos[1] // CELL_SIZE\n",
    "                    \n",
    "                    # Clic izquierdo: Agregar fruta (si no existe)\n",
    "                    if event.button == 1 and (row, col) not in fruits:\n",
    "                        fruits.append((row, col))\n",
    "                    # Clic derecho: Agregar veneno (si no existe)\n",
    "                    elif event.button == 3 and (row, col) not in poisons:\n",
    "                        poisons.append((row, col))\n",
    "                \n",
    "                # Espacio: Iniciar demostraci贸n si hay frutas configuradas\n",
    "                if event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE:\n",
    "                    if fruits:  # Solo si hay al menos una fruta\n",
    "                        mode = \"run\"\n",
    "                        # Inicializar entorno con configuraci贸n actual\n",
    "                        env.reset(agent_pos=(0,0), fruit_pos=fruits, poison_pos=poisons)\n",
    "\n",
    "        # Renderizado seg煤n el modo actual\n",
    "        if mode == \"setup\":\n",
    "            # Modo configuraci贸n: mostrar elementos sin agente\n",
    "            draw_elements(win, np.array([-1,-1]), fruits, poisons)\n",
    "            \n",
    "        elif mode == \"run\":\n",
    "            # Modo demostraci贸n: agente autom谩tico\n",
    "            \n",
    "            # Obtener estado actual y generar acci贸n\n",
    "            state = env.get_state()\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Ejecutar acci贸n y verificar terminaci贸n\n",
    "            _, _, done = env.step(action)\n",
    "            \n",
    "            # Renderizar estado actualizado\n",
    "            draw_elements(win, env.agent_pos, env.fruit_pos, env.poison_pos)\n",
    "            \n",
    "            # Procesar terminaci贸n del episodio\n",
    "            if done:\n",
    "                print(\"隆Simulaci贸n terminada!\")\n",
    "                pygame.time.delay(2000)  # Pausa para observar resultado final\n",
    "                # Reset autom谩tico a modo configuraci贸n\n",
    "                fruits, poisons = [], []\n",
    "                mode = \"setup\"\n",
    "            \n",
    "            # Controlar velocidad de demostraci贸n\n",
    "            pygame.time.delay(300)  # 300ms entre acciones para visibilidad\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a9c28",
   "metadata": {},
   "source": [
    "### Jugador humano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee67918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modo de juego humano con controles aleatorios - Come Frutas.\n",
    "\n",
    "Este m贸dulo implementa una versi贸n jugable del entorno donde un humano puede\n",
    "controlar el agente directamente. La caracter铆stica 煤nica es que los controles\n",
    "de movimiento se asignan aleatoriamente cada vez que se inicia una partida,\n",
    "a帽adiendo un elemento de desaf铆o y adaptabilidad.\n",
    "\n",
    "Caracter铆sticas principales:\n",
    "- Modo Setup: Configuraci贸n manual del escenario\n",
    "- Modo Humano: Control directo del agente por el jugador\n",
    "- Controles aleatorios: Mapeo aleatorio de teclas a movimientos\n",
    "- Interfaz intuitiva: Gr谩ficos y feedback visual\n",
    "- Desaf铆o adaptativo: Cada partida requiere aprender nuevos controles\n",
    "\n",
    "Prop贸sito educativo:\n",
    "- Comparar rendimiento humano vs. IA\n",
    "- Experimentar la dificultad de adaptaci贸n a controles cambiantes\n",
    "- Entender la importancia de la consistencia en interfaces\n",
    "- Apreciar la flexibilidad del aprendizaje humano\n",
    "\n",
    "Autor: [Tu nombre]\n",
    "Fecha: Agosto 2025\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "# CONFIGURACIN DEL ENTORNO VISUAL\n",
    "\"\"\"\n",
    "Par谩metros visuales y dimensiones de la interfaz de juego.\n",
    "Utiliza celdas m谩s grandes (120px) para mejor visibilidad durante el juego manual.\n",
    "\"\"\"\n",
    "GRID_WIDTH = 5              # Ancho de la cuadr铆cula en celdas\n",
    "GRID_HEIGHT = 5             # Alto de la cuadr铆cula en celdas\n",
    "CELL_SIZE = 120             # Tama帽o de cada celda en p铆xeles (mayor para juego manual)\n",
    "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE    # Ancho total de la ventana (600px)\n",
    "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE  # Alto del 谩rea de juego (600px)\n",
    "\n",
    "# PALETA DE COLORES CONSISTENTE\n",
    "\"\"\"\n",
    "Esquema de colores oscuro profesional, consistente con otros m贸dulos del proyecto.\n",
    "\"\"\"\n",
    "COLOR_FONDO = (25, 25, 25)      # Gris muy oscuro para el fondo\n",
    "COLOR_LINEAS = (40, 40, 40)     # Gris oscuro para l铆neas de cuadr铆cula\n",
    "COLOR_CURSOR = (255, 255, 0)    # Amarillo brillante para cursor de selecci贸n\n",
    "COLOR_TEXTO = (230, 230, 230)   # Gris claro para texto legible\n",
    "\n",
    "# SISTEMA DE CONTROLES ALEATORIOS\n",
    "\"\"\"\n",
    "Genera un conjunto de teclas v谩lidas para asignaci贸n aleatoria de controles.\n",
    "Se evitan teclas especiales para prevenir conflictos con funciones del sistema.\n",
    "\"\"\"\n",
    "TECLAS_VALIDAS = [getattr(pygame, f\"K_{c}\") for c in string.ascii_lowercase + string.digits]\n",
    "\n",
    "class EntornoHumano:\n",
    "    \"\"\"\n",
    "    Entorno de juego optimizado para control humano directo.\n",
    "    \n",
    "    Esta clase maneja la l贸gica del juego cuando un humano controla el agente,\n",
    "    incluyendo movimiento, colisiones, recolecci贸n de objetos y condiciones\n",
    "    de victoria/derrota. Se enfoca en proporcionar feedback inmediato y\n",
    "    una experiencia de juego fluida.\n",
    "    \n",
    "    Diferencias con entornos de IA:\n",
    "    - Feedback inmediato con mensajes en consola\n",
    "    - L贸gica de juego simplificada (sin recompensas num茅ricas)\n",
    "    - Terminaci贸n inmediata en victoria/derrota\n",
    "    - Controles responsivos para jugabilidad humana\n",
    "    \n",
    "    Attributes:\n",
    "        agente_pos (tuple): Posici贸n actual del agente (x, y)\n",
    "        frutas (set): Conjunto de posiciones con frutas\n",
    "        venenos (set): Conjunto de posiciones con venenos\n",
    "        paredes (set): Conjunto de posiciones con paredes/obst谩culos\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno con configuraci贸n vac铆a.\n",
    "        \n",
    "        El agente comienza en la esquina superior izquierda (0,0) y todos\n",
    "        los conjuntos de elementos est谩n vac铆os, permitiendo configuraci贸n manual.\n",
    "        \"\"\"\n",
    "        self.agente_pos = (0, 0)    # Posici贸n inicial del agente\n",
    "        self.frutas = set()         # Conjunto de posiciones de frutas\n",
    "        self.venenos = set()        # Conjunto de posiciones de venenos\n",
    "        self.paredes = set()        # Conjunto de posiciones de paredes\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resetea la posici贸n del agente al inicio del juego.\n",
    "        \n",
    "        Coloca al agente en la posici贸n inicial (0,0) sin modificar\n",
    "        la configuraci贸n del escenario. Utilizado al comenzar una nueva partida.\n",
    "        \"\"\"\n",
    "        self.agente_pos = (0, 0)\n",
    "\n",
    "    def limpiar(self):\n",
    "        \"\"\"\n",
    "        Elimina todos los elementos del entorno.\n",
    "        \n",
    "        Limpia frutas, venenos y paredes del escenario, dejando una\n",
    "        cuadr铆cula vac铆a para configuraci贸n desde cero.\n",
    "        \"\"\"\n",
    "        self.frutas.clear()\n",
    "        self.venenos.clear()\n",
    "        self.paredes.clear()\n",
    "\n",
    "    def step(self, accion):\n",
    "        \"\"\"\n",
    "        Ejecuta una acci贸n del jugador humano en el entorno.\n",
    "        \n",
    "        Procesa el movimiento del agente, verifica colisiones y maneja\n",
    "        las interacciones con elementos del entorno. Proporciona feedback\n",
    "        inmediato al jugador mediante mensajes en consola.\n",
    "        \n",
    "        Args:\n",
    "            accion (int): Direcci贸n de movimiento:\n",
    "                         0 = Arriba (decrementar y)\n",
    "                         1 = Abajo (incrementar y)\n",
    "                         2 = Izquierda (decrementar x)\n",
    "                         3 = Derecha (incrementar x)\n",
    "        \n",
    "        Returns:\n",
    "            bool: True si el juego termin贸 (victoria o derrota), False si contin煤a\n",
    "        \"\"\"\n",
    "        # Calcular nueva posici贸n basada en la acci贸n\n",
    "        x, y = self.agente_pos\n",
    "        if accion == 0:     # Arriba\n",
    "            y -= 1\n",
    "        elif accion == 1:   # Abajo\n",
    "            y += 1\n",
    "        elif accion == 2:   # Izquierda\n",
    "            x -= 1\n",
    "        elif accion == 3:   # Derecha\n",
    "            x += 1\n",
    "\n",
    "        # Verificar colisiones: l铆mites del tablero o paredes\n",
    "        if x < 0 or x >= GRID_WIDTH or y < 0 or y >= GRID_HEIGHT or (x, y) in self.paredes:\n",
    "            # Movimiento inv谩lido: no actualizar posici贸n\n",
    "            return False\n",
    "\n",
    "        # Movimiento v谩lido: actualizar posici贸n del agente\n",
    "        self.agente_pos = (x, y)\n",
    "        \n",
    "        # Verificar interacciones con elementos del entorno\n",
    "        if self.agente_pos in self.frutas:\n",
    "            # Fruta recogida: eliminar del conjunto\n",
    "            self.frutas.remove(self.agente_pos)\n",
    "            \n",
    "            # Verificar condici贸n de victoria\n",
    "            if not self.frutas:\n",
    "                print(\"\\n 隆Ganaste! Recolectaste todas las frutas.\\n\")\n",
    "                return True  # Juego terminado con 茅xito\n",
    "                \n",
    "        elif self.agente_pos in self.venenos:\n",
    "            # Veneno tocado: derrota inmediata\n",
    "            print(\"\\n锔 隆Oh no! Tocaste un veneno.\\n\")\n",
    "            return True  # Juego terminado con fallo\n",
    "            \n",
    "        # Continuar juego\n",
    "        return False\n",
    "\n",
    "    def dibujar(self, pantalla, modo, cursor_pos, img_fruta, img_veneno, img_pared, img_agente, _):\n",
    "        \"\"\"\n",
    "        Renderiza el estado completo del entorno con interfaz interactiva.\n",
    "        \n",
    "        Dibuja todos los elementos visuales del juego incluyendo grid, objetos\n",
    "        del entorno y cursor de selecci贸n. Proporciona feedback visual para\n",
    "        la interacci贸n del jugador en diferentes modos (colocaci贸n/juego).\n",
    "        \n",
    "        Args:\n",
    "            pantalla (pygame.Surface): Superficie donde renderizar\n",
    "            modo (str): Modo actual de la interfaz ('frutas', 'venenos', 'paredes', 'jugar')\n",
    "            cursor_pos (tuple): Posici贸n (x,y) del cursor en coordenadas de grid\n",
    "            img_fruta (pygame.Surface): Sprite de las frutas\n",
    "            img_veneno (pygame.Surface): Sprite de los venenos\n",
    "            img_pared (pygame.Surface): Sprite de las paredes\n",
    "            img_agente (pygame.Surface): Sprite del agente\n",
    "            _ : Par谩metro no utilizado (compatibilidad de interfaz)\n",
    "        \n",
    "        Note:\n",
    "            Renderiza en orden espec铆fico: fondo, grid, objetos, agente, cursor.\n",
    "            El cursor cambia de color seg煤n el modo de colocaci贸n activo.\n",
    "        \"\"\"\n",
    "        # Limpiar pantalla con color de fondo\n",
    "        pantalla.fill(COLOR_FONDO)\n",
    "\n",
    "        # Dibujar l铆neas del grid para gu铆a visual\n",
    "        for x in range(0, SCREEN_WIDTH, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (x, 0), (x, SCREEN_HEIGHT))\n",
    "        for y in range(0, SCREEN_HEIGHT, CELL_SIZE):\n",
    "            pygame.draw.line(pantalla, COLOR_LINEAS, (0, y), (SCREEN_WIDTH, y))\n",
    "\n",
    "        # Dibujar elementos del entorno\n",
    "        for fruta in self.frutas:\n",
    "            pantalla.blit(img_fruta, (fruta[0]*CELL_SIZE, fruta[1]*CELL_SIZE))\n",
    "        for veneno in self.venenos:\n",
    "            pantalla.blit(img_veneno, (veneno[0]*CELL_SIZE, veneno[1]*CELL_SIZE))\n",
    "        for pared in self.paredes:\n",
    "            pantalla.blit(img_pared, (pared[0]*CELL_SIZE, pared[1]*CELL_SIZE))\n",
    "\n",
    "        # Dibujar agente (jugador) - siempre visible en primer plano\n",
    "        pantalla.blit(img_agente, (self.agente_pos[0]*CELL_SIZE, self.agente_pos[1]*CELL_SIZE))\n",
    "\n",
    "        # Dibujar cursor de selecci贸n en modo configuraci贸n\n",
    "        if modo == \"SETUP\":\n",
    "            cursor_rect = pygame.Rect(cursor_pos[0]*CELL_SIZE, cursor_pos[1]*CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "            pygame.draw.rect(pantalla, COLOR_CURSOR, cursor_rect, 3)\n",
    "\n",
    "        # Renderizar informaci贸n de interfaz\n",
    "        font = pygame.font.Font(None, 30)\n",
    "        pantalla.blit(font.render(f\"Modo: {modo}\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 5))\n",
    "        pantalla.blit(font.render(\"F: Fruta, V: Veneno, W: Pared, C: Limpiar, H: Jugar\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 30))\n",
    "        pantalla.blit(font.render(\"Descubre los controles ocultos usando letras/n煤meros\", True, COLOR_TEXTO), (10, SCREEN_HEIGHT + 55))\n",
    "\n",
    "def cargar_imagen(nombre, fallback_color):\n",
    "    \"\"\"\n",
    "    Carga una imagen desde archivo con sistema de respaldo.\n",
    "    \n",
    "    Intenta cargar una imagen sprite desde el directorio actual.\n",
    "    Si la carga falla, crea una superficie de color s贸lido como respaldo.\n",
    "    Escala autom谩ticamente al tama帽o de celda definido.\n",
    "    \n",
    "    Args:\n",
    "        nombre (str): Nombre del archivo de imagen a cargar\n",
    "        fallback_color (tuple): Color RGB (r,g,b) para superficie de respaldo\n",
    "    \n",
    "    Returns:\n",
    "        pygame.Surface: Superficie cargada y escalada, o superficie de color\n",
    "                       si la carga fall贸\n",
    "    \n",
    "    Note:\n",
    "        Todas las im谩genes se escalan a CELL_SIZE x CELL_SIZE p铆xeles.\n",
    "        Utiliza convert_alpha() para optimizar el renderizado con transparencia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir ruta completa al archivo de imagen\n",
    "        ruta = os.path.join(os.path.dirname(__file__), nombre)\n",
    "        # Cargar imagen con soporte de transparencia\n",
    "        img = pygame.image.load(ruta).convert_alpha()\n",
    "        # Escalar a tama帽o de celda est谩ndar\n",
    "        return pygame.transform.scale(img, (CELL_SIZE, CELL_SIZE))\n",
    "    except:\n",
    "        # Crear superficie de respaldo con color s贸lido si falla la carga\n",
    "        s = pygame.Surface((CELL_SIZE, CELL_SIZE))\n",
    "        s.fill(fallback_color)\n",
    "        return s\n",
    "\n",
    "def generar_controles_aleatorios():\n",
    "    \"\"\"\n",
    "    Genera un mapeo aleatorio de teclas para controles de movimiento.\n",
    "    \n",
    "    Crea una asignaci贸n aleatoria entre teclas del teclado y direcciones\n",
    "    de movimiento para a帽adir un elemento de desaf铆o y descubrimiento\n",
    "    al juego. Los jugadores deben encontrar qu茅 teclas controlan cada direcci贸n.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapeo de c贸digos de tecla pygame a acciones de movimiento:\n",
    "              {tecla_pygame: accion_int}\n",
    "              donde accion_int es 0=Arriba, 1=Abajo, 2=Izquierda, 3=Derecha\n",
    "    \n",
    "    Note:\n",
    "        Utiliza teclas alfanum茅ricas (A-Z, 0-9) para m谩xima compatibilidad.\n",
    "        Garantiza que cada direcci贸n tenga exactamente una tecla asignada.\n",
    "    \"\"\"\n",
    "    # Seleccionar 4 teclas aleatorias del conjunto disponible\n",
    "    teclas = random.sample(TECLAS_VALIDAS, 4)\n",
    "    # Crear lista de acciones de movimiento\n",
    "    acciones = [0, 1, 2, 3]  # Arriba, abajo, izquierda, derecha\n",
    "    # Mezclar aleatoriamente las acciones\n",
    "    random.shuffle(acciones)\n",
    "    # Crear diccionario de mapeo tecla->acci贸n\n",
    "    return dict(zip(teclas, acciones))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci贸n principal del juego en modo humano.\n",
    "    \n",
    "    Inicializa Pygame, configura la ventana de juego y ejecuta el bucle\n",
    "    principal que maneja dos modos: configuraci贸n del entorno y juego\n",
    "    con controles aleatorios. Proporciona una experiencia interactiva\n",
    "    donde el jugador puede dise帽ar niveles y luego jugarlos.\n",
    "    \n",
    "    Flujo del juego:\n",
    "        1. Modo SETUP: Colocar frutas, venenos y paredes con el mouse\n",
    "        2. Modo JUGAR: Controlar agente con teclas aleatorias descubiertas\n",
    "        3. Victoria: Recolectar todas las frutas\n",
    "        4. Derrota: Tocar veneno\n",
    "    \n",
    "    Controles SETUP:\n",
    "        - Mouse: Mover cursor\n",
    "        - F: Colocar fruta\n",
    "        - V: Colocar veneno  \n",
    "        - W: Colocar pared\n",
    "        - C: Limpiar todo\n",
    "        - H: Iniciar juego\n",
    "    \n",
    "    Controles JUGAR:\n",
    "        - Teclas aleatorias para movimiento (descubrir experimentando)\n",
    "        - ESC: Volver a configuraci贸n\n",
    "    \"\"\"\n",
    "    # Inicializar Pygame y configurar ventana\n",
    "    pygame.init()\n",
    "    pantalla = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT + 100))\n",
    "    pygame.display.set_caption(\"Modo Humano Aleatorio - Come Frutas\")\n",
    "\n",
    "    # Inicializar entorno y variables de estado\n",
    "    entorno = EntornoHumano()\n",
    "    cursor_pos = [0, 0]\n",
    "    modo = \"SETUP\"  # Modo inicial: configuraci贸n del entorno\n",
    "    mapeo_controles = {}  # Mapeo de teclas aleatorias (generado al jugar)\n",
    "\n",
    "    # Cargar sprites con colores de respaldo\n",
    "    img_fruta = cargar_imagen(\"fruta.png\", (40, 200, 40))\n",
    "    img_veneno = cargar_imagen(\"veneno.png\", (255, 50, 50))\n",
    "    img_pared = cargar_imagen(\"pared.jpg\", (80, 80, 80))\n",
    "    img_agente = cargar_imagen(\"agente.png\", (60, 100, 255))\n",
    "\n",
    "    # Variables de control del juego\n",
    "    reloj = pygame.time.Clock()\n",
    "    corriendo = True\n",
    "\n",
    "    # Bucle principal del juego\n",
    "    while corriendo:\n",
    "        # Procesar eventos de entrada\n",
    "        for evento in pygame.event.get():\n",
    "            if evento.type == pygame.QUIT:\n",
    "                corriendo = False\n",
    "            elif evento.type == pygame.KEYDOWN:\n",
    "                if evento.key == pygame.K_s:\n",
    "                    modo = \"SETUP\"\n",
    "                elif evento.key == pygame.K_h:\n",
    "                    modo = \"HUMANO\"\n",
    "                    entorno.reset()\n",
    "                    mapeo_controles = generar_controles_aleatorios()\n",
    "\n",
    "                if modo == \"SETUP\":\n",
    "                    if evento.key == pygame.K_UP: cursor_pos[1] = max(0, cursor_pos[1]-1)\n",
    "                    elif evento.key == pygame.K_DOWN: cursor_pos[1] = min(GRID_HEIGHT-1, cursor_pos[1]+1)\n",
    "                    elif evento.key == pygame.K_LEFT: cursor_pos[0] = max(0, cursor_pos[0]-1)\n",
    "                    elif evento.key == pygame.K_RIGHT: cursor_pos[0] = min(GRID_WIDTH-1, cursor_pos[0]+1)\n",
    "                    # Colocaci贸n de elementos con teclas espec铆ficas\n",
    "                    pos = tuple(cursor_pos)\n",
    "                    if evento.key == pygame.K_f: \n",
    "                        # F: Colocar/quitar fruta (toggle)\n",
    "                        entorno.frutas.symmetric_difference_update({pos})\n",
    "                        entorno.venenos.discard(pos)\n",
    "                        entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_v: \n",
    "                        # V: Colocar/quitar veneno (toggle)\n",
    "                        entorno.venenos.symmetric_difference_update({pos})\n",
    "                        entorno.frutas.discard(pos)\n",
    "                        entorno.paredes.discard(pos)\n",
    "                    elif evento.key == pygame.K_w: \n",
    "                        # W: Colocar/quitar pared (toggle)\n",
    "                        entorno.paredes.symmetric_difference_update({pos})\n",
    "                        entorno.frutas.discard(pos)\n",
    "                        entorno.venenos.discard(pos)\n",
    "                    elif evento.key == pygame.K_c: \n",
    "                        # C: Limpiar todo el entorno\n",
    "                        entorno.limpiar()\n",
    "\n",
    "                # Controles espec铆ficos del modo HUMANO\n",
    "                elif modo == \"HUMANO\":\n",
    "                    if evento.key in mapeo_controles:\n",
    "                        # Ejecutar acci贸n de movimiento con tecla aleatoria\n",
    "                        accion = mapeo_controles[evento.key]\n",
    "                        terminado = entorno.step(accion)\n",
    "                        if terminado:\n",
    "                            # Volver a configuraci贸n al terminar el juego\n",
    "                            modo = \"SETUP\"\n",
    "\n",
    "        # Renderizar estado actual del juego\n",
    "        entorno.dibujar(pantalla, modo, cursor_pos, img_fruta, img_veneno, img_pared, img_agente, mapeo_controles)\n",
    "        pygame.display.flip()\n",
    "        reloj.tick(30)\n",
    "\n",
    "    # Limpiar recursos al salir\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
