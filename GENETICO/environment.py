# environment.py
"""
Entorno de cuadr铆cula especializado para algoritmos gen茅ticos.

Este m贸dulo implementa un entorno modificado para el entrenamiento de agentes mediante
algoritmos gen茅ticos. La principal diferencia con el entorno DQN es el manejo de venenos:
en lugar de terminar el episodio, el agente es enviado de vuelta a la posici贸n inicial
con una penalizaci贸n, permitiendo episodios m谩s largos y mejor evaluaci贸n de fitness.

Caracter铆sticas espec铆ficas para GA:
- Venenos no terminan el episodio, sino que resetean la posici贸n
- Episodios m谩s largos para mejor evaluaci贸n de fitness
- Recompensas ajustadas para discriminar mejor entre agentes
- Seguimiento de posici贸n inicial para reset de venenos

"""

import numpy as np

class GridEnvironment:
    """
    Entorno de cuadr铆cula optimizado para algoritmos gen茅ticos.
    
    Este entorno est谩 dise帽ado espec铆ficamente para la evaluaci贸n de agentes
    mediante algoritmos gen茅ticos. La principal modificaci贸n es que tocar venenos
    no termina el episodio, sino que env铆a al agente de vuelta al inicio,
    permitiendo episodios m谩s largos y una mejor discriminaci贸n entre agentes.
    
    Caracter铆sticas para GA:
    - Episodios m谩s largos para mejor evaluaci贸n de fitness
    - Venenos causan reset de posici贸n en lugar de game over
    - Recompensas ajustadas para mejor selecci贸n evolutiva
    - Seguimiento de posici贸n inicial para mec谩nica de reset
    
    Attributes:
        size (int): Tama帽o de la cuadr铆cula (size x size)
        start_pos (np.array): Posici贸n inicial del agente en el episodio
        agent_pos (np.array): Posici贸n actual del agente
        fruit_pos (list): Lista de posiciones de frutas
        poison_pos (list): Lista de posiciones de venenos
    """
    def __init__(self, size=5):
        """
        Inicializa el entorno de cuadr铆cula para algoritmos gen茅ticos.
        
        Args:
            size (int, optional): Tama帽o de la cuadr铆cula. Por defecto es 5x5.
        """
        self.size = size
        self.start_pos = (0, 0)  # Guardar posici贸n inicial para reset de venenos
        self.reset()

    def reset(self, agent_pos=(0, 0), fruit_pos=[], poison_pos=[]):
        """
        Reinicia el entorno con una configuraci贸n espec铆fica.
        
        Establece las posiciones iniciales y guarda la posici贸n de inicio del agente
        para la mec谩nica de reset por venenos. Esta posici贸n inicial es crucial
        en el paradigma de algoritmos gen茅ticos ya que permite que el agente
        contin煤e intentando despu茅s de errores.
        
        Args:
            agent_pos (tuple, optional): Posici贸n inicial del agente (fila, columna). 
                                       Por defecto (0, 0).
            fruit_pos (list, optional): Lista de tuplas con posiciones de frutas.
                                       Por defecto lista vac铆a.
            poison_pos (list, optional): Lista de tuplas con posiciones de venenos.
                                        Por defecto lista vac铆a.
        
        Returns:
            np.array: Estado inicial del entorno como array 3D (3, size, size).
        """
        self.start_pos = np.array(agent_pos)  # Guardar posici贸n inicial del episodio
        self.agent_pos = np.array(agent_pos)
        self.fruit_pos = [np.array(p) for p in fruit_pos]
        self.poison_pos = [np.array(p) for p in poison_pos]
        return self.get_state()

    def get_state(self):
        """
        Genera la representaci贸n del estado actual del entorno.
        
        Id茅ntica implementaci贸n al entorno DQN. El estado se representa como una 
        "imagen" de 3 canales que puede ser procesada por redes convolucionales.
        
        - Canal 0: Posici贸n del agente (1.0 donde est谩 el agente, 0.0 en el resto)
        - Canal 1: Posiciones de frutas (1.0 donde hay frutas, 0.0 en el resto)  
        - Canal 2: Posiciones de venenos (1.0 donde hay venenos, 0.0 en el resto)
        
        Esta representaci贸n permite que el agente "vea" todo el entorno de una vez
        y es compatible con arquitecturas de redes neuronales convolucionales.
        
        Returns:
            np.array: Estado del entorno como array 3D de forma (3, size, size)
                     con valores float32.
        """
        state = np.zeros((3, self.size, self.size), dtype=np.float32)
        
        # Canal 0: Posici贸n del agente
        # Canal 0: Posici贸n del agente
        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0
        
        # Canal 1: Posiciones de las frutas
        for fruit in self.fruit_pos:
            state[1, fruit[0], fruit[1]] = 1.0
            
        # Canal 2: Posiciones de los venenos
        for poison in self.poison_pos:
            state[2, poison[0], poison[1]] = 1.0
            
        return state

    def step(self, action):
        """
        Ejecuta una acci贸n en el entorno optimizado para algoritmos gen茅ticos.
        
        Esta funci贸n implementa la l贸gica principal del juego con modificaciones
        espec铆ficas para algoritmos gen茅ticos. La diferencia clave es el manejo
        de venenos: en lugar de terminar el episodio, el agente se resetea a la
        posici贸n inicial, permitiendo episodios m谩s largos y mejor evaluaci贸n.
        
        Diferencias con DQN:
        - Venenos NO terminan el episodio
        - Venenos resetean la posici贸n del agente al inicio
        - Penalizaci贸n mayor por venenos (-10.0 vs -1.0)
        - Episodios m谩s largos para mejor discriminaci贸n de fitness
        
        Args:
            action (int): Acci贸n a realizar:
                         0 = Arriba (decrementar fila)
                         1 = Abajo (incrementar fila)  
                         2 = Izquierda (decrementar columna)
                         3 = Derecha (incrementar columna)
        
        Returns:
            tuple: (nuevo_estado, recompensa, terminado)
                - nuevo_estado (np.array): Estado del entorno despu茅s de la acci贸n
                - recompensa (float): Recompensa obtenida por la acci贸n
                - terminado (bool): True solo si todas las frutas fueron recogidas
        """
        
        # FASE 1: MOVIMIENTO DEL AGENTE
        # L贸gica id茅ntica al entorno DQN
        if action == 0: 
            self.agent_pos[0] -= 1    # Arriba
        elif action == 1: 
            self.agent_pos[0] += 1    # Abajo
        elif action == 2: 
            self.agent_pos[1] -= 1    # Izquierda
        elif action == 3: 
            self.agent_pos[1] += 1    # Derecha
            
        # Limitar posici贸n a los l铆mites del tablero
        self.agent_pos = np.clip(self.agent_pos, 0, self.size - 1)

        # FASE 2: INICIALIZACIN DE RECOMPENSAS
        reward = -0.05  # Peque帽o castigo por cada movimiento
        done = False

        # FASE 2: INICIALIZACIN DE RECOMPENSAS
        reward = -0.05  # Peque帽o castigo por cada movimiento
        done = False

        # FASE 3: MANEJO ESPECIAL DE VENENOS (DIFERENCIA CLAVE CON DQN)
        if any(np.array_equal(self.agent_pos, p) for p in self.poison_pos):
            # Veneno tocado: penalizaci贸n severa pero NO termina el episodio
            reward = -10.0
            # CARACTERSTICA PRINCIPAL: Reset a posici贸n inicial
            self.agent_pos = np.copy(self.start_pos)
            # CRTICO: done permanece False, el episodio contin煤a
            print(" Agente toc贸 veneno, reseteado a posici贸n inicial")
        else:
            # FASE 4: LGICA NORMAL (SOLO SI NO HAY VENENO)
            # Verificar si se recogi贸 una fruta
            eaten_fruit_this_step = False
            for i, fruit in enumerate(self.fruit_pos):
                if np.array_equal(self.agent_pos, fruit):
                    reward += 1.0  # Recompensa por fruta
                    self.fruit_pos.pop(i)  # Remover fruta del entorno
                    eaten_fruit_this_step = True
                    print(" Fruta recogida!")
                    break  # Solo una fruta por paso

            # Reward shaping opcional (sin implementar aqu铆)
            if not eaten_fruit_this_step and self.fruit_pos:
                # Aqu铆 se podr铆a agregar l贸gica de distancia como en DQN
                # Dejado como comentario para mantener simplicidad
                pass

            # FASE 5: CONDICIN DE VICTORIA
            if not self.fruit_pos:
                done = True
                reward += 10.0  # Gran recompensa por completar el nivel
                print(" 隆Todas las frutas recogidas! Episodio completado")

        return self.get_state(), reward, done