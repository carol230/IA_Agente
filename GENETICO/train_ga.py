# train_ga.py
"""
Implementaci√≥n completa de algoritmo gen√©tico para entrenar agentes de IA.

Este m√≥dulo implementa un algoritmo gen√©tico completo que evoluciona poblaciones
de agentes para resolver el problema de recolecci√≥n de frutas. El algoritmo
simula la evoluci√≥n natural mediante selecci√≥n, cruzamiento y mutaci√≥n.

Proceso evolutivo:
1. Inicializaci√≥n: Crear poblaci√≥n aleatoria de agentes
2. Evaluaci√≥n: Probar cada agente en escenarios aleatorios  
3. Selecci√≥n: Elegir los mejores agentes como padres
4. Cruzamiento: Combinar genes de padres para crear hijos
5. Mutaci√≥n: Introducir variaci√≥n aleatoria en los genes
6. Reemplazo: Formar nueva generaci√≥n con elitismo
7. Repetir hasta convergencia

Caracter√≠sticas del algoritmo:
- Evaluaci√≥n en escenarios aleatorios para robustez
- Elitismo para preservar mejores soluciones
- Mutaci√≥n gaussiana para exploraci√≥n controlada
- Cruzamiento uniforme para recombinaci√≥n equilibrada

"""

import torch
import numpy as np
from environment import GridEnvironment
from agent_ga import Agent, AgentNetwork
import random

# HIPERPAR√ÅMETROS DEL ALGORITMO GEN√âTICO
"""
Configuraci√≥n de par√°metros evolutivos que controlan el comportamiento
del algoritmo gen√©tico. Estos valores han sido ajustados emp√≠ricamente
para balancear exploraci√≥n vs. explotaci√≥n.
"""
POPULATION_SIZE = 100    # Tama√±o de la poblaci√≥n por generaci√≥n
NUM_GENERATIONS = 500    # N√∫mero total de generaciones a evolucionar
MUTATION_RATE = 0.05     # Probabilidad de mutaci√≥n por gen (5%)
ELITISM_COUNT = 25       # Mejores agentes que pasan directamente (25%)

GRID_SIZE = 5           # Tama√±o del entorno de evaluaci√≥n

def create_initial_population():
    """
    Crea la poblaci√≥n inicial de agentes con genes aleatorios.
    
    Genera una poblaci√≥n de agentes donde cada uno tiene pesos de red neuronal
    inicializados aleatoriamente seg√∫n la inicializaci√≥n por defecto de PyTorch.
    Esta diversidad inicial es crucial para el √©xito del algoritmo gen√©tico.
    
    Returns:
        list: Lista de POPULATION_SIZE agentes con genes aleatorios
        
    Note:
        La diversidad gen√©tica inicial determina el espacio de b√∫squeda
        que el algoritmo puede explorar durante la evoluci√≥n.
    """
    return [Agent() for _ in range(POPULATION_SIZE)]

def evaluate_fitness(population, env):
    """
    Eval√∫a el fitness de cada agente en la poblaci√≥n.
    
    Cada agente se prueba en un escenario aleatorio generado din√°micamente.
    La variabilidad en los escenarios asegura que los agentes desarrollen
    estrategias robustas y generalizables en lugar de sobreajustarse a
    configuraciones espec√≠ficas.
    
    Proceso de evaluaci√≥n:
    1. Generar escenario aleatorio (posiciones, frutas, venenos)
    2. Ejecutar agente por m√°ximo 50 pasos
    3. Acumular recompensas totales como fitness
    4. Repetir para todos los agentes de la poblaci√≥n
    
    Args:
        population (list): Lista de agentes a evaluar
        env (GridEnvironment): Entorno de evaluaci√≥n
        
    Note:
        El fitness se calcula como la suma total de recompensas obtenidas
        durante el episodio, incluyendo penalizaciones por movimientos,
        recompensas por frutas y penalizaciones por venenos.
    """
    for agent in population:
        # GENERACI√ìN DE ESCENARIO ALEATORIO
        # N√∫mero variable de frutas (1-4) para diversidad de dificultad
        num_fruits = np.random.randint(1, 5)
        
        # Crear lista de todas las posiciones posibles
        all_pos = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)]
        random.shuffle(all_pos)  # Barajar para selecci√≥n aleatoria
        
        # Asignar posiciones sin solapamiento
        agent_pos = all_pos.pop()   # Posici√≥n inicial del agente
        fruit_pos = [all_pos.pop() for _ in range(num_fruits)]  # Posiciones de frutas
        poison_pos = [all_pos.pop() for _ in range(np.random.randint(1, 4))]  # 1-3 venenos

        # EJECUCI√ìN DEL EPISODIO
        state = env.reset(agent_pos=agent_pos, fruit_pos=fruit_pos, poison_pos=poison_pos)
        total_reward = 0
        
        # M√°ximo 50 pasos para evitar episodios infinitos
        for _ in range(50):
            action = agent.choose_action(state)
            state, reward, done = env.step(action)
            total_reward += reward
            
            # Terminar si el agente completa el objetivo
            if done:
                break
                
        # Asignar fitness como recompensa total acumulada
        agent.fitness = total_reward

def selection(population):
    """
    Selecciona los mejores agentes de la poblaci√≥n para reproducci√≥n.
    
    Implementa selecci√≥n elitista donde solo los agentes con mayor fitness
    se seleccionan como padres para la siguiente generaci√≥n. Este m√©todo
    asegura que las caracter√≠sticas exitosas se preserven y propaguen.
    
    Estrategia de selecci√≥n:
    - Ordenar poblaci√≥n por fitness (mayor a menor)
    - Seleccionar el 20% superior como padres
    - Estos padres participar√°n en cruzamiento y algunos en elitismo
    
    Args:
        population (list): Poblaci√≥n de agentes evaluados
        
    Returns:
        list: Los mejores agentes seleccionados para reproducci√≥n
        
    Note:
        Un porcentaje del 20% permite suficiente diversidad gen√©tica
        mientras mantiene presi√≥n selectiva hacia mejores soluciones.
    """
    # Ordenar por fitness descendente (mejores primero)
    population.sort(key=lambda x: x.fitness, reverse=True)
    
    # Seleccionar el 20% superior de la poblaci√≥n
    return population[:int(POPULATION_SIZE * 0.2)]

def crossover(parent1, parent2):
    """
    Crea un nuevo agente combinando genes de dos padres.
    
    Implementa cruzamiento uniforme donde cada par√°metro (gen) del hijo
    se hereda aleatoriamente de uno de los dos padres. Este m√©todo mantiene
    bloques funcionales de la red mientras permite recombinaci√≥n gen√©tica.
    
    Proceso de cruzamiento:
    1. Crear nuevo agente hijo
    2. Para cada par√°metro de la red neuronal:
       - Probabilidad 50%: heredar del padre 1
       - Probabilidad 50%: heredar del padre 2
    3. Cargar genes combinados en el hijo
    
    Args:
        parent1 (Agent): Primer padre seleccionado
        parent2 (Agent): Segundo padre seleccionado
        
    Returns:
        Agent: Nuevo agente hijo con genes combinados
        
    Note:
        El cruzamiento uniforme preserva mejor las estructuras funcionales
        de las redes neuronales comparado con cruzamiento de un punto.
    """
    # Crear nuevo agente hijo
    child = Agent()
    
    # Obtener diccionarios de par√°metros (genes) de los padres
    p1_genes = parent1.network.state_dict()
    p2_genes = parent2.network.state_dict()
    child_genes = child.network.state_dict()

    # Cruzamiento uniforme: cada gen se hereda aleatoriamente
    for key in child_genes.keys():
        # 50% probabilidad de heredar cada gen de cada padre
        if random.random() < 0.5:
            child_genes[key] = p1_genes[key].clone()  # Heredar del padre 1
        else:
            child_genes[key] = p2_genes[key].clone()  # Heredar del padre 2
    
    # Cargar genes combinados en la red del hijo
    child.network.load_state_dict(child_genes)
    return child

def mutate(agent):
    """
    Introduce variaci√≥n gen√©tica aleatoria en un agente.
    
    Implementa mutaci√≥n gaussiana donde cada par√°metro tiene una probabilidad
    MUTATION_RATE de ser alterado con ruido gaussiano. Esta mutaci√≥n permite
    explorar nuevas regiones del espacio de b√∫squeda y evitar convergencia
    prematura a √≥ptimos locales.
    
    Proceso de mutaci√≥n:
    1. Para cada par√°metro de la red neuronal:
       - Probabilidad MUTATION_RATE: agregar ruido gaussiano
       - Magnitud del ruido: distribuci√≥n normal œÉ=0.1
    2. Recargar par√°metros modificados en la red
    
    Args:
        agent (Agent): Agente a mutar
        
    Returns:
        Agent: El mismo agente con genes posiblemente mutados
        
    Note:
        La mutaci√≥n gaussiana con œÉ=0.1 proporciona un balance entre
        exploraci√≥n (nuevas soluciones) y explotaci√≥n (preservar buenas soluciones).
    """
    child_genes = agent.network.state_dict()
    
    # Aplicar mutaci√≥n a cada par√°metro independientemente
    for key in child_genes.keys():
        if random.random() < MUTATION_RATE:
            # Agregar ruido gaussiano con desviaci√≥n est√°ndar 0.1
            noise = torch.randn_like(child_genes[key]) * 0.1
            child_genes[key] += noise
    
    # Recargar par√°metros mutados en la red
    agent.network.load_state_dict(child_genes)
    return agent

if __name__ == "__main__":
    """
    Bucle principal del algoritmo gen√©tico.
    
    Ejecuta el proceso evolutivo completo a trav√©s de m√∫ltiples generaciones,
    implementando el ciclo: evaluaci√≥n ‚Üí selecci√≥n ‚Üí cruzamiento ‚Üí mutaci√≥n.
    Incluye elitismo para preservar las mejores soluciones y logging detallado
    del progreso evolutivo.
    """
    print("=" * 80)
    print("üß¨ INICIANDO ENTRENAMIENTO CON ALGORITMO GEN√âTICO üß¨")
    print("=" * 80)
    print(f"Par√°metros de evoluci√≥n:")
    print(f"üîπ Tama√±o de poblaci√≥n: {POPULATION_SIZE}")
    print(f"üîπ Generaciones: {NUM_GENERATIONS}")
    print(f"üîπ Tasa de mutaci√≥n: {MUTATION_RATE*100}%")
    print(f"üîπ Elitismo: {ELITISM_COUNT} agentes")
    print("=" * 80)
    
    # INICIALIZACI√ìN
    env = GridEnvironment()
    population = create_initial_population()
    
    print("üå± Poblaci√≥n inicial creada")
    print("üéØ Comenzando evoluci√≥n...")
    print()

    # BUCLE EVOLUTIVO PRINCIPAL
    for gen in range(NUM_GENERATIONS):
        print(f"üîÑ Generaci√≥n {gen+1}/{NUM_GENERATIONS}")
        
        # FASE 1: EVALUACI√ìN DE FITNESS
        evaluate_fitness(population, env)
        
        # FASE 2: SELECCI√ìN DE PADRES
        parents = selection(population)
        
        # FASE 3: AN√ÅLISIS DE PROGRESO
        best_agent_of_gen = parents[0]  # El mejor agente de esta generaci√≥n
        best_fitness = best_agent_of_gen.fitness
        avg_fitness = np.mean([agent.fitness for agent in population])
        
        # Logging del progreso evolutivo
        print(f"   üìä Mejor fitness: {best_fitness:.2f}")
        print(f"   üìà Fitness promedio: {avg_fitness:.2f}")
        print(f"   üèÜ Mejora: {best_fitness - avg_fitness:.2f}")

        # FASE 4: PRESERVACI√ìN DEL MEJOR AGENTE
        # Guardar genes del mejor agente de esta generaci√≥n
        torch.save(best_agent_of_gen.network.state_dict(), "best_agent_genes.pth")
        print(f"   üíæ Mejor agente guardado")

        # FASE 5: CREACI√ìN DE NUEVA GENERACI√ìN
        new_population = []
        
        # ELITISMO: Los mejores agentes pasan directamente
        new_population.extend(parents[:ELITISM_COUNT])
        print(f"   üëë {ELITISM_COUNT} elite preservados")

        # REPRODUCCI√ìN: Llenar resto con descendencia
        offspring_count = 0
        while len(new_population) < POPULATION_SIZE:
            # Seleccionar dos padres aleatoriamente del pool de elite
            parent1, parent2 = random.sample(parents, 2)
            
            # Cruzamiento: combinar genes de padres
            child = crossover(parent1, parent2)
            
            # Mutaci√≥n: introducir variaci√≥n gen√©tica
            child = mutate(child)
            
            new_population.append(child)
            offspring_count += 1
            
        print(f"   üë∂ {offspring_count} descendientes creados")
        
        # Reemplazar poblaci√≥n anterior
        population = new_population
        
        print("   ‚úÖ Generaci√≥n completada")
        print("-" * 60)

    # FINALIZACI√ìN
    print("\n" + "=" * 80)
    print("üéâ ¬°ENTRENAMIENTO EVOLUTIVO COMPLETADO! üéâ")
    print("=" * 80)
    print("üìÅ El mejor ADN evolutivo est√° guardado en 'best_agent_genes.pth'")
    print("üöÄ Puedes usar este agente en los demostradores para ver su comportamiento")
    print("üß¨ El agente ha evolucionado a trav√©s de", NUM_GENERATIONS, "generaciones")
    print("=" * 80)